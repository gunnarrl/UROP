{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8de8261d",
   "metadata": {},
   "source": [
    "# Testing Multi Sample Dropout using fastai\n",
    "Multi-Sample Dropout introduced in the paper [Multi-Sample Dropout for Accelearted Training and Better Generalization](https://arxiv.org/abs/1905.09788) is a new way to expand the tradional Dropout by using multiple dropout masks for the same mini-batch.\n",
    "\n",
    "The original dropout creates a randomly selected subset (called a dropout sample) from the input in each training iteration while the multi-sample dropout creates multiple dropout samples. The loss is calculated for each sample, and then the losses are averaged to obtain the final loss.\n",
    "\n",
    "The paper shows that multi-sample dropout significantly accelerates training by reducing the number of iterations until convergence for image classification tasks using the old way of training neural networks i.e. using a constant learning rate and decaying it. So I test this method for cyclic learning and see if I can reproduce the results from the paper.\n",
    "\n",
    "**Note**:- If you are not familiar with cyclic learning I wrote a jupyter notebook explaining the 4 key papers that introduced all the techniques by Leslie N. Smith, [Reproducing Leslie N. Smith's papers using fastai](https://github.com/KushajveerSingh/Deep-Learning-Notebooks/blob/master/Leslie%20N.%20Smith%20paper%20notebooks%20(fastai)/main.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b8d7d9",
   "metadata": {},
   "source": [
    "# Table of Contents:\n",
    "1. Load CIFAR-100 (initially I test using CIFAR-100)\n",
    "2. Resnet-56\n",
    "    1. How to implement multi-sample dropout in model\n",
    "    2. Diversity among samples is needed\n",
    "3. Code for Multi-Sample Dropout\n",
    "4. Code for Multi-Sample Dropout Loss function\n",
    "5. Get baseline without Multi-Sample Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c94df0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aed280c",
   "metadata": {},
   "source": [
    "## Load CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770e5e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I do the initial tests for CIFAR-100\n",
    "path = Path('/home/kushaj/Desktop/Data/cifar100/')\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab920aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (ImageList.from_folder(path)\n",
    "                 .split_by_folder(valid='test')\n",
    "                 .label_from_folder()\n",
    "                 .transform(get_transforms(), size=(32,32))\n",
    "                 .databunch(bs=128, val_bs=512, num_workers=8)\n",
    "                 .normalize(cifar_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f672e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(3, figsize=(6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef8f4aa",
   "metadata": {},
   "source": [
    "## Resnet-56\n",
    "There is nothing special about this model. For quick tests I like to use this model especially for images with small sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9c57e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Classes for the head of our model\n",
    "class AdaptiveConcatPool2d(nn.Module):\n",
    "    \"Layer that concats `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d`.\"\n",
    "    def __init__(self, sz=None):\n",
    "        \"Output will be 2*sz or 2 if sz is None\"\n",
    "        super().__init__()\n",
    "        self.output_size = sz or 1\n",
    "        self.ap = nn.AdaptiveAvgPool2d(self.output_size)\n",
    "        self.mp = nn.AdaptiveMaxPool2d(self.output_size)\n",
    "    def forward(self, x): \n",
    "        return torch.cat([self.mp(x), self.ap(x)], 1)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    \"Flatten `x` to a single dimension, often used at the end of a model. `full` for rank-1 tensor\"\n",
    "    def __init__(self, full:bool=False):\n",
    "        super().__init__()\n",
    "        self.full = full\n",
    "    def forward(self, x):\n",
    "        return x.view(-1) if self.full else x.view(x.size(0), -1)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, c_in, c_out, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(c_in, c_out, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(c_out)\n",
    "        self.conv2 = nn.Conv2d(c_out, c_out, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(c_out)\n",
    "        \n",
    "        if stride != 1 or c_in != c_out:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(c_in, c_out, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(c_out)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x) if hasattr(self, 'shortcut') else x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += shortcut\n",
    "        return F.relu(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99553fe",
   "metadata": {},
   "source": [
    "### How to implement multi-sample dropout in model\n",
    "Multi-Sample Dropout is used in the dropout layers that are present in the fully connected layers of your model, that is at the end of the model. So to implement it, you don't have to change your base architecture. Only the head needs to be changed.\n",
    "\n",
    "For my model the following layers form the head of my model:\n",
    "```python\n",
    "head = nn.Sequential(\n",
    "    AdaptiveConcatPool2d(),\n",
    "    Flatten(),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.Dropout(0.25),\n",
    "    nn.Linear(128, 128, bias=True),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(128, num_classes, bias=True)\n",
    ")\n",
    "```\n",
    "\n",
    "### Diversity among samples is needed\n",
    "The key to faster training with multi-sample dropout is the diversity among dropout samples; if there is no diversity, the multi-sample technique gives no gain and simply wastes computation resources. Although dropout is one of the best sources of diversity, the multi-sample technique can be used with other sources of diversity. In the paper they used these two\n",
    "1. **Horizontal flipping**: This is applied to half of the dropout samples. To implement this, use this code\n",
    "    ```python\n",
    "    # Here the suze of out is (num_batches, num_channels, height, width)\n",
    "    out_flip = torch.flip(out, dims=[3])\n",
    "    ```\n",
    "2. **Zero padding at a pooling layer**: This is a tricky one because I don't use max-pool layers at the end of my models. I use Adaptive pooling instead. I don't know if it makes sense to introduce a padding layer before adaptive pooling and then pooling over a random crop. So I skip it for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52429723",
   "metadata": {},
   "source": [
    "## Code for Multi-Sample Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48306af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet(nn.Module):\n",
    "    \"\"\"\n",
    "    Forward function can be divided into two parts. First the main backbone of the model\n",
    "    which is implemented using\n",
    "    \n",
    "        out = F.relu(self.bnorm1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "    \n",
    "    So if you want to add multi-samply dropout to your own models, you don't have to worry\n",
    "    about changing about your backbone.\n",
    "    \n",
    "    Multi-Sample code\n",
    "    Here I separate the loss computation and the generation of multi-samples. So in the model\n",
    "    I only create a list and then store my samples in it.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_blocks=[9,9,9], num_classes:int=100, num_samples:int=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bnorm1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self.make_group(16, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self.make_group(16, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self.make_group(32, 64, num_blocks[2], stride=2)\n",
    "        \n",
    "        # The layers that form my head\n",
    "        self.pool = AdaptiveConcatPool2d()\n",
    "        self.flat = Flatten()\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.lin1 = nn.Linear(128, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.lin2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def make_group(self, c_in, c_out, num_blocks, stride):\n",
    "        layers = [BasicBlock(c_in, c_out, stride)]\n",
    "        for i in range(num_blocks-1):\n",
    "            layers.append(BasicBlock(c_out, c_out, stride=1))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bnorm1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        \n",
    "        # Multi Sample Dropout\n",
    "        out_samples = []\n",
    "        \n",
    "        # I store the horizontal flip of the feature maps in f_map_flip so as to\n",
    "        # avoid duplicate computation.\n",
    "        if self.num_samples > 1:\n",
    "            f_map_flip = torch.flip(out, dims=[3])\n",
    "            f_map_flip = self.bn1(self.flat(self.pool(f_map_flip)))\n",
    "        f_map = self.bn1(self.flat(self.pool(out)))\n",
    "        \n",
    "        for i in range(self.num_samples):\n",
    "            if i%2 == 0:\n",
    "                # Do not flip the feature map\n",
    "                out_s = F.dropout(f_map)\n",
    "            else:\n",
    "                # Flip the feature map\n",
    "                out_s = F.dropout(f_map_flip)\n",
    "            \n",
    "            out_s = self.lin1(out_s)\n",
    "            out_s = F.relu(out_s)\n",
    "            out_s = self.bn2(out_s)\n",
    "            out_s = F.dropout(out_s)\n",
    "            out_s = self.lin2(out_s)\n",
    "            \n",
    "            out_samples.append(out_s)\n",
    "        \n",
    "        return out_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b442a14",
   "metadata": {},
   "source": [
    "## Code for Multi-Sample Dropout Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aeeb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiSampleLoss(input, target, num_samples:int=1):\n",
    "    total = 0\n",
    "    for sample in input:\n",
    "        total += F.cross_entropy(sample, target)\n",
    "    return total/float(num_samples)\n",
    "\n",
    "multiSampleLoss = partial(MultiSampleLoss, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc273ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_msd(input, targs):\n",
    "    for sample in input:\n",
    "        F.softmax(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39b93c3",
   "metadata": {},
   "source": [
    "## Get baseline without Multi-Sample Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376608c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data, \n",
    "                Resnet(), \n",
    "                loss_func=multiSampleLoss,\n",
    "                wd=1e-4, \n",
    "                path='.',\n",
    "                callback_fns=ShowGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed88ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad822740",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ea635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, max_lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c6f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiSampleLoss = partial(MultiSampleLoss, num_samples=2)\n",
    "learn = Learner(data, \n",
    "                Resnet(num_samples=2), \n",
    "                loss_func=multiSampleLoss,\n",
    "                wd=1e-4, \n",
    "                path='.',\n",
    "                callback_fns=ShowGraph)\n",
    "learn.fit_one_cycle(5, max_lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bf7565",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiSampleLoss = partial(MultiSampleLoss, num_samples=8)\n",
    "learn = Learner(data, \n",
    "                Resnet(num_samples=8), \n",
    "                loss_func=multiSampleLoss,\n",
    "                wd=1e-4, \n",
    "                path='.',\n",
    "                callback_fns=ShowGraph)\n",
    "learn.fit_one_cycle(5, max_lr=1e-1)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
