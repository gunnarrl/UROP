{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d20c611",
   "metadata": {},
   "source": [
    "# Reconstruction sampling\n",
    "\n",
    "The concept behind reconstruction sampling is simple:\n",
    "- we know that a probability distribution can be represented with only a handful of points via Gaussian process regression (GPR)\n",
    "- we don't know a-priori the true mean and covariance of the parameters in the model\n",
    "- we can iteratively guess both by repeatedly evaluating the posterior at specific points, performing reconstruction, and sampling the reconstructed posterior via MCMC\n",
    "\n",
    "In lieu of actually writing a piece of software, this notebook is for attemping to implement this algorithm on a simple distribution and doing each iteration in different cells, rather than in a function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c9deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import emcee\n",
    "import pyDOE2\n",
    "import matplotlib.pyplot as plt\n",
    "import ARTsampler as ARTsampler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad59e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot configurations\n",
    "plt.rc(\"text\", usetex=True)\n",
    "plt.rc(\"font\", size=24, family=\"serif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e52d66",
   "metadata": {},
   "source": [
    "## Step 0: define a true distribution\n",
    "\n",
    "We will start with a simple example - a 2D Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f475c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#True mean and covariance\n",
    "true_means = np.array([100.0, 0.0])\n",
    "stddevs = np.array([50.0, 0.1]) #standard deviations\n",
    "rho = -0.99 #correlation\n",
    "true_cov = np.array([[stddevs[0]**2, rho*stddevs[0]*stddevs[1]],\n",
    "                    [rho*stddevs[0]*stddevs[1], stddevs[1]]])\n",
    "true_samples = np.random.multivariate_normal(mean=true_means, cov=true_cov, size=10000).T\n",
    "args = {\"true_mean\": true_means, \"true_covariance\": true_cov}\n",
    "plt.scatter(true_samples[0], true_samples[1], marker='.', c='b', alpha=0.2, s=0.5)\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$y$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26dd868",
   "metadata": {},
   "source": [
    "## Step 0.5: define the log-likelihood\n",
    "\n",
    "Our target distribution needs a log-likelihood. We will assume a flat prior so that the posterior is just the likelihood. In this really silly example, the likelihood knows the true mean and covariance, but in a real analysis there would be some kind of model involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8059fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_log_likelihood(params, args):\n",
    "    #Params is a 2D array containing the two parameters (labeled x and y in figures)\n",
    "    mu = args[\"true_mean\"]\n",
    "    C = args[\"true_covariance\"]\n",
    "    D = mu - params\n",
    "    return -0.5 * np.dot(D, np.linalg.solve(C, D))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde3879",
   "metadata": {},
   "source": [
    "## Step 1: guess a mean and covariance\n",
    "\n",
    "The first step for the reconstruction sampler is to guess a mean and covariance. We will then generate training samples using this guess, and compute the log-posterior probability of each of these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6b6b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our guess for the mean and covariance\n",
    "guess_mean = np.array([20.0, 0.5]) #Note: parameter y is 5sigma away from the true mean\n",
    "guess_cov = np.array([[30.**2, 0],[0, 0.08**2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e990bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a method for subsampling the Gaussian chain with a LH method.\n",
    "def make_training_points(guess_mean, guess_cov, Nsamples=100):\n",
    "    #Create LH training samples\n",
    "    x = pyDOE2.lhs(len(guess_mean), samples=Nsamples,\n",
    "                   criterion=\"center\", iterations=5)\n",
    "        \n",
    "    #Transform them correctly\n",
    "    x -= 0.5 #center the training points\n",
    "    s = 8 #scale\n",
    "    w, RT = np.linalg.eig(guess_cov)\n",
    "    R = RT.T\n",
    "\n",
    "    return np.dot(s*x[:]*np.sqrt(w), R.T)[:] + guess_mean\n",
    "\n",
    "training_points = make_training_points(guess_mean, guess_cov)\n",
    "plt.scatter(true_samples[0], true_samples[1], marker='.', c='b', alpha=0.2, s=0.5)\n",
    "plt.scatter(training_points[:,0], training_points[:,1], c='k', s=10)\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$y$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da69feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "lnlikes = np.array([true_log_likelihood(p, args) for p in training_points])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e00b6a2",
   "metadata": {},
   "source": [
    "# Step 2: construct and ARTsampler\n",
    "\n",
    "Here we construct and ARTsampler object. The final version of this sampler will just take a log-posterior function and a guess for the mean and covariance, but for now it will take in the points we have computed so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5410eaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "art = ARTsampler.ARTstage(guess_mean, guess_cov, training_points, lnlikes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ce26b0",
   "metadata": {},
   "source": [
    "# Step 3: Perform MCMC on the reconstructed posterior\n",
    "\n",
    "We want to make an update to our `guess_mean` and `guess_cov`, which we can do by running an MCMC on the reconstructed posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1147c071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run a test where we draw from the predicted likelihood surface\n",
    "initial = guess_mean\n",
    "ndim, nwalkers = len(initial), 16\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, art.predict)\n",
    "print(\"Running first burn-in\")\n",
    "p0 = initial + 1e-4*np.random.randn(nwalkers, ndim)\n",
    "p0, lp, _ = sampler.run_mcmc(p0, 100)\n",
    "print(\"Running second burn-in\")\n",
    "p0 = p0[np.argmax(lp)] + 1e-4*np.random.randn(nwalkers, ndim)\n",
    "p0, lp, _ = sampler.run_mcmc(p0, 100)\n",
    "sampler.reset()\n",
    "print(\"Running production...\")\n",
    "sampler.run_mcmc(p0, 2000)\n",
    "testchain = sampler.flatchain;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0b1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(true_samples[0], true_samples[1], marker='.', c='b', alpha=0.2, s=0.5)\n",
    "plt.scatter(training_points[:,0], training_points[:,1], c='k', s=10)\n",
    "plt.scatter(testchain[:,0], testchain[:,1], marker='.', c='r', alpha=0.2, s=0.5)\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$y$\")\n",
    "plt.savefig(\"single_iteration.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e7322f",
   "metadata": {},
   "source": [
    "## Step 3: repeat until convergence\n",
    "\n",
    "Wooooo looking good! We can see that even though the training points (black) only partially cover the high-probability region, the reconstructed probability does a reasonable job at updating things. From here we can make a new `guess_mean` and `guess_cov` and repeat the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6245709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_mean = np.mean(testchain, 0)\n",
    "guess_cov = np.cov(testchain.T)\n",
    "training_points = make_training_points(guess_mean, guess_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f601237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lnlikes = np.array([true_log_likelihood(p, args) for p in training_points])\n",
    "art = ARTsampler.ARTstage(guess_mean, guess_cov, training_points, lnlikes)\n",
    "\n",
    "initial = guess_mean\n",
    "ndim, nwalkers = len(initial), 16\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, art.predict)\n",
    "print(\"Running first burn-in\")\n",
    "p0 = initial + 1e-4*np.random.randn(nwalkers, ndim)\n",
    "p0, lp, _ = sampler.run_mcmc(p0, 100)\n",
    "print(\"Running second burn-in\")\n",
    "p0 = p0[np.argmax(lp)] + 1e-4*np.random.randn(nwalkers, ndim)\n",
    "p0, lp, _ = sampler.run_mcmc(p0, 100)\n",
    "sampler.reset()\n",
    "print(\"Running production...\")\n",
    "sampler.run_mcmc(p0, 2000)\n",
    "testchain = sampler.flatchain;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ff4300",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(true_samples[0], true_samples[1], marker='.', c='b', alpha=0.2, s=0.5)\n",
    "plt.scatter(training_points[:,0], training_points[:,1], c='k', s=10)\n",
    "plt.scatter(testchain[:,0], testchain[:,1], marker='.', c='r', alpha=0.2, s=0.5)\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$y$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795b59de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True means:  \", true_means)\n",
    "print(\"2iter means: \", np.mean(testchain, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ae480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True cov:  \\n\", true_cov)\n",
    "print(\"2iter cov: \\n\", np.cov(testchain.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af7e868",
   "metadata": {},
   "source": [
    "## Great success!\n",
    "\n",
    "We see that only after two iteration we achieve what we want. A 1% estimate of the means of our parameters and good estimate of the covariance."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
