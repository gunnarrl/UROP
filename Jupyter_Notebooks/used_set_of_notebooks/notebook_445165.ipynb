{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b781b8c",
   "metadata": {},
   "source": [
    "# Actividad práctica: Predicción de series de tiempo\n",
    "\n",
    "En esta tarea se pide entrenar y evaluar un predictor para la serie de tiempo Mackey-Glass. Esta serie de tiempo se obtiene de la solución de la siguiente ecuación diferencial\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dt} = 0.2 \\frac{ y(t-\\tau)}{1 + y(t-\\tau)^{10}} - 0.1 y(t),\n",
    "$$\n",
    "\n",
    "donde el parámetro $\\tau$ controla el comportamiento dinámico de la serie de tiempo \n",
    "\n",
    "- Siga las instrucciones en este notebook para resolver el problema de predicción\n",
    "- Conteste las preguntas que se encuentran en este enunciado\n",
    "- Finalmente envíe su notebook con los resultados y respuestas a phuijse@inf.uach.cl\n",
    "- No olvide cambiar el título para reflejar los integrantes de su grupo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb2ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib notebook\n",
    "from IPython.display import display, Audio, HTML\n",
    "import matplotlib.pylab as plt\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import scipy.signal\n",
    "%matplotlib notebook\n",
    "from matplotlib import animation, patches\n",
    "import soundfile as sf\n",
    "from style import *\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import *\n",
    "\n",
    "NMSE = lambda y, yhat : np.sum((y - yhat)**2)/np.var(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b61eeef",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Use el código que se muestra a continuación para generar la serie de tiempo Mackey Glass\n",
    "\n",
    "- Considere una razón señal a ruido (SNR) de 2.\n",
    "- Considere $\\tau=17$ (comportamiento debilmente caótico)\n",
    "\n",
    "Se generaran 1000 muestras de la serie de tiempo. Use los primeros 500 puntos para entrenar y los siguientes 500 puntos para hacer predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576db2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# número de observaciones (no modificar)\n",
    "N = 1000\n",
    "# Razón señal a ruido (2., 0.5)\n",
    "SNR = 2.\n",
    "# constantes de la ecuación diferencial (no modificar)\n",
    "a, b = 0.1, 0.2\n",
    "# comportamiento dinámico de Mackey-Glass (17, 30)\n",
    "tau = 17.\n",
    "print(tau)\n",
    "# paso de integración (no modificar)\n",
    "dt = 0.05\n",
    "# condición incial (no modificar)\n",
    "y0 = 0.9\n",
    "# largo temporal (no modificar)\n",
    "tt = 5.\n",
    "t = np.linspace(0, tt, num=N)\n",
    "\n",
    "N_full, tau_full = int(N*tt/dt), int(tau/dt)\n",
    "ymg = y0*np.ones(shape=(N_full, ))\n",
    "# Runge-Kutta integration\n",
    "for n in range(tau_full, N_full-1):\n",
    "    byd = b*ymg[n-tau_full]/(1.0 + ymg[n-tau_full]**10.0)\n",
    "    yk1 = dt*(-a*ymg[n] + byd)\n",
    "    yk2 = dt*(-a*(ymg[n]+yk1/2) + byd)\n",
    "    yk3 = dt*(-a*(ymg[n]+yk2/2) + byd)\n",
    "    yk4 = dt*(-a*(ymg[n]+yk3) + byd)\n",
    "    ymg[n+1] = ymg[n] + yk1/6 + yk2/3 +yk3/3 +yk4/6;\n",
    "ymg = ymg[::int(tt/dt)]\n",
    "#ymg = ymg - np.mean(ymg) \n",
    "# Contaminación con ruido blanco aditivo\n",
    "s_noise = np.sqrt(np.var(ymg)/SNR) \n",
    "np.random.seed(0)\n",
    "y_obs = ymg + s_noise*np.random.randn(len(ymg))\n",
    "\n",
    "# Gráfico\n",
    "fig, ax = plt.subplots(1, figsize=(9, 3), tight_layout=True)\n",
    "ax.plot(t[:500], y_obs[:500])\n",
    "ax.set_title('Serie de tiempo Mackey-Glass (entrenamiento)');\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3d13e0",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "1. Describa en detalle el algoritmo LMS indicando sus semejanzas y diferencias con el filtro de Wiener \n",
    "1. Partiendo del error instantaneo $J_n^s(\\textbf{w}) = e_n^2$ derive la regla de actualización de pesos\n",
    "1. La siguiente clase de *Python* predice y entrena un filtro LMS. Complete la línea que dice \n",
    "\n",
    "` self.w = ? `\n",
    "\n",
    "con el valor correcto de actualización de peso del filtro LMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4887315",
   "metadata": {},
   "source": [
    " 1) EL algoritmo LMS es un tipo de filtro adaptativo, usa el descenso del gradiente para estimar una señal variable en el tiempo. Este algoritmo es uno de los más utilizados usado debido a su baja complejidad computacional y estabilidad.\n",
    "\n",
    "   El filtro consiste en dos componentes, la primera es el calculo de la salida del filtro FIR convulucionado la entrada con los coeficientes. Ademas de la estimación del error.  y la segunda componente es el coeficiente de actualizacion de pesos.\n",
    "   \n",
    "Semejanzas: \n",
    "            - LMS converge a la solucion de Wiener\n",
    "            - LMS es derivado de Wiener\n",
    "            - Son predictores/estimadores\n",
    "            - Ambos puede ser usados para identificar la respuesta al impulso de sistema desconocido.\n",
    "            - Se entrena de manera recursiva y online\n",
    "            \n",
    "Diferencias: \n",
    "             - LMS: No se requiere conocimiento estadístico del proceso\n",
    "             - LMS: No es necesario calcular e invertir la matriz de correlación\n",
    "             - LMS se actualiza online y tiene costo 𝐿.\n",
    "             - LMS: El aprendizaje es estadístico\n",
    "             - Wiener: se entrena offline y tiene costo 𝐿2\n",
    "             - Wiener: El aprendizaje es determinista.\n",
    "\n",
    "\n",
    "2) Derivada $J_n^s(\\textbf{w}) = e_n^2$\n",
    "\n",
    "Sabiendo que $e_n = (d_n - y_n)$,  tambien $y_n = \\sum_{k=0}^{L} w_{n, k} u_{n-k}$\n",
    "               \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J^s_n(\\textbf{w}) &= e_n^2  \\\\\n",
    "&= (d_n - y_n)^2 \\\\\n",
    "&= (d_n - \\sum_{k=0}^{L} w_{n, k} u_{n-k} )^2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dJ(w)}{dw} &=   2 (d_n - \\sum_{k=0}^{L} w_{n, k} u_{n-k})  -u_{n-k} \\\\\n",
    "\\frac{dJ(w)}{dw} &=   -2 e_n u_{n-k} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Usando la regla SGD llegamos a:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{w}_{n+1} = \\textbf{w}_{n} + 2 \\mu e_n \\textbf{u}_{n}\\nonumber \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db8b0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMS_filter(object):\n",
    "    \n",
    "    def __init__(self, L=1, mu=0.5, normalized=True):\n",
    "        self.L = L\n",
    "        self.mu = mu\n",
    "        self.w = np.zeros(shape=(L, ))\n",
    "        self.normalized = normalized\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.L\n",
    "    \n",
    "    def predict(self, u):\n",
    "        return np.dot(self.w, u)\n",
    "    \n",
    "    def update(self, u, d):\n",
    "        d_pred = self.predict(u)\n",
    "        norm = 1.\n",
    "        if self.normalized:\n",
    "            norm = np.sum(u**2) + 1e-6\n",
    "        self.w = self.w + 2 * self.mu * (d - d_pred) * u / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ca3cc0",
   "metadata": {},
   "source": [
    "***\n",
    "## Predicción con algoritmo LMS\n",
    "\n",
    "1. Entrene el predictor con el algoritmo normalized LMS usando el siguiente bloque de código\n",
    "1. Construya una tabla con los NMSE de entrenamiento y prueba para distintos valores de $\\mu$ y $L$\n",
    "    - Se recomienda hacer un barrido logarítmico en $\\mu$ (por ejemplo `mu=np.logspace(-2, 0, num=20)`)\n",
    "    - Use al menos los siguientes valores de $L$: [5, 10, 20, 30]\n",
    "1. Describa cada experimento analizando sus resultados de forma cuantitativa y cualitativa\n",
    "    - ¿Se sobreajuste el filtro a los datos de entrenamiento? \n",
    "    - ¿Se desestabiliza el filtro?\n",
    "1. Indique que combinación obtiene menor MSE de prueba \n",
    "***\n",
    "\n",
    "1. Repita el experimento para $\\tau = 30$ (comportamiento fuertemente caótico) \n",
    "1. Compare los resultados obtenidos con cada serie de tiempo. ¿Qué casos son más sencillos y cuales más complicados?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf69f21b",
   "metadata": {},
   "source": [
    "3) Dada la cantidad de posibiladades que existen entre L y mu, revisaremos algunos casos mas relevantes:\n",
    "    \n",
    "    \n",
    "Para 𝜏=17 (puede probar los valores, abajo se encuentra el gráfico con un slider)\n",
    "\n",
    "a) L=10 y  mu=0.784760 Se puede apreciar en el grafico que esta sobreajustado, es decir, anda muy bien \n",
    "  para el entrenamiento, pero en el test se ve terriblemente afectado. Se desastebiliza bastante.\n",
    "  Siendo MSE entrenamiento 1032.4427, prueba 5186.2362\n",
    "\n",
    "b) L=20 y mu=0.29 Este es uno de los mejores casos, no esta sobreajustado, para el entrenamiento como con el test anda bastante bien,\n",
    "ya que el error cuadratico es bastante pequeño. El filtro no se desastebiliza en general.\n",
    "Siendo MSE entrenamiento 303.7687, prueba 311.9917.\n",
    "\n",
    "c) L=30 y mu=0.61 Este es un caso medio, tiene un sobreajuste medio, posee un error cuadratico moderado,\n",
    "                 Siendo MSE entrenamiento 377.1947, prueba 502.9953\n",
    "\n",
    "\n",
    "4)\n",
    " Valores Optimos L = 30, mu = 0.088587, test = 187.915610\n",
    "\n",
    "\n",
    "\n",
    "Para 𝜏=30\n",
    "\n",
    "a) L=10 y  mu=0.784760 Se puede ver en el gráfico que se encuentra sobreajustado, tiene un error cuadratico tremendo, por lo que se desastebiliza bastante.\n",
    "Siendo MSE entrenamiento 1519.2776, prueba 7629.6103\n",
    "\n",
    "b) L=20 y mu=0.29 En este caso, se puede apreciar que esta mediamante sobreajustado, el error cuadratico es medio. Se desastebiliza medianamente. Siendo MSE entrenamiento 413.9651, prueba 869.1495\n",
    "\n",
    "c) L=30 Y mu=0.61 Esta bastante sobreajustado, el error cuadratico es muy grande. Se desastebiliza bastante. Siendo MSE entrenamiento 395.9658, prueba 2181.1657.\n",
    "\n",
    "4) Valores Optimos L = 20, mu = 0.069519, test = 316.133279\n",
    "\n",
    "----------------------------------------------------------------------------------\n",
    "Compare los resultados obtenidos con cada serie de tiempo. ¿Qué casos son más sencillos y cuales más complicados?\n",
    "\n",
    "Se puede apreciar que para el 𝜏=30, en los casos anteriormentes estudiados, el resultado es mucho más caotico, por ejemplo para 𝜏=17 L=20 y mu=0.29, resulta ser uno de lo mejores casos, pero para el 𝜏=30, para ser un caso sin ningún atractivo. Tambien sucede para la mayoria de los otros casos, en vez de mejorar se empeoran más.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef52aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_NMSE_train = []\n",
    "data_NMSE_test = []\n",
    "L = [5,10,20,30]\n",
    "mu = np.logspace(-2,0,num=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386703c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lms_update(L,mu,print_plot):\n",
    "    lms = LMS_filter(L, mu, normalized=True)\n",
    "\n",
    "    # Entrenamiento\n",
    "    y_pred = np.zeros(shape=(len(y_obs), ))\n",
    "    for k in range(lms.__len__(), 500):\n",
    "        y_window = y_obs[k-lms.__len__():k]\n",
    "        y_pred[k] = lms.predict(y_window)\n",
    "        lms.update(d=y_obs[k], u=y_window)\n",
    "    # Prueba\n",
    "    for k in range(500, len(y_obs)):\n",
    "        y_window = y_obs[k-lms.__len__():k]\n",
    "        y_pred[k] = lms.predict(y_window)\n",
    "\n",
    "    \n",
    "    data_NMSE_train.append((NMSE(ymg[lms.__len__():500], y_pred[lms.__len__():500])))\n",
    "    data_NMSE_test.append(NMSE(ymg[500:], y_pred[500:]))\n",
    "                           \n",
    "    if print_plot:\n",
    "        print(\"MSE entrenamiento %0.4f, prueba %0.4f\" %(NMSE(ymg[lms.__len__():500], y_pred[lms.__len__():500]), \n",
    "                                                NMSE(ymg[500:], y_pred[500:])))\n",
    "    \n",
    "        fig, ax = plt.subplots(3, figsize=(9, 6), tight_layout=True)\n",
    "        ax[0].plot(t, y_obs, 'k.', alpha=0.5, label='Observado'); ax[0].legend();\n",
    "        ax[1].plot(t, ymg, 'g-', alpha=0.5, lw=2, label='Intrínseco'); \n",
    "        ax[1].plot(t[:500], y_pred[:500], alpha=0.75, lw=2, label='Predicho train'); \n",
    "        ax[1].plot(t[500:], y_pred[500:], alpha=0.75, lw=2, label='Predicho test'); ax[1].legend();\n",
    "\n",
    "        ax[2].plot(t[:500], (ymg[:500] - y_pred[:500])**2, label='Error cuadrático train'); \n",
    "        ax[2].plot(t[500:], (ymg[500:] - y_pred[500:])**2, label='Error cuadrático test'); ax[2].legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11382737",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [5,10,20,30]\n",
    "mu = np.logspace(-2,0,num=20)\n",
    "\n",
    "l_opt = mu_opt = test_opt = 100000000\n",
    "\n",
    "for l_val in L:\n",
    "    print(\"L = %i\"%l_val)\n",
    "    data_NMSE_train = []\n",
    "    data_NMSE_test = []\n",
    "    for mu_val in mu:\n",
    "        lms_update(l_val,mu_val,print_plot=False)\n",
    "    df = DataFrame({\"MU\":mu, \"MSE TRAIN\": data_NMSE_train,\"MSE TEST\": data_NMSE_test})\n",
    "    print(df)\n",
    "    \n",
    "    for opt in data_NMSE_test:\n",
    "        if opt < test_opt:\n",
    "            test_opt = opt\n",
    "            l_opt = l_val\n",
    "            mu_opt = mu[data_NMSE_test.index(opt)]\n",
    "    \n",
    "    if l_val < 30 : print(\"--------------------------------------------------------------------------\")\n",
    "    \n",
    "print(\"\\n Valores Optimos L = %i, mu = %f, test = %f\"%(l_opt,mu_opt,test_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a594c594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lms_slider(L,mu):\n",
    "    lms_update(L,mu,print_plot=True)\n",
    "\n",
    "print(\"Plot for TAU=%f\"%tau)\n",
    "interact(lms_slider, L=widgets.SelectionSlider(options=L, value=l_opt),mu=np.logspace(-2,0,num=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2acf408",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# número de observaciones (no modificar)\n",
    "N = 1000\n",
    "# Razón señal a ruido (2., 0.5)\n",
    "SNR = 2.\n",
    "# constantes de la ecuación diferencial (no modificar)\n",
    "a, b = 0.1, 0.2\n",
    "# comportamiento dinámico de Mackey-Glass (17, 30)\n",
    "tau = 30.\n",
    "print(\"SERIES RESTARTED WITH TAU=%f\"%tau)\n",
    "# paso de integración (no modificar)\n",
    "dt = 0.05\n",
    "# condición incial (no modificar)\n",
    "y0 = 0.9\n",
    "# largo temporal (no modificar)\n",
    "tt = 5.\n",
    "t = np.linspace(0, tt, num=N)\n",
    "\n",
    "N_full, tau_full = int(N*tt/dt), int(tau/dt)\n",
    "ymg = y0*np.ones(shape=(N_full, ))\n",
    "# Runge-Kutta integration\n",
    "for n in range(tau_full, N_full-1):\n",
    "    byd = b*ymg[n-tau_full]/(1.0 + ymg[n-tau_full]**10.0)\n",
    "    yk1 = dt*(-a*ymg[n] + byd)\n",
    "    yk2 = dt*(-a*(ymg[n]+yk1/2) + byd)\n",
    "    yk3 = dt*(-a*(ymg[n]+yk2/2) + byd)\n",
    "    yk4 = dt*(-a*(ymg[n]+yk3) + byd)\n",
    "    ymg[n+1] = ymg[n] + yk1/6 + yk2/3 +yk3/3 +yk4/6;\n",
    "ymg = ymg[::int(tt/dt)]\n",
    "#ymg = ymg - np.mean(ymg) \n",
    "# Contaminación con ruido blanco aditivo\n",
    "s_noise = np.sqrt(np.var(ymg)/SNR) \n",
    "np.random.seed(0)\n",
    "y_obs = ymg + s_noise*np.random.randn(len(ymg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf24f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_opt = 100000000\n",
    "\n",
    "for l_val in L:\n",
    "    print(\"L = %i\"%l_val)\n",
    "    data_NMSE_train = []\n",
    "    data_NMSE_test = []\n",
    "    for mu_val in mu:\n",
    "        lms_update(l_val,mu_val,print_plot=False)\n",
    "    df = DataFrame({\"MU\":mu, \"MSE TRAIN\": data_NMSE_train,\"MSE TEST\": data_NMSE_test})\n",
    "    print(df)\n",
    "    \n",
    "    for opt in data_NMSE_test:\n",
    "        if opt < test_opt:\n",
    "            test_opt = opt\n",
    "            l_opt = l_val\n",
    "            mu_opt = mu[data_NMSE_test.index(opt)]\n",
    "    \n",
    "    if l_val < 30 : print(\"--------------------------------------------------------------------------\")\n",
    "    \n",
    "print(\"\\n Valores Optimos L = %i, mu = %f, test = %f\"%(l_opt,mu_opt,test_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0a1d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lms_slider(L,mu):\n",
    "    lms_update(L,mu,print_plot=True)\n",
    "\n",
    "print(\"Plot for TAU=%f\"%tau)\n",
    "interact(lms_slider, L=widgets.SelectionSlider(options=L, value=l_opt),mu=np.logspace(-2,0,num=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029366ed",
   "metadata": {},
   "source": [
    "***\n",
    "## Predicción con algoritmo RLS\n",
    "\n",
    "1. Describa en detalle el algoritmo RLS indicando sus semejanzas y diferencias con el algoritmo LMS\n",
    "1. Partiendo del error histórico $J_N(\\textbf{w}) = \\sum_{i=1}^N \\beta^{N-i} e_i^2$ derive la regla recursiva de actualización de pesos \n",
    "1. La siguiente clase de *Python* predice y entrena un filtro RLS. Complete las líneas que dice \n",
    "\n",
    "` self.w = ? ` y `self.Phi_inv = ?`\n",
    "\n",
    "con el valor correcto de actualización de peso del filtro RLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386f4bb6",
   "metadata": {},
   "source": [
    "1. El  algoritmo RLS provee una mayor velocidad de convergencia y menor error que el algoritmo LMS.\n",
    "Este algoritmo es una versión muy parecida del algoritmo LMS, pero en el criterio de optimización, \n",
    "es donde el algoritmo RLS, calcula una solución más exacta, es debido a esto que es más costoso (requiere muchas más operaciones).\n",
    "\n",
    "Semejanzas: \n",
    "            - Son estimadores/predictores\n",
    "            - Ambos son secuenciales, metodos online que resulven el mismo tipo de problema\n",
    "            - Ambos convergen a la \"misma\" solución\n",
    "\n",
    "Diferencias: \n",
    "\n",
    "             - RLS: Mayor velocidad de convergencia\n",
    "             - RLS: Menos error\n",
    "             - RLS: Mayor costo en operaciones \n",
    "             - RLS: Capaz de adaptarse a cambios más bruscos.\n",
    "             \n",
    "             - LMS: Menos costoso\n",
    "             - LMS: Proceso Markov\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1a7cae",
   "metadata": {},
   "source": [
    "***\n",
    "2. Partiendo de\n",
    "$$\n",
    "\\begin{align}\n",
    "    J_N(\\textbf{w}) = \\sum_{i=1}^n \\beta^{n-i} e_i^2, \\text{donde } 0<\\beta<=1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "El coste de la funcion se minimiza tomando la derivada parcial de $k$ de los coeficientes de $\\textbf{w}_n$ e igualando a cero.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\partial{J(\\textbf{w}_n)}}{\\partial \\textbf{w}_n(k)} = \\sum_{i=0}^n 2 \\beta^{n-i}e_i\\frac{\\partial e_i}{\\partial \\textbf{w}_n (k)} = -\\sum_{i=0}^n 2 \\beta^{n-i}e_i \\textbf{u}(i-k)=0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Reemplazando $e_i$ por la definicion de la señal de error obtenemos.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{i=0}^n \\beta^{N-i} \\Big[ d_i - \\sum_{k=0}^L \\textbf{w}_{n,k} \\textbf{u}_{i-k} \\Big] d_{i-k} = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Reordenando las ecuaciones.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{l=0}^L \\textbf{w}_{n,k} \\Big[ \\sum_{i=0}^n \\beta^{n-i} \\textbf{u}_{i-l} \\textbf{u}_{i-k} \\Big] = \\sum_{i=0}^n\\beta^{n-i} d_i \\textbf{u}_{i-k}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Esta forma puede ser representada en terminos de matrices.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Phi_n \\textbf{w}_n = \\theta_n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Donde\n",
    "- Matriz de correlacion ponderada. $\\Phi_n = \\beta \\Phi_{n-1} + \\textbf{u}_n \\textbf{u}_n^T$\n",
    "- Vector de correlacion cruzada ponderada. $\\theta_n = \\theta_{n-1} + d_n \\textbf{u}_n$\n",
    "\n",
    "Asi, podemos encontrar los coeficientes que minimizan la funcion como:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{w}_n = \\Phi_n^{-1} \\theta_n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Queremos derivar una solucion recursiva de la forma\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{w}_n = \\textbf{w}_{n-1} - \\Delta \\textbf{w}_{n-1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Estableciendo las condiciones iniciales ($\\Phi_0 = \\delta I, \\theta_0 = 0 $) y usando la identidad de Woodbury: $(A + UCV)^{-1} = A^{-1} - A^{-1} U (C^{-1} + VA^{-1} U)^{-1} V A^{-1}$\n",
    "\n",
    "Con: $A = \\Phi_{n-1}^{-1}, U = \\textbf{u}_n, C = 1, V = \\textbf{u}_n^{-1}$.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Phi^{-1} &= \\Big[ \\beta \\Phi_{n-1} + \\textbf{u}_n \\textbf{u}_n^T\\Big]^{-1} \\\\\n",
    "&= \\beta^{-1} \\Phi_{n-1}^{-1} - \\beta^{-1} \\textbf{k}_n \\textbf{u}_n^T \\Phi_{n-1}^{-1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Donde $\\textbf{k}_n = \\Phi^{-1}_n \\textbf{u}_n$ es la ganancia.\n",
    "\n",
    "Asi\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{w}_n &= \\Phi^{-1}_n \\theta_n \\\\\n",
    "&= \\beta \\Phi^{-1}_n \\theta_{n-1} + d_n \\Phi^{-1}_n \\textbf{u}_n \\\\\n",
    "&=  \\Phi_{n-1}^{-1} \\theta_{n-1} - \\textbf{k}_n \\textbf{u}_n^T \\Phi_{n-1}^{-1} \\theta_{n-1} + \\Phi_n^{-1} \\textbf{u}_n d_n ; \\text{  si  }   \\textbf{w}_{n-1} =  \\Phi_{n-1}^{-1} \\theta_{n-1} \\\\\n",
    "&= \\textbf{w}_{n-1} + \\textbf{k}_n ( d_n - \\textbf{u}_n^T  \\textbf{w}_{n-1} ) \\\\\n",
    "&=  \\textbf{w}_{n-1} + \\textbf{k}_n e_n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Donde $\\textbf{k}_n e_n = \\Delta \\textbf{w}_{n-1}$ tambien llamado Factor de correccion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bcc912",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLS_filter(object):\n",
    "    \n",
    "    def __init__(self, L=1, beta=0.9, delta=10.):\n",
    "        self.L = L\n",
    "        self.beta = beta\n",
    "        self.w = np.zeros(shape=(L, ))\n",
    "        self.Phi_inv = delta*np.eye(L)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.L\n",
    "    \n",
    "    def predict(self, u):\n",
    "        return np.dot(self.w, u)\n",
    "    \n",
    "    def update(self, u, d):          \n",
    "        invbeta = 1.0/self.beta\n",
    "        d_pred = self.predict(u)\n",
    "        e = d - d_pred\n",
    "        r = 1. + invbeta*np.dot(np.dot(u, self.Phi_inv), u.T)\n",
    "        k = invbeta*np.dot(self.Phi_inv, u)/r\n",
    "        self.Phi_inv = invbeta* self.Phi_inv - invbeta * self.Phi_inv *np.sum(k*u)\n",
    "        self.w = self.w + np.dot(k,e)\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc9d563",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "1. Entrene el predictor con el algoritmo RLS usando el siguiente bloque de código\n",
    "1. Considere primero  $\\tau=17$\n",
    "1. Construya una tabla con los NMSE de entrenamiento y prueba para distintos valores de $\\beta$ y $L$\n",
    "    - Se recomienda hacer un barrido lineal en $\\beta$ (por ejemplo `mu=np.linspace(0.8, 1.0, num=20)`)\n",
    "    - Use al menos los siguientes valores de $L$: [5, 10, 20, 30]\n",
    "1. Describa cada experimento analizando sus resultados de forma cuantitativa y cualitativa\n",
    "    - ¿Cuánto demora el filtro en estabilizarse? \n",
    "    - ¿Se sobreajuste el filtro a los datos de entrenamiento? \n",
    "    - ¿Se desestabiliza el filtro?\n",
    "1. Indique que combinación obtiene menor MSE de prueba \n",
    "1. Repita el experimento para $\\tau=30$\n",
    "1. Compare con los resultados obtenidos con el algoritmo LMS ¿Qué algoritmo demora menos en converger?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6852ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# número de observaciones (no modificar)\n",
    "N = 1000\n",
    "# Razón señal a ruido (2., 0.5)\n",
    "SNR = 2.\n",
    "# constantes de la ecuación diferencial (no modificar)\n",
    "a, b = 0.1, 0.2\n",
    "# comportamiento dinámico de Mackey-Glass (17, 30)\n",
    "tau = 17.\n",
    "print(\"SERIES RESTARTED WITH TAU=%f\"%tau)\n",
    "# paso de integración (no modificar)\n",
    "dt = 0.05\n",
    "# condición incial (no modificar)\n",
    "y0 = 0.9\n",
    "# largo temporal (no modificar)\n",
    "tt = 5.\n",
    "t = np.linspace(0, tt, num=N)\n",
    "\n",
    "N_full, tau_full = int(N*tt/dt), int(tau/dt)\n",
    "ymg = y0*np.ones(shape=(N_full, ))\n",
    "# Runge-Kutta integration\n",
    "for n in range(tau_full, N_full-1):\n",
    "    byd = b*ymg[n-tau_full]/(1.0 + ymg[n-tau_full]**10.0)\n",
    "    yk1 = dt*(-a*ymg[n] + byd)\n",
    "    yk2 = dt*(-a*(ymg[n]+yk1/2) + byd)\n",
    "    yk3 = dt*(-a*(ymg[n]+yk2/2) + byd)\n",
    "    yk4 = dt*(-a*(ymg[n]+yk3) + byd)\n",
    "    ymg[n+1] = ymg[n] + yk1/6 + yk2/3 +yk3/3 +yk4/6;\n",
    "ymg = ymg[::int(tt/dt)]\n",
    "#ymg = ymg - np.mean(ymg) \n",
    "# Contaminación con ruido blanco aditivo\n",
    "s_noise = np.sqrt(np.var(ymg)/SNR) \n",
    "np.random.seed(0)\n",
    "y_obs = ymg + s_noise*np.random.randn(len(ymg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db0fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.linspace(0.8, 1.0, num=20)\n",
    "\n",
    "data_NMSE_train = []\n",
    "data_NMSE_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc1e051",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.linspace(0.8, 1.0, num=20)\n",
    "\n",
    "def rls_update(L,beta,print_plot):\n",
    "    NMSE = lambda y, yhat : np.sum((y - yhat)**2)/np.var(y)\n",
    "    \n",
    "    rls = RLS_filter(L, beta, delta=1.)\n",
    "    # Entrenamiento\n",
    "    y_pred = np.zeros(shape=(len(y_obs), ))\n",
    "    for k in range(rls.__len__(), 500):\n",
    "        y_window = y_obs[k-rls.__len__():k]\n",
    "        rls.update(d=y_obs[k], u=y_window)\n",
    "        y_pred[k] = rls.predict(y_window)\n",
    "    # Prueba\n",
    "    for k in range(500, len(y_obs)):\n",
    "        y_window = y_obs[k-rls.__len__():k]\n",
    "        y_pred[k] = rls.predict(y_window)\n",
    "        \n",
    "    data_NMSE_train.append(NMSE(ymg[rls.__len__():500], y_pred[rls.__len__():500]))\n",
    "    data_NMSE_test.append(NMSE(ymg[500:], y_pred[500:]))\n",
    "\n",
    "    if print_plot:\n",
    "        print(\"MSE entrenamiento %0.6f, prueba %0.6f\" %(NMSE(ymg[rls.__len__():500], y_pred[rls.__len__():500]), \n",
    "                                                        NMSE(ymg[500:], y_pred[500:])))\n",
    "        fig, ax = plt.subplots(3, figsize=(9, 6), tight_layout=True)\n",
    "        ax[0].plot(t, y_obs, 'k.', alpha=0.5, label='Observado'); ax[0].legend();\n",
    "        ax[1].plot(t, ymg, 'g-', alpha=0.5, lw=2, label='Intrínseco'); \n",
    "        ax[1].plot(t[:500], y_pred[:500], alpha=0.75, lw=2, label='Predicho train'); \n",
    "        ax[1].plot(t[500:], y_pred[500:], alpha=0.75, lw=2, label='Predicho test'); ax[1].legend();\n",
    "\n",
    "        ax[2].plot(t[:500], (ymg[:500] - y_pred[:500])**2, label='Error cuadrático train'); \n",
    "        ax[2].plot(t[500:], (ymg[500:] - y_pred[500:])**2, label='Error cuadrático test'); ax[2].legend(); \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e8e4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_opt = beta_opt = test_opt = 100000000\n",
    "\n",
    "\n",
    "for l_val in L:\n",
    "    print(\"L = %i\"%l_val)\n",
    "    data_NMSE_train = []\n",
    "    data_NMSE_test = []\n",
    "    for beta_val in beta:\n",
    "        rls_update(l_val,beta_val,print_plot=False)\n",
    "    df = DataFrame({\"BETA\":beta, \"MSE TRAIN\": data_NMSE_train,\"MSE TEST\": data_NMSE_test})\n",
    "    print(df)\n",
    "    \n",
    "    for opt in data_NMSE_test:\n",
    "        if opt < test_opt:\n",
    "            test_opt = opt\n",
    "            l_opt = l_val\n",
    "            beta_opt = beta[data_NMSE_test.index(opt)]\n",
    "    \n",
    "    if l_val < 30 : print(\"--------------------------------------------------------------------------\")\n",
    "    \n",
    "print(\"\\n Valores Optimos L = %i, BETA = %f, test = %f\"%(l_opt,beta_opt,test_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8d648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rls_slider(L,beta):\n",
    "    rls_update(L,beta,print_plot=True)\n",
    "\n",
    "print(\"Plot for TAU=%f\"%tau)\n",
    "interact(rls_slider, L=widgets.SelectionSlider(options=L, value=l_opt),beta = np.linspace(0.8, 1.0, num=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60038984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# número de observaciones (no modificar)\n",
    "N = 1000\n",
    "# Razón señal a ruido (2., 0.5)\n",
    "SNR = 2.\n",
    "# constantes de la ecuación diferencial (no modificar)\n",
    "a, b = 0.1, 0.2\n",
    "# comportamiento dinámico de Mackey-Glass (17, 30)\n",
    "tau = 30.\n",
    "print(\"SERIES RESTARTED WITH TAU=%f\"%tau)\n",
    "# paso de integración (no modificar)\n",
    "dt = 0.05\n",
    "# condición incial (no modificar)\n",
    "y0 = 0.9\n",
    "# largo temporal (no modificar)\n",
    "tt = 5.\n",
    "t = np.linspace(0, tt, num=N)\n",
    "\n",
    "N_full, tau_full = int(N*tt/dt), int(tau/dt)\n",
    "ymg = y0*np.ones(shape=(N_full, ))\n",
    "# Runge-Kutta integration\n",
    "for n in range(tau_full, N_full-1):\n",
    "    byd = b*ymg[n-tau_full]/(1.0 + ymg[n-tau_full]**10.0)\n",
    "    yk1 = dt*(-a*ymg[n] + byd)\n",
    "    yk2 = dt*(-a*(ymg[n]+yk1/2) + byd)\n",
    "    yk3 = dt*(-a*(ymg[n]+yk2/2) + byd)\n",
    "    yk4 = dt*(-a*(ymg[n]+yk3) + byd)\n",
    "    ymg[n+1] = ymg[n] + yk1/6 + yk2/3 +yk3/3 +yk4/6;\n",
    "ymg = ymg[::int(tt/dt)]\n",
    "#ymg = ymg - np.mean(ymg) \n",
    "# Contaminación con ruido blanco aditivo\n",
    "s_noise = np.sqrt(np.var(ymg)/SNR) \n",
    "np.random.seed(0)\n",
    "y_obs = ymg + s_noise*np.random.randn(len(ymg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b7b385",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_opt = beta_opt = test_opt = 100000000\n",
    "\n",
    "\n",
    "for l_val in L:\n",
    "    print(\"L = %i\"%l_val)\n",
    "    data_NMSE_train = []\n",
    "    data_NMSE_test = []\n",
    "    for beta_val in beta:\n",
    "        rls_update(l_val,beta_val,print_plot=False)\n",
    "    df = DataFrame({\"BETA\":beta, \"MSE TRAIN\": data_NMSE_train,\"MSE TEST\": data_NMSE_test})\n",
    "    print(df)\n",
    "    \n",
    "    for opt in data_NMSE_test:\n",
    "        if opt < test_opt:\n",
    "            test_opt = opt\n",
    "            l_opt = l_val\n",
    "            beta_opt = beta[data_NMSE_test.index(opt)]\n",
    "    \n",
    "    if l_val < 30 : print(\"--------------------------------------------------------------------------\")\n",
    "    \n",
    "print(\"\\n Valores Optimos L = %i, BETA = %f, test = %f\"%(l_opt,beta_opt,test_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d834fc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rls_slider(L,beta):\n",
    "    rls_update(L,beta,print_plot=True)\n",
    "\n",
    "print(\"Plot for TAU=%f\"%tau)\n",
    "interact(rls_slider, L=widgets.SelectionSlider(options=L, value=l_opt),beta = np.linspace(0.8, 1.0, num=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af85fc0",
   "metadata": {},
   "source": [
    "RLS\n",
    "Para tau = 17\n",
    "\n",
    "4.\n",
    "- Observando los graficos comparativos del filtro RLS, se puede afirmar que la convergencia (entiendase decaimiento  y estabilizacion del error cuadratico) del filtro RLS es bastante rapida (t < 0.2) si miramos el error cuadratico de entrenamiento.\n",
    "- Para ciertos valores de L y $\\beta$ (por ejemplo, L=30 y $\\beta$ > 0.9) se puede observar claramente el sobreajuste del filtro a los datos.\n",
    "- Por lo observado en el grafico, el filtro no alcanza a desestabilizarse de forma notoria, independiente de los valores de L y $\\beta$.\n",
    "5.\n",
    "A partir de los datos de la tabla para cada combinacion de L y $\\beta$, el mejor MSE de prueba para tau=17. se obtiene con \n",
    "$L = 30, \\beta = 0.821, test = 188.056$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9426e0b3",
   "metadata": {},
   "source": [
    "RLS\n",
    "Para tau = 30.\n",
    "\n",
    "4.\n",
    "- Para este valor de tau, la convergencia del filtro (entiendase decaimiento y estabilizacion del error cuadratico) es mucho mas rapida, observandose valores por debajo de 0.15[seg] (para L = 30, $\\beta$ = 0.905263), sin embargo, para valores de L = 5 y $\\beta$ < 1 el valor del error cuadratico medio cambia de manera muy rapida y su valor es en general ,comparativamente para este tau y L, el mas alto.\n",
    "- Se puede observar claramente un sobreajuste para valores de L=20 y $\\beta$ > 0.9. Este sobreajuste es bastante pronunciado, ya que la diferencia entre la seccion del grafico de prueba y el grafico intrinseco es bastante marcada.\n",
    "- El filtro no se desestabiliza de forma notoria para ningun valor de L y $\\beta$.\n",
    "5.\n",
    "Analizando los datos de la tabla para tau=30, L=[5,10,20,30] y $0.8 <= \\beta <= 1$; el mejor MSE de prueba se obtiene con $L = 30, \\beta = 0.905263$ y su valor es MSE Test = $308.8732$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d1826",
   "metadata": {},
   "source": [
    "7. Comparando los resultados obtenidos con los diferentes valores de tau, tanto para el filtro LMS como para el filtro RLS, el algoritmo que tiende a converger de manera mas rapida es el algoritmo RLS, aunque se requiera un gasto computacional mayor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75c630f",
   "metadata": {},
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
