{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27900af2",
   "metadata": {},
   "source": [
    "# Inference with GPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63be5ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ac960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe7903a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "rcParams[\"savefig.dpi\"] = 100\n",
    "rcParams[\"figure.dpi\"] = 100\n",
    "rcParams[\"figure.figsize\"] = 12, 4\n",
    "rcParams[\"font.size\"] = 16\n",
    "rcParams[\"text.usetex\"] = False\n",
    "rcParams[\"font.family\"] = [\"sans-serif\"]\n",
    "rcParams[\"font.sans-serif\"] = [\"cmss10\"]\n",
    "rcParams[\"axes.unicode_minus\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777bf04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/matplotlib/matplotlib/issues/12039\n",
    "try:\n",
    "    old_get_unicode_index\n",
    "except NameError:\n",
    "    print('Patching matplotlib.mathtext.get_unicode_index')\n",
    "    import matplotlib.mathtext as mathtext\n",
    "    old_get_unicode_index = mathtext.get_unicode_index\n",
    "    mathtext.get_unicode_index = lambda symbol, math=True:\\\n",
    "        ord('-') if symbol == '-' else old_get_unicode_index(symbol, math)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22ff81b",
   "metadata": {},
   "source": [
    "## The Marginal Likelihood\n",
    "\n",
    "In the previous notebook, we learned how to construct and sample from a simple GP. This is useful for making predictions, i.e., interpolating or extrapolating based on the data you measured. But the true power of GPs comes from their application to *regression* and *inference*: given a dataset $D$ and a model $M(\\theta)$, what are the values of the model parameters $\\theta$ that are consistent with $D$? The parameters $\\theta$ can be the hyperparameters of the GP (the amplitude and time scale), the parameters of some parametric model, or all of the above.\n",
    "\n",
    "A very common use of GPs is to model things you don't have an explicit physical model for, so quite often they are used to model \"nuisances\" in the dataset. But just because you don't care about these nuisances doesn't mean they don't affect your inference: in fact, unmodelled correlated noise can often lead to strong biases in the parameter values you infer. In this notebook, we'll learn how to compute likelihoods of Gaussian Processes so that we can *marginalize* over the nuisance parameters (given suitable priors) and obtain unbiased estimates for the physical parameters we care about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4c7541",
   "metadata": {},
   "source": [
    "Given a set of measurements $y$ distributed according to\n",
    "$$\n",
    "\\begin{align}\n",
    "    y \\sim \\mathcal{N}(\\mathbf{\\mu}(\\theta), \\mathbf{\\Sigma}(\\alpha))\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\theta$ are the parameters of the mean model $\\mu$ and $\\alpha$ are the hyperparameters of the covariance model $\\mathbf{\\Sigma}$, the *marginal likelihood* of $y$ is\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\ln P(y | \\theta, \\alpha) = -\\frac{1}{2}(y-\\mu)^\\top \\mathbf{\\Sigma}^{-1} (y-\\mu) - \\frac{1}{2}\\ln |\\mathbf{\\Sigma}| - \\frac{N}{2} \\ln 2\\pi\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $||$ denotes the determinant and $N$ is the number of measurements. The term *marginal* refers to the fact that this expression implicitly integrates over all possible values of the Gaussian Process; this is not the likelihood of the data given one particular draw from the GP, but given the ensemble of all possible draws from $\\mathbf{\\Sigma}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd54c1a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #D6EAF8; border-left: 15px solid #2E86C1;\">\n",
    "    <h1 style=\"line-height:2.5em; margin-left:1em;\">Exercise 1</h1>\n",
    "</div>\n",
    "\n",
    "Define a function ``ln_gp_likelihood(t, y, sigma, **kwargs)`` that returns the log-likelihood defined above for a vector of measurements ``y`` at a set of times ``t`` with uncertainty ``sigma``. As before, ``**kwargs`` should get passed direcetly to the kernel function. Note that you're going to want to use [np.linalg.slogdet](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linalg.slogdet.html) to compute the log-determinant of the covariance instead of ``np.log(np.linalg.det)``. (Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a05ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ExpSquaredKernel(t1, t2=None, A=1.0, l=1.0):\n",
    "    \"\"\"\n",
    "    Return the ``N x M`` exponential squared\n",
    "    covariance matrix.\n",
    "    \n",
    "    \"\"\"\n",
    "    if t2 is None:\n",
    "        t2 = t1\n",
    "    T2, T1 = np.meshgrid(t2, t1)\n",
    "    return A ** 2 * np.exp(-0.5 * (T1 - T2) ** 2 / l ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dd68b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_gp_likelihood(t, y, sigma=0, **kwargs):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # The covariance and its determinant\n",
    "    npts = len(t)\n",
    "    kernel = ExpSquaredKernel\n",
    "    K = kernel(t, **kwargs) + sigma ** 2 * np.eye(npts)\n",
    "    \n",
    "    # The marginal log likelihood\n",
    "    log_like = -0.5 * np.dot(y.T, np.linalg.solve(K, y))\n",
    "    log_like -= 0.5 * np.linalg.slogdet(K)[1]\n",
    "    log_like -= 0.5 * npts * np.log(2 * np.pi)\n",
    "    \n",
    "    return log_like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd93ac50",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #D6EAF8; border-left: 15px solid #2E86C1;\">\n",
    "    <h1 style=\"line-height:2.5em; margin-left:1em;\">Exercise 2</h1>\n",
    "</div>\n",
    "\n",
    "The following dataset was generated from a zero-mean Gaussian Process with a Squared Exponential Kernel of unity amplitude and unknown timescale. Compute the marginal log likelihood of the data over a range of reasonable values of $l$ and find the maximum. Plot the **likelihood** (not log likelihood) versus $l$; it should be pretty Gaussian. How well are you able to constrain the timescale of the GP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e6e147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import cho_factor\n",
    "\n",
    "def draw_from_gaussian(mu, S, ndraws=1, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Generate samples from a multivariate gaussian\n",
    "    specified by covariance ``S`` and mean ``mu``.\n",
    "    \n",
    "    \"\"\"\n",
    "    npts = S.shape[0]\n",
    "    L, _ = cho_factor(S + eps * np.eye(npts), lower=True)\n",
    "    L = np.tril(L)\n",
    "    u = np.random.randn(npts, ndraws)\n",
    "    x = np.dot(L, u) + mu[:, None]\n",
    "    return x.T\n",
    "\n",
    "def compute_gp(t_train, y_train, t_test, sigma=0, **kwargs):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # Compute the required matrices\n",
    "    kernel = ExpSquaredKernel\n",
    "    Stt = kernel(t_train, **kwargs)\n",
    "    Stt += sigma ** 2 * np.eye(Stt.shape[0])\n",
    "    Spp = kernel(t_test, **kwargs)\n",
    "    Spt = kernel(t_test, t_train, **kwargs)\n",
    "\n",
    "    # Compute the mean and covariance of the GP\n",
    "    mu = np.dot(Spt, np.linalg.solve(Stt, y_train))\n",
    "    S = Spp - np.dot(Spt, np.linalg.solve(Stt, Spt.T))\n",
    "    \n",
    "    return mu, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5e470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset\n",
    "import os\n",
    "l_true = 0.33\n",
    "t = np.linspace(0, 10, 1000)\n",
    "gp_mu, gp_S = compute_gp([], [], t, A=1.0, l=l_true)\n",
    "np.random.seed(3)\n",
    "y_true = draw_from_gaussian(gp_mu, gp_S)[0]\n",
    "sigma = np.ones_like(t) * 0.05\n",
    "y = y_true + sigma * np.random.randn(len(t))\n",
    "X = np.hstack((t.reshape(-1, 1), y.reshape(-1, 1), sigma.reshape(-1, 1)))\n",
    "if not (os.path.exists(\"data\")):\n",
    "    os.mkdir(\"data\")\n",
    "np.savetxt(\"data/sample_data.txt\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef35090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "t, y, sigma = np.loadtxt(\"data/sample_data.txt\", unpack=True)\n",
    "plt.plot(t, y, \"k.\", alpha=0.5, ms=3)\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"data\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6cd534",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.linspace(l_true - 0.1, l_true + 0.1, 300)\n",
    "lnlike = np.array([ln_gp_likelihood(t, y, sigma=sigma, l=li) for li in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8f0c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "like = np.exp(lnlike - lnlike.max())\n",
    "plt.plot(l, like)\n",
    "plt.axvline(l_true, color=\"C1\")\n",
    "plt.xlabel(\"timescale\")\n",
    "plt.ylabel(\"relative likelihood\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b48b4b3",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #D6EAF8; border-left: 15px solid #2E86C1;\">\n",
    "    <h1 style=\"line-height:2.5em; margin-left:1em;\">Exercise 3a</h1>\n",
    "</div>\n",
    "\n",
    "The timeseries below was generated by a linear function of time, $y(t)= mt + b$. In addition to observational uncertainty $\\sigma$ (white noise), there is a fair bit of correlated (red) noise, which we will assume is well described\n",
    "by the squared exponential covariance with a certain (unknown) amplitude $A$ and timescale $l$.\n",
    "\n",
    "Your task is to estimate the values of $m$ and $b$, the slope and intercept of the line, respectively. In this part of the exercise, **assume there is no correlated noise.** Your model for the $n^\\mathrm{th}$ datapoint is thus\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    y_n \\sim \\mathcal{N}(m t_n + b, \\sigma_n\\mathbf{I})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and the probability of the data given the model can be computed by calling your GP likelihood function:\n",
    "\n",
    "```python\n",
    "def lnprob(params):\n",
    "    m, b = params\n",
    "    model = m * t + b\n",
    "    return ln_gp_likelihood(t, y - model, sigma, A=0, l=1)\n",
    "```\n",
    "\n",
    "Note, importantly, that we are passing the **residual vector**, $y - (mt + b)$, to the GP, since above we coded up a zero-mean Gaussian process. We are therefore using the GP to model the **residuals** of the data after applying our physical model (the equation of the line).\n",
    "\n",
    "To estimate the values of $m$ and $b$ we could generate a fine grid in those two parameters and compute the likelihood at every point. But since we'll soon be fitting for four parameters (in the next part), we might as well upgrade our inference scheme and use the ``emcee`` package to do Markov Chain Monte Carlo (MCMC). If you haven't used ``emcee`` before, check out the first few tutorials on the [documentation page](https://emcee.readthedocs.io/en/latest/). The basic setup for the problem is this:\n",
    "\n",
    "```python\n",
    "import emcee\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob)\n",
    "\n",
    "print(\"Running burn-in...\")\n",
    "p0, _, _ = sampler.run_mcmc(p0, nburn)   # nburn = 500 should do\n",
    "sampler.reset()\n",
    "\n",
    "print(\"Running production...\")\n",
    "sampler.run_mcmc(p0, nsteps);            # nsteps = 1000 should do\n",
    "```\n",
    "\n",
    "where ``nwalkers`` is the number of walkers (something like 20 or 30 is fine), ``ndim`` is the number of dimensions (2 in this case), and ``lnprob`` is the log-probability function for the data given the model. Finally, ``p0`` is a list of starting positions for each of the walkers. Pick some fiducial/eyeballed value for $m$ and $b$, then add a small random number to each to generate different initial positions for each walker. This will initialize all walkers in a ball centered on some point, and as the chain progresses they'll diffuse out and begin to explore the posterior.\n",
    "\n",
    "Once you have sampled the posterior, plot several draws from it on top of the data. Also plot the **true** line that generated the dataset (given by the variables ``m_true`` and ``b_true`` below). Do they agree, or is there bias in your inferred values? Use the ``corner`` package to plot the joint posterior. How many standard deviations away from the truth are your inferred values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d00f09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "m_true = 3.10\n",
    "b_true = 17.4\n",
    "l_true = 1.25\n",
    "A_true = 3.50\n",
    "s_true = 2.00\n",
    "t = np.linspace(0, 10, 50)\n",
    "gp_mu, gp_S = compute_gp([], [], t, A=A_true, l=l_true)\n",
    "np.random.seed(9)\n",
    "y_true = m_true * t + b_true\n",
    "trend = draw_from_gaussian(gp_mu, gp_S)[0]\n",
    "noise = np.ones_like(t) * s_true\n",
    "y = y_true + trend + noise * np.random.randn(len(t))\n",
    "X = np.hstack((t.reshape(-1, 1), y.reshape(-1, 1), noise.reshape(-1, 1)))\n",
    "np.savetxt(\"data/sample_data_line.txt\", X)\n",
    "np.savetxt(\"data/sample_data_line_truths.txt\", [m_true, b_true, A_true, l_true])\n",
    "\n",
    "# Plot it\n",
    "t, y, sigma = np.loadtxt(\"data/sample_data_line.txt\", unpack=True)\n",
    "plt.plot(t, y_true, \"C0\", label=\"truth\")\n",
    "plt.plot(t, y_true + trend, \"C1\", alpha=0.5, label=\"truth + trend\")\n",
    "plt.plot(t, y, \"k.\",  ms=5, label=\"observed\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"data\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4999fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t, y, sigma = np.loadtxt(\"data/sample_data_line.txt\", unpack=True)\n",
    "m_true, b_true, A_true, l_true = np.loadtxt(\"data/sample_data_line_truths.txt\", unpack=True)\n",
    "plt.errorbar(t, y, yerr=sigma, fmt=\"k.\", label=\"observed\")\n",
    "plt.plot(t, m_true * t + b_true, color=\"C0\", label=\"truth\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"data\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c806165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnprob(p):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    m, b = p\n",
    "    if (m < 0) or (m > 10):\n",
    "        return -np.inf\n",
    "    elif (b < 0) or (b > 30):\n",
    "        return -np.inf\n",
    "    model = m * t + b\n",
    "    lnlike = ln_gp_likelihood(t, y - model, sigma, A=0, l=1)\n",
    "    return lnlike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ba5e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "print(\"Using emcee version {0}\".format(emcee.__version__))\n",
    "\n",
    "initial = [4.0, 15.0]\n",
    "ndim = len(initial)\n",
    "nwalkers = 32\n",
    "p0 = initial + 1e-3 * np.random.randn(nwalkers, ndim)\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob)\n",
    "\n",
    "print(\"Running burn-in...\")\n",
    "p0, _, _ = sampler.run_mcmc(p0, 500)\n",
    "sampler.reset()\n",
    "\n",
    "print(\"Running production...\")\n",
    "sampler.run_mcmc(p0, 1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5de2882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.errorbar(t, y, yerr=sigma, fmt=\".k\", capsize=0)\n",
    "\n",
    "# The positions where the prediction should be computed\n",
    "x = np.linspace(0, 10, 500)\n",
    "\n",
    "# Plot 24 posterior samples\n",
    "samples = sampler.flatchain\n",
    "for s in samples[np.random.randint(len(samples), size=24)]:\n",
    "    m, b = s\n",
    "    model = m * x + b\n",
    "    plt.plot(x, model, color=\"#4682b4\", alpha=0.3)\n",
    "\n",
    "# Plot the truth\n",
    "plt.plot(x, m_true * x + b_true, \"C1\", label=\"truth\")\n",
    "    \n",
    "plt.ylabel(\"data\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.title(\"fit assuming uncorrelated noise\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396874f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "labels = [\"slope\", \"intercept\"]\n",
    "truths = [m_true, b_true]\n",
    "corner.corner(sampler.flatchain, truths=truths, labels=labels, range=[[3, 4.4], [11, 18]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b178de54",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #D6EAF8; border-left: 15px solid #2E86C1;\">\n",
    "    <h1 style=\"line-height:2.5em; margin-left:1em;\">Exercise 3b</h1>\n",
    "</div>\n",
    "\n",
    "This time, let's actually model the correlated noise. Re-define your ``lnprob`` function to accept four parameters (slope, intercept, amplitude, and timescale). If you didn't before, it's a good idea to enforce some priors to keep the parameters within reasonable (and physical) ranges. If any parameter falls outside this range, have ``lnprob`` return negative infinity (i.e., zero probability).\n",
    "\n",
    "You'll probably want to run your chains for a bit longer this time, too. As before, plot some posterior samples for the line, as well as the corner plot. How did you do this time? Is there any bias in your inferred values? How does the variance compare to the previous estimate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d68b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnprob(p):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    m, b, A, l = p\n",
    "    if (m < 0) or (m > 10):\n",
    "        return -np.inf\n",
    "    elif (b < 0) or (b > 30):\n",
    "        return -np.inf\n",
    "    elif (A < 0) or (A > 10):\n",
    "        return -np.inf\n",
    "    elif (l < 0) or (l > 10):\n",
    "        return -np.inf\n",
    "    model = m * t + b\n",
    "    lnlike = ln_gp_likelihood(t, y - model, sigma, A=A, l=l)\n",
    "    return lnlike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9810a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = [4.0, 15.0, 2.0, 1.0]\n",
    "ndim = len(initial)\n",
    "nwalkers = 32\n",
    "p0 = initial + 1e-3 * np.random.randn(nwalkers, ndim)\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob)\n",
    "\n",
    "print(\"Running burn-in...\")\n",
    "p0, _, _ = sampler.run_mcmc(p0, 1500)\n",
    "sampler.reset()\n",
    "\n",
    "print(\"Running production...\")\n",
    "sampler.run_mcmc(p0, 2000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be08c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.errorbar(t, y, yerr=sigma, fmt=\".k\", capsize=0, label=\"data\")\n",
    "\n",
    "# The positions where the prediction should be computed\n",
    "x = np.linspace(0, 10, 500)\n",
    "\n",
    "# Plot 24 posterior samples\n",
    "samples = sampler.flatchain\n",
    "label = \"sample\"\n",
    "for s in samples[np.random.randint(len(samples), size=24)]:\n",
    "    m, b, A, l = s\n",
    "    model = m * x + b\n",
    "    plt.plot(x, model, color=\"#4682b4\", alpha=0.3, label=label)\n",
    "    label = None\n",
    "\n",
    "# Plot the truth\n",
    "plt.plot(x, m_true * x + b_true, \"C1\", label=\"truth\")\n",
    "    \n",
    "plt.ylabel(\"data\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.title(\"fit assuming correlated noise\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1c2816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "labels = [\"slope\", \"intercept\", r\"$A$\", r\"$l$\"]\n",
    "truths = [m_true, b_true, A_true, l_true]\n",
    "corner.corner(sampler.flatchain, truths=truths, labels=labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb3ca49",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #D6EAF8; border-left: 15px solid #2E86C1;\">\n",
    "    <h1 style=\"line-height:2.5em; margin-left:1em;\">Exercise 3c</h1>\n",
    "</div>\n",
    "\n",
    "If you didn't do this already, re-plot the posterior samples on top of the data, but this time draw them from the GP, *conditioned on the data*. How good is the fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d48e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.errorbar(t, y, yerr=sigma, fmt=\".k\", capsize=0, label=\"data\")\n",
    "\n",
    "# The positions where the prediction should be computed\n",
    "x = np.linspace(0, 10, 500)\n",
    "\n",
    "# Plot 24 posterior samples\n",
    "samples = sampler.flatchain\n",
    "label = \"sample\"\n",
    "for s in samples[np.random.randint(len(samples), size=24)]:\n",
    "    m, b, A, l = s\n",
    "    model = m * x + b\n",
    "    gp_mu, gp_S = compute_gp(t, y - (m * t + b), x, sigma=sigma, A=A, l=l)\n",
    "    trend = draw_from_gaussian(gp_mu, gp_S)[0]\n",
    "    plt.plot(x, model + trend, color=\"#4682b4\", alpha=0.3, label=label)\n",
    "    label = None\n",
    "\n",
    "# Plot the truth\n",
    "plt.plot(x, m_true * x + b_true, \"C1\", label=\"truth\")\n",
    "    \n",
    "plt.ylabel(\"data\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.title(\"fit assuming correlated noise\");"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
