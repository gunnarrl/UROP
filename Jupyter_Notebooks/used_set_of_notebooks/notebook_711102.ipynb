{"cells":[{"cell_type":"markdown","id":"5357abe1","metadata":{"id":"5357abe1"},"source":["# A1 Data Curation\n","## Daniel White, DATA 512"]},{"cell_type":"markdown","id":"0e115a92","metadata":{"id":"0e115a92"},"source":["### Overview\n","This Jupyter Notebook shows the code and intermediate outputs used to develop this chart (https://wiki.communitydata.cc/upload/4/48/En-wikipedia_traffic_200801-201709_thompson.png) displaying wikipedia page views over the last decade. The data behind the chart was collected using two different APIs associated with Wikipedia -- Legacy Pagecounts API and Pageviews API. The key difference being that Pagecounts included page views that came from Web crawlers which was phased out in May 2015."]},{"cell_type":"markdown","id":"f03778da","metadata":{"id":"f03778da"},"source":["### Data Acquisition"]},{"cell_type":"markdown","id":"8343f84e","metadata":{"id":"8343f84e"},"source":["This section details the process of collecting data from the APIs and outputting the raw data to JSON files. First, a function and parameters were created to collect the data from the Legacy Pagecounts and Pageviews API."]},{"cell_type":"code","execution_count":null,"id":"d4bfa7a1","metadata":{"id":"d4bfa7a1"},"outputs":[],"source":["#Import required packages\n","import json\n","import requests\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"id":"5ebef52b","metadata":{"id":"5ebef52b"},"outputs":[],"source":["endpoint_legacy = 'https://wikimedia.org/api/rest_v1/metrics/legacy/pagecounts/aggregate/{project}/{access-site}/{granularity}/{start}/{end}'\n","endpoint_pageviews = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/aggregate/{project}/{access}/{agent}/{granularity}/{start}/{end}'\n","\n","\n","params_legacy_d = {\"project\" : \"en.wikipedia.org\",\n","                 \"access-site\" : \"desktop-site\",\n","                 \"granularity\" : \"monthly\",\n","                 \"start\" : \"2007120100\",\n","                # for end use 1st day of month following final month of data\n","                 \"end\" : \"2016080100\"\n","                    }\n","\n","# SAMPLE parameters for getting aggregated current standard pageview data\n","# see: https://wikimedia.org/api/rest_v1/#!/Pageviews_data/get_metrics_pageviews_aggregate_project_access_agent_granularity_start_end\n","params_pageviews_d = {\"project\" : \"en.wikipedia.org\",\n","                    \"access\" : \"desktop\",\n","                    \"agent\" : \"user\",\n","                    \"granularity\" : \"monthly\",\n","                    \"start\" : \"2001010100\",\n","                    # for end use 1st day of month following final month of data\n","                    \"end\" : '2018101000'\n","                        }\n","\n","params_legacy_m = {\"project\" : \"en.wikipedia.org\",\n","                 \"access-site\" : \"mobile-site\",\n","                 \"granularity\" : \"monthly\",\n","                 \"start\" : \"2007120100\",\n","                # for end use 1st day of month following final month of data\n","                 \"end\" : \"2016080100\"\n","                    }\n","\n","# see: https://wikimedia.org/api/rest_v1/#!/Pageviews_data/get_metrics_pageviews_aggregate_project_access_agent_granularity_start_end\n","params_pageviews_ma = {\"project\" : \"en.wikipedia.org\",\n","                    \"access\" : \"mobile-app\",\n","                    \"agent\" : \"user\",\n","                    \"granularity\" : \"monthly\",\n","                    \"start\" : \"2001010100\",\n","                    # for end use 1st day of month following final month of data\n","                    \"end\" : '2018101000'\n","                        }\n","\n","params_pageviews_mw = {\"project\" : \"en.wikipedia.org\",\n","                    \"access\" : \"mobile-web\",\n","                    \"agent\" : \"user\",\n","                    \"granularity\" : \"monthly\",\n","                    \"start\" : \"2001010100\",\n","                    # for end use 1st day of month following final month of data\n","                    \"end\" : '2018101000'\n","                      }\n","headers = {\n","    'User-Agent': 'https://github.com/dwhite105',\n","    'From': 'dkwhite@uw.edu'\n","}\n","def api_call(endpoint,parameters):\n","    call = requests.get(endpoint.format(**parameters), headers=headers)\n","    response = call.json()\n","    return response"]},{"cell_type":"markdown","id":"c94b71c2","metadata":{"id":"c94b71c2"},"source":["With the api_call function, five separate calls are made to the two APIs to collected all data related to desktop and mobile views. The raw data is then outputted into json files which are available in the respository."]},{"cell_type":"code","execution_count":null,"id":"370f5cf8","metadata":{"id":"370f5cf8"},"outputs":[],"source":["legacy_d = api_call(endpoint_legacy, params_legacy_d)\n","legacy_m = api_call(endpoint_legacy, params_legacy_m)\n","pageviews_d = api_call(endpoint_pageviews, params_pageviews_d)\n","pageviews_ma = api_call(endpoint_pageviews, params_pageviews_ma)\n","pageviews_mw = api_call(endpoint_pageviews, params_pageviews_mw)"]},{"cell_type":"code","execution_count":null,"id":"f1d83d8d","metadata":{"id":"f1d83d8d"},"outputs":[],"source":["#Write API calls to json files\n","with open('pagecounts_desktop-site_200712-201608.json', 'w') as outfile1:\n","    json.dump(legacy_d, outfile1)\n","with open('pagecounts_mobile-site_200712-201608.json', 'w') as outfile:\n","    json.dump(legacy_m, outfile)\n","with open('pageviews_desktop_201507-201809.json', 'w') as outfile:\n","    json.dump(pageviews_d, outfile)\n","with open('pageviews_mobile-app_201507-201809.json', 'w') as outfile:\n","    json.dump(pageviews_ma, outfile)\n","with open('pageviews_mobile-web_201507-201809.json', 'w') as outfile:\n","    json.dump(pageviews_mw, outfile)"]},{"cell_type":"markdown","id":"b5d818b9","metadata":{"id":"b5d818b9"},"source":["### Data Processing\n","This section covers the data processing steps taken to take the data from raw json files into a pandas dataframe. The output is then saved as a .csv file. First, two functions are created to parse the data from the pagecount and pageviews API outputs. The parser takes the timestamp and views count and outputs a pandas dataframe containing the year, month, and page views."]},{"cell_type":"code","execution_count":null,"id":"0e3b656d","metadata":{"id":"0e3b656d"},"outputs":[],"source":["def pagecounts_json_parser(json_file):\n","    import json\n","    import pandas as pd\n","    pagecounts_year = []\n","    pagecounts_month = []\n","    pagecounts_count = []\n","    traffic = json_file['items'][0]['access-site']\n","    for i in range(0, len(json_file['items'])):\n","        pagecounts_year.append(json_file['items'][i]['timestamp'][:4])\n","        pagecounts_month.append(json_file['items'][i]['timestamp'][4:6])\n","        pagecounts_count.append(json_file['items'][i]['count'])\n","    pagecounts_df = pd.DataFrame(data = ({'year': pagecounts_year,\n","                                        'month': pagecounts_month,\n","                                        ('pc_count_' + traffic): pagecounts_count}))\n","    return pagecounts_df"]},{"cell_type":"code","execution_count":null,"id":"cb417426","metadata":{"id":"cb417426"},"outputs":[],"source":["def pageviews_json_parser(json_file):\n","    import json\n","    import pandas as pd\n","    pageviews_year = []\n","    pageviews_month = []\n","    pageviews_count = []\n","    traffic = json_file['items'][0]['access']\n","    for i in range(0, len(json_file['items'])):\n","        pageviews_year.append(json_file['items'][i]['timestamp'][:4])\n","        pageviews_month.append(json_file['items'][i]['timestamp'][4:6])\n","        pageviews_count.append(json_file['items'][i]['views'])\n","    pageviews_df = pd.DataFrame(data = ({'year': pageviews_year,\n","                                        'month': pageviews_month,\n","                                        ('pv_count_' + traffic): pageviews_count}))\n","    return pageviews_df"]},{"cell_type":"code","execution_count":null,"id":"0ec6424a","metadata":{"id":"0ec6424a"},"outputs":[],"source":["pagecounts_d_df = pagecounts_json_parser(legacy_d)\n","pagecounts_m_df = pagecounts_json_parser(legacy_m)\n","pageviews_d_df = pageviews_json_parser(pageviews_d)\n","pageviews_ma_df = pageviews_json_parser(pageviews_ma)\n","pageviews_mw_df = pageviews_json_parser(pageviews_mw)"]},{"cell_type":"markdown","id":"334b6f00","metadata":{"id":"334b6f00"},"source":["A sample of the dataframe outputted from these functions is displayed below. Next, the dataframes are merged together on common year and month keys."]},{"cell_type":"code","execution_count":null,"id":"9108e7e8","metadata":{"id":"9108e7e8"},"outputs":[],"source":["pagecounts_d_df.head()"]},{"cell_type":"code","execution_count":null,"id":"0da71b69","metadata":{"id":"0da71b69"},"outputs":[],"source":["pagecounts_df = pd.merge(pagecounts_d_df, pagecounts_m_df, how='outer',on = ['month','year'])\n","pageviews_df = pd.merge(pd.merge(pageviews_d_df, pageviews_ma_df, how = 'outer', on = ['month', 'year']),\n","                        pageviews_mw_df, how = 'outer', on = ['month', 'year'])\n","df = pd.merge(pagecounts_df, pageviews_df, how = 'outer', on = ['month', 'year'])"]},{"cell_type":"code","execution_count":null,"id":"debdf911","metadata":{"id":"debdf911"},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","id":"8125d5d1","metadata":{"id":"8125d5d1"},"source":["Next, some quick data cleaning and processing steps are performed. This includes replacing all the NaN with zeroes, calculating new fields, changing column names, and filtering out unused columns. The resulting dataframe is shown below and is outputted to a .csv file."]},{"cell_type":"code","execution_count":null,"id":"733b0139","metadata":{"id":"733b0139"},"outputs":[],"source":["df.fillna(0, inplace = True)\n","df['pageview_mobile_views'] = df['pv_count_mobile-app'] + df['pv_count_mobile-web']\n","df['pageview_all_views'] = df['pageview_mobile_views'] + df['pv_count_desktop']\n","df['pagecount_all_views'] = df['pc_count_desktop-site'] + df['pc_count_mobile-site']\n","df = df.rename(columns = {'pv_count_desktop': 'pageview_desktop_views',\n","                          'pc_count_mobile-site' : 'pagecount_mobile_views',\n","                          'pc_count_desktop-site': 'pagecount_desktop_views'})\n","\n","df = df[['year',\n","         'month',\n","         'pagecount_all_views',\n","         'pagecount_desktop_views',\n","         'pagecount_mobile_views',\n","         'pageview_all_views',\n","         'pageview_desktop_views',\n","         'pageview_mobile_views']]\n","df.head()"]},{"cell_type":"code","execution_count":null,"id":"5cbacf45","metadata":{"id":"5cbacf45"},"outputs":[],"source":["df.to_csv('en-wikipedia_traffic_200712-201809.csv', index = False)"]},{"cell_type":"markdown","id":"83c27e88","metadata":{"id":"83c27e88"},"source":["### Data Analysis\n","This section shows the steps to replicate the original chart. First, the date and month columns were converted into a datetime object to make a time series chart. The zeroes were replaced with NaNs to make for a cleaner looking chart where data did not exist. Six different columns containing views over time are then plotted on the chart. The chart axes and titles are meant to mimic the original chart. Some key differences in my version include the X and Y axes labels and the timeframe of analysis (data up to 09-2018 instead of 09-2017). The resulting figure is then outputted as a png file."]},{"cell_type":"code","execution_count":null,"id":"8595a0a3","metadata":{"id":"8595a0a3"},"outputs":[],"source":["df['datetime'] =  pd.to_datetime(df['month'] + '-' + df['year'], format=\"%m-%Y\")\n","df = df.replace(0, np.nan)\n","df.head()"]},{"cell_type":"code","execution_count":null,"id":"34ce1fa4","metadata":{"id":"34ce1fa4"},"outputs":[],"source":["fig = plt.figure(figsize=(14,6))\n","ax1 = fig.add_subplot(111)\n","plt.plot(df['datetime'], df['pagecount_desktop_views']/1000000, color = 'green', linestyle = 'dashed')\n","plt.plot(df['datetime'], df['pagecount_mobile_views']/1000000, color = 'blue', linestyle = 'dashed')\n","plt.plot(df['datetime'], df['pagecount_all_views']/1000000, color = 'black', linestyle = 'dashed')\n","plt.plot(df['datetime'], df['pageview_desktop_views']/1000000, color = 'green', linestyle = 'solid')\n","plt.plot(df['datetime'], df['pageview_mobile_views']/1000000, color = 'blue', linestyle = 'solid')\n","plt.plot(df['datetime'], df['pageview_all_views']/1000000, color = 'black', linestyle = 'solid')\n","plt.title(\"Page Views on English Wikipedia (x 1,000,000)\")\n","plt.legend(['main', 'mobile', 'total'], loc = 'upper left')\n","plt.xlabel(\"Year\")\n","plt.ylabel(\"Views (millions)\")\n","plt.xlim('2008-01-01','2018-10-01')\n","plt.ylim(0,12000)\n","plt.grid(True)\n","plt.suptitle(\"May 2015: a new pageview definition took effect, which eliminated all crawler traffic. Solid lines mark new definition.\",\n","             y = '0.01', va = 'bottom', color = 'darkred')\n","plt.savefig('en-wikipedia_traffic_200712-201809.png')\n","plt.show();"]},{"cell_type":"markdown","id":"a856a753","metadata":{"id":"a856a753"},"source":["Observing the chart, you can see when Pagecounts began tracking desktop vs. mobile views towards the end of 2014. Also, the new Pageviews API had fewer views than Pagecounts during the period in which both were active. This can be accounted for by the elimination of views that came from Web crawlers during that time."]}],"metadata":{"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}