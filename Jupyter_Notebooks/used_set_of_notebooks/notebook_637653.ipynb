{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "511ec7fc",
   "metadata": {},
   "source": [
    "\n",
    "MNIST (\"Modified National Institute of Standards and Technology\") is computer vision dataset released in 1999. It contains data of handwritten images and it is the \"de facto\" benchmark for classification algorithms. The goal is to correctly identify digits from a dataset of tens of thousands of handwritten images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fd654f",
   "metadata": {},
   "source": [
    "![](http://corochann.com/wp-content/uploads/2017/02/mnist_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f1e82",
   "metadata": {},
   "source": [
    "# The data\n",
    "\n",
    "The data description can be found at [Kaggle](https://www.kaggle.com/c/digit-recognizer/data):\n",
    "\n",
    "> The data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine.\n",
    "\n",
    "> **Each image is 28 pixels in height and 28 pixels in width**, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. **This pixel-value is an integer between 0 and 255, inclusive**.\n",
    "\n",
    "> The training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n",
    "\n",
    "> Each pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42a0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d33111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc502a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 14, 8\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737650d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = MNIST('./data', train=True, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )\n",
    "\n",
    "test = MNIST('./data', train=False, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )\n",
    "\n",
    "\n",
    "dataloader_args = dict(shuffle=True, batch_size=64,num_workers=1)\n",
    "train_loader = dataloader.DataLoader(train, **dataloader_args)\n",
    "test_loader = dataloader.DataLoader(test, **dataloader_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a9aa12",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a4b8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.train_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ed81bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.test_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c909ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.train_data[0] # .numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc76286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_digit(digit):\n",
    "    plt.imshow(digit.view(28, 28).numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4905d431",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_digit(train.train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82216d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train.train_data\n",
    "train_data = train.transform(train_data.numpy())\n",
    "\n",
    "print('[Train]')\n",
    "print(' - Numpy Shape:', train.train_data.cpu().numpy().shape)\n",
    "print(' - Tensor Shape:', train.train_data.size())\n",
    "print(' - Transformed Shape:', train_data.size())\n",
    "print(' - min:', torch.min(train_data))\n",
    "print(' - max:', torch.max(train_data))\n",
    "print(' - mean:', torch.mean(train_data))\n",
    "print(' - std:', torch.std(train_data))\n",
    "print(' - var:', torch.var(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8bcfcb",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640bf1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = nn.Linear(784, 520)\n",
    "        self.l2 = nn.Linear(520, 320)\n",
    "        self.l3 = nn.Linear(320, 240)\n",
    "        self.l4 = nn.Linear(240, 120)\n",
    "        self.l5 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the data (n, 1, 28, 28)-> (n, 784)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        return self.l5(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a5e536",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b052be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86093ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    e_train_loss = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(data) \n",
    "\n",
    "        loss = criterion(y_pred, target)\n",
    "        e_train_loss.append(loss.data[0])\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    mean_train_loss = np.mean(e_train_loss)\n",
    "    train_loss.append(mean_train_loss)\n",
    "        \n",
    "        \n",
    "        \n",
    "    model.eval()\n",
    "    e_test_loss = []\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = criterion(output, target).data[0]\n",
    "        e_test_loss.append(loss)\n",
    "        \n",
    "        # get the index of the max\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    mean_test_loss = np.mean(e_test_loss)\n",
    "    test_loss.append(mean_test_loss)\n",
    "    \n",
    "    print(f\"\"\"\n",
    "    ######### Epoch {epoch + 1} #########\\n\n",
    "    average train loss: {mean_train_loss}\n",
    "    average test loss: {mean_test_loss}\n",
    "    test accuracy: {correct}/{len(test_loader.dataset)} {100. * correct / len(test_loader.dataset)}%\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e4d7dc",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65c262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "plt.plot(train_loss, \"b\", label=\"Train loss\")\n",
    "\n",
    "plt.plot(test_loss, \"g\", label=\"Test loss\")\n",
    "\n",
    "plt.title(\"Loss change during training\")\n",
    "plt.legend(loc='upper right', shadow=True)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylim(0)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dbe43c",
   "metadata": {},
   "source": [
    "## Extracting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb47d146",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "ys = []\n",
    "y_hats = []\n",
    "\n",
    "for data, target in test_loader:\n",
    "    data= Variable(data, volatile=True)\n",
    "    output = model(data)\n",
    "\n",
    "    pred = output.data.max(1, keepdim=True)[1].numpy().flatten()\n",
    "    \n",
    "    y_hats.extend(pred)\n",
    "    ys.extend(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e3c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(ys, y_hats)\n",
    "\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\"d\")\n",
    "plt.title(\"Digit missclasification matrix\")\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273a96c8",
   "metadata": {},
   "source": [
    "## Classify single digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4ffa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_digit(test.test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2ec19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.test_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad6ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "data= Variable(test.test_data[0].float(), volatile=True)\n",
    "output = model(data)\n",
    "\n",
    "pred = output.data.max(1, keepdim=True)[1]\n",
    "pred"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
