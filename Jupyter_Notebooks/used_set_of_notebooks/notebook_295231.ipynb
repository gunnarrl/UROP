{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b1e2474",
   "metadata": {},
   "source": [
    "# MRI reconstruction from multicoil data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22587ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib notebook\n",
    "\n",
    "import os\n",
    "import sigpy.mri as mr\n",
    "\n",
    "import sigpy as sp\n",
    "import sigpy.mri as mr\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from include import *\n",
    "\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import h5py\n",
    "#from skimage.metrics import structural_similarity as ssim\n",
    "from common.evaluate import *\n",
    "from pytorch_msssim import ms_ssim\n",
    "import pickle\n",
    "from common.subsample import MaskFunc\n",
    "\n",
    "from DIP_UNET_models.skip import *\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim\n",
    "from torch.autograd import Variable\n",
    "#from models import *\n",
    "#from utils.denoising_utils import *\n",
    "\n",
    "# from facebook MRI\n",
    "#import transforms\n",
    "from include import transforms as transform\n",
    "\n",
    "GPU = True\n",
    "if GPU == True:\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    #os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    gpu = 2\n",
    "    torch.cuda.set_device(gpu)\n",
    "    print(\"num GPUs\",torch.cuda.device_count())\n",
    "else:\n",
    "    dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204b5bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_untrained(parnet, num_channels, mask, in_size, slice_ksp, slice_ksp_torchtensor, LR=0.008, num_iter=20000):\n",
    "    ### fixing the scaling (note that it can be done using the under-sampled kspace as well, but we do it using the full kspace)\n",
    "    scale_out = 1\n",
    "    scaling_factor,ni = get_scale_factor(parnet,\n",
    "                                       num_channels,\n",
    "                                       in_size,\n",
    "                                       slice_ksp,\n",
    "                                       scale_out=scale_out)\n",
    "    slice_ksp_torchtensor = slice_ksp_torchtensor * scaling_factor\n",
    "    slice_ksp = slice_ksp * scaling_factor\n",
    "    ### mask the ksapce\n",
    "    masked_kspace, mask = transform.apply_mask(slice_ksp_torchtensor, mask = mask)\n",
    "    unders_measurement = np_to_var( masked_kspace.data.cpu().numpy() ).type(dtype)\n",
    "    sampled_image2 = transform.ifft2(masked_kspace)\n",
    "    \n",
    "    measurement = ksp2measurement(slice_ksp).type(dtype)\n",
    "    lsimg = lsreconstruction(measurement)\n",
    "    \n",
    "    ### fit the network to the under-sampled measurement\n",
    "    out = []\n",
    "    for img in sampled_image2:\n",
    "        out += [ img[:,:,0].numpy() , img[:,:,1].numpy() ]\n",
    "    lsest = torch.tensor(np.array([out]))\n",
    "    scale_out,sover,pover,norm_ratio,par_mse_n, par_mse_t, parni, parnet = fit( in_size = in_size,\n",
    "                                                                num_channels=[num_channels]*(num_layers-1),\n",
    "                                                                num_iter=num_iter,\n",
    "                                                                LR=LR,\n",
    "                                                                mask = mask2d,\n",
    "                                                                apply_f = forwardm,\n",
    "                                                                img_noisy_var=unders_measurement,\n",
    "                                                                net=parnet,\n",
    "                                                                upsample_mode=\"free\",\n",
    "                                                                img_clean_var=Variable(lsest).type(dtype),\n",
    "                                                                #lsimg = lsimg,\n",
    "                                                                find_best=True,\n",
    "                                                                loss_type=\"MSE\",\n",
    "                                                                scale_out=scale_out,\n",
    "                                                                net_input = ni,\n",
    "                                                                OPTIMIZER = \"adam\"\n",
    "                                                                          )\n",
    "    return parnet, parni, slice_ksp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edda3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_consistency(parnet, parni, mask1d, slice_ksp):\n",
    "    img = parnet(parni.type(dtype))\n",
    "    s = img.shape\n",
    "    ns = int(s[1]/2) # number of slices\n",
    "    fimg = Variable( torch.zeros( (s[0],ns,s[2],s[3],2 ) ) ).type(dtype)\n",
    "    for i in range(ns):\n",
    "        fimg[0,i,:,:,0] = img[0,2*i,:,:]\n",
    "        fimg[0,i,:,:,1] = img[0,2*i+1,:,:]\n",
    "    Fimg = transform.fft2(fimg) # dim: (1,num_slices,x,y,2)\n",
    "    # ksp has dim: (num_slices,x,y)\n",
    "    meas = ksp2measurement(slice_ksp) # dim: (1,num_slices,x,y,2)\n",
    "    mask = torch.from_numpy(np.array(mask1d, dtype=np.uint8))\n",
    "    ksp_dc = Fimg.clone()\n",
    "    ksp_dc = ksp_dc.detach().cpu()\n",
    "    ksp_dc[:,:,:,mask==1,:] = meas[:,:,:,mask==1,:] # after data consistency block\n",
    "\n",
    "    img_dc = transform.ifft2(ksp_dc)[0]\n",
    "    out = []\n",
    "    for img in img_dc.detach().cpu():\n",
    "        out += [ img[:,:,0].numpy() , img[:,:,1].numpy() ]\n",
    "\n",
    "    par_out_chs = np.array(out)\n",
    "    #par_out_chs = parnet( parni.type(dtype),scale_out=scale_out ).data.cpu().numpy()[0]\n",
    "    par_out_imgs = channels2imgs(par_out_chs)\n",
    "\n",
    "    prec = crop_center2(root_sum_of_squares2(par_out_imgs),320,320)\n",
    "    return prec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3019cc9",
   "metadata": {},
   "source": [
    "# Loading MRI measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01acdf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get image from the validation set\n",
    "filename = '/hdd/multicoil_val/file1001191.h5' # with fat suppression\n",
    "#filename = '/hdd/multicoil_val/file1000126.h5' # without fat suppression\n",
    "\n",
    "\n",
    "f = h5py.File(filename, 'r') # contains a kspace measurement f['kspace'] and rss reconstruction f['reconstruction_rss']\n",
    "print(\"Kspace shape (number slices, number coils, x, y): \", f['kspace'].shape)\n",
    "\n",
    "# which slice to consider in the following\n",
    "slicenu = f[\"kspace\"].shape[0]//2\n",
    "slice_ksp = f['kspace'][slicenu]\n",
    "slice_ksp_torchtensor = transform.to_tensor(slice_ksp)      # Convert from numpy array to pytorch tensor\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(f[\"reconstruction_rss\"][slicenu],\"gray\")\n",
    "ax.set(title=\"ground truth\")\n",
    "ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7456f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # if the file already has a mask\n",
    "    temp = np.array([1 if e else 0 for e in f[\"mask\"]])\n",
    "    temp = temp[np.newaxis].T\n",
    "    temp = np.array([[temp]])\n",
    "    mask = transform.to_tensor(temp).type(dtype).detach().cpu()\n",
    "except: # if we need to create a mask\n",
    "    desired_factor = 4 # desired under-sampling factor\n",
    "    undersampling_factor = 0\n",
    "    tolerance = 0.03\n",
    "    while undersampling_factor < desired_factor - tolerance or undersampling_factor > desired_factor + tolerance:\n",
    "        mask_func = MaskFunc(center_fractions=[0.07], accelerations=[desired_factor])  # Create the mask function object\n",
    "        masked_kspace, mask = transform.apply_mask(slice_ksp_torchtensor, mask_func=mask_func)   # Apply the mask to k-space\n",
    "        mask1d = var_to_np(mask)[0,:,0]\n",
    "        undersampling_factor = len(mask1d) / sum(mask1d)\n",
    "\n",
    "mask1d = var_to_np(mask)[0,:,0]\n",
    "\n",
    "# The provided mask and data have last dim of 368, but the actual data is smaller.\n",
    "# To prevent forcing the network to learn outside the data region, we force the mask to 0 there.\n",
    "mask1d[:mask1d.shape[-1]//2-160] = 0 \n",
    "mask1d[mask1d.shape[-1]//2+160:] =0\n",
    "mask2d = np.repeat(mask1d[None,:], slice_ksp.shape[1], axis=0).astype(int) # Turning 1D Mask into 2D that matches data dimensions\n",
    "mask2d = np.pad(mask2d,((0,),((slice_ksp.shape[-1]-mask2d.shape[-1])//2,)),mode='constant') # Zero padding to make sure dimensions match up\n",
    "mask = transform.to_tensor( np.array( [[mask2d[0][np.newaxis].T]] ) ).type(dtype).detach().cpu()\n",
    "print(\"under-sampling factor:\",round(len(mask1d)/sum(mask1d),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1a40cb",
   "metadata": {},
   "source": [
    "### Setup and fit ConvDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e88bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_depth = slice_ksp.shape[0]*2\n",
    "out_size = slice_ksp.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3d3edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_name = \"ConvDecoder\"\n",
    "###\n",
    "num_channels = 160 #256\n",
    "num_layers = 8\n",
    "strides = [1]*(num_layers-1)\n",
    "in_size = [8,4]\n",
    "kernel_size = 3\n",
    "net = convdecoder(out_size,in_size,output_depth,\n",
    "                     num_layers,strides,num_channels, act_fun = nn.ReLU(),\n",
    "                     skips=False,need_sigmoid=False,bias=False, need_last = True,\n",
    "                     kernel_size=kernel_size,upsample_mode=\"nearest\").type(dtype)\n",
    "print(\"#prameters of {}:\".format(arch_name),num_param(net))\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5988a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "net,ni,slice_ksp_cd = fit_untrained(net, num_channels, mask, in_size, slice_ksp, slice_ksp_torchtensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a525079",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_convD = data_consistency(net, ni, mask1d, slice_ksp_cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b178519c",
   "metadata": {},
   "source": [
    "### Setup and fit Deep Decoder (DD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a603bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### delete cashe\n",
    "del(net,ni)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce9c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_name = \"DD\"\n",
    "###\n",
    "num_channels = 368\n",
    "num_layers = 10\n",
    "in_size = [16,16]\n",
    "\n",
    "net = skipdecoder(out_size,in_size,output_depth,\n",
    "                   num_layers,num_channels,skips=False,need_last=True,\n",
    "                   need_sigmoid=False,upsample_mode=\"bilinear\").type(dtype)\n",
    "print(\"#prameters of {}:\".format(arch_name),num_param(net))\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caed688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net,ni,slice_ksp_cd = fit_untrained(net, num_channels, mask, in_size, slice_ksp, slice_ksp_torchtensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbc6dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_DD = data_consistency(net, ni, mask1d, slice_ksp_cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f425ac2",
   "metadata": {},
   "source": [
    "### Setup and fit Deep Image Prior (DIP) (encoder-decoder style architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3c317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### delete cashe\n",
    "del(net,ni)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873c333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_name = \"DIP\"\n",
    "### \n",
    "in_size = slice_ksp.shape[-2:]\n",
    "pad = \"zero\" #'reflection' # 'zero'\n",
    "num_channels = 256\n",
    "net = skip(in_size,num_channels, output_depth, \n",
    "           num_channels_down = [num_channels] * 8,\n",
    "           num_channels_up =   [num_channels] * 8,\n",
    "           num_channels_skip =    [num_channels*0] * 6 + [4,4],  \n",
    "           filter_size_up = 3, filter_size_down = 5, \n",
    "           upsample_mode='nearest', filter_skip_size=1,\n",
    "           need_sigmoid=False, need_bias=True, pad=pad, act_fun='ReLU').type(dtype)\n",
    "print(\"#prameters of {}:\".format(arch_name),num_param(net))\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231855d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "net,ni,slice_ksp_cd = fit_untrained(net, num_channels, mask, in_size, slice_ksp, slice_ksp_torchtensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e05746",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_DIP = data_consistency(net, ni, mask1d, slice_ksp_cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62641cc3",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27ce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(im1,im2):\n",
    "    im1 = (im1-im1.mean()) / im1.std()\n",
    "    im1 *= im2.std()\n",
    "    im1 += im2.mean()\n",
    "    \n",
    "    vif_ = vifp_mscale(im1,im2,sigma_nsq=im1.mean())\n",
    "    \n",
    "    ssim_ = ssim(np.array([im1]), np.array([im2]))\n",
    "    psnr_ = psnr(np.array([im1]),np.array([im2]))\n",
    "\n",
    "    dt = torch.FloatTensor\n",
    "    im11 = torch.from_numpy(np.array([[im1]])).type(dt)\n",
    "    im22 = torch.from_numpy(np.array([[im2]])).type(dt)\n",
    "    ms_ssim_ = ms_ssim(im11, im22,data_range=im22.max()).data.cpu().numpy()[np.newaxis][0]\n",
    "    return vif_, ms_ssim_, ssim_, psnr_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67bae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = f[\"reconstruction_rss\"][slicenu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c55a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_cd, ms_ssim_cd, ssim_cd, psnr_cd  = scores(gt, rec_convD)\n",
    "vif_dd, ms_ssim_dd, ssim_dd, psnr_dd  = scores(gt, rec_DD)\n",
    "vif_dip, ms_ssim_dip, ssim_dip, psnr_dip  = scores(gt, rec_DIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3893be1",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830c83eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (16,14)) # create a 5 x 5 figure \n",
    "    \n",
    "ax1 = fig.add_subplot(221)\n",
    "ax1.imshow(gt,cmap='gray')\n",
    "ax1.set_title('Ground Truth')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax2.imshow(rec_convD,cmap='gray')\n",
    "ax2.set_title( \"ConvDecoder\") \n",
    "ax2.axis('off') \n",
    "\n",
    "ax3 = fig.add_subplot(223)\n",
    "ax3.imshow(rec_DD,cmap='gray')\n",
    "ax3.set_title( \"Deep Decoder\" ) \n",
    "ax3.axis('off')\n",
    "\n",
    "ax4 = fig.add_subplot(224)\n",
    "ax4.imshow(rec_DIP,cmap='gray')\n",
    "ax4.set_title( \"Deep Image Prior\" ) \n",
    "ax4.axis('off')\n",
    "\n",
    "print(\"ConvDecoder       --> VIF: %.2f, MS-SSIM: %.2f, SSIM: %.2f, PSNR: %.2f \" % (vif_cd,ms_ssim_cd,ssim_cd,psnr_cd))\n",
    "print(\"Deep Decoder      --> VIF: %.2f, MS-SSIM: %.2f, SSIM: %.2f, PSNR: %.2f \" % (vif_dd,ms_ssim_dd,ssim_dd,psnr_dd))\n",
    "print(\"Deep Image Prior  --> VIF: %.2f, MS-SSIM: %.2f, SSIM: %.2f, PSNR: %.2f \" % (vif_dip,ms_ssim_dip,ssim_dip,psnr_dip))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
