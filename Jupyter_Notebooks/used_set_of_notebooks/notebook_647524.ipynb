{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "import pandas as pd\n",
    "import requests, sys, webbrowser,xml\n",
    "import numpy as np\n",
    "import bs4\n",
    "import re\n",
    "import random\n",
    "from urllib.error import HTTPError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fad730a",
   "metadata": {},
   "source": [
    "### Pipeline usage to perform test generation, using the hugging face package transformers. Citation is below:\n",
    "@article{Wolf2019HuggingFacesTS,\n",
    "  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},\n",
    "  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},\n",
    "  journal={ArXiv},\n",
    "  year={2019},\n",
    "  volume={abs/1910.03771}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09cf131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting up pipeline\n",
    "def start_pipeline():\n",
    "    generator = pipeline('fill-mask', model='bert-base-uncased')\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4debea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = start_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c06a608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a number letters and some sequence, returns the sequence with generated attached words\n",
    "def generate_text(generator,intro_sequence:str, num_words = 5)->str:\n",
    "    # Imported random text generation\n",
    "    text = generator(intro_sequence, max_length=len(intro_sequence) + num_words, num_return_sequences=1)[0].get(\"generated_text\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca2fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(url = \"https://www.keepinspiring.me/famous-quotes/\" ):\n",
    "    # Requesting data from url, finding specialized tags for this particular website\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    soup = bs4.BeautifulSoup(res.text, \"html.parser\")\n",
    "    text  = soup.find_all(\"div\", class_ = 'author-quotes')  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ef8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dcdd8a",
   "metadata": {},
   "source": [
    "## Game List: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3ba6ec",
   "metadata": {},
   "source": [
    "*Format: select the game -> select the text type*\n",
    "\n",
    "1) **Mad Libs** \n",
    "<ul>\n",
    "    <li>Favorite song Edition</li>\n",
    "    <li>Wikipedia edition</li>\n",
    "    <li>All time famous quotes edition</li>\n",
    "    <li>Generate a normal mad-lib using sentiment analysis to get parts of speech, tell user in random 'noun,verb, etc.'</li>\n",
    "</ul>\n",
    "\n",
    "2) **Find the real text**\n",
    "<ul>\n",
    "    <li>Favorite song Edition</li>\n",
    "    <li>Wikipedia edition</li>\n",
    "    <li>All time famous quotes edition</li>\n",
    "</ul>\n",
    "\n",
    "3) **How well do you know your favorite song?**\n",
    "\n",
    "4) **Can you guess what Wikipedia page this used to be?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b02f475",
   "metadata": {},
   "source": [
    "## Word Processing Below\n",
    "Scraping the data from our quote website and cleaning it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef367bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtainin author from quote\n",
    "def authors(quote):\n",
    "    return quote.split(\"”\")[1]\n",
    "\n",
    "#Removing author and adding lost smartquote\n",
    "def remove_authors(quote):\n",
    "    return (quote.split(\"”\")[0] + (\"”\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f551e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tag processing functions to clean up nasty html formatting, replaces div tags\n",
    "def processing_div(tag):\n",
    "    return tag.replace('<div class=\"author-quotes\">', \"\").replace(\"</div>\", \"\")\n",
    "\n",
    "#span processing, replaces span tag\n",
    "def processing_span(tag):\n",
    "    return tag.replace(\"<span class=\\\"quote-author-name\\\">\", \"\").replace(\"</span>\", \"\")\n",
    "\n",
    "# Checks for tags that have yet to be removed, not given standard format. Reasoning - we don't know when ads will pop up\n",
    "def cleaner(table):\n",
    "    arr = np.array([])\n",
    "    for i in table.get(\"quote\"):\n",
    "        arr = np.append(arr,(\"<\" in i))\n",
    "    clean = table[(arr != 1)]\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30168a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame().assign(quote = text)\n",
    "\n",
    "#Formatting, processing, and splitting quotes and authors\n",
    "def table_process(table):\n",
    "    table = table.assign(quote = table.get(\"quote\").apply(str))\n",
    "    table = table.assign(quote = table.get(\"quote\").apply(processing_div).apply(processing_span))\n",
    "    table = cleaner(table)\n",
    "    table = table.assign(author  = table.get(\"quote\").apply(authors), quote = table.get(\"quote\").apply(remove_authors))\n",
    "    return table\n",
    "table = table_process(table)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7032ec29",
   "metadata": {},
   "source": [
    "## Random Word Replacement to be used in each individual Turing game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c69331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_words_at_random(generator, word_arr: [str], difficulty = 1) -> ([str],[int]):\n",
    "    num_words  = int(np.ceil((len(word_arr) * (1/(2*difficulty)))))\n",
    "    #  Generate num_words random indices\n",
    "    indices = random_generation(num_words, 0, len(word_arr))\n",
    "    print(indices)\n",
    "    \n",
    "    # Place word masks at each of the randomly chosen indices\n",
    "    # Fill in each mask with the language model\n",
    "    for i in indices:\n",
    "        #Ensuring array is not overaccessed\n",
    "        if(i < len(word_arr)):\n",
    "            word_arr[i] = '[MASK]'\n",
    "        \n",
    "        #Adding together everything leading up to string so as to add some context into the model\n",
    "        join  = \" \".join(word_arr)\n",
    "\n",
    "        # Generate the next word\n",
    "        text = (generator(join)[5-difficulty].get('sequence').replace(\"[CLS]\", '').replace('[SEP]', '')).strip()\n",
    "        word_arr = text.split(\" \")\n",
    "\n",
    "    return text, indices \n",
    "\n",
    "# Generates a desired number of unique random digits in a certain range\n",
    "def random_generation(num_words, lowNum, highNum):\n",
    "    random_digits = np.unique(np.random.randint(low = lowNum, high  = highNum, size = num_words))\n",
    "    while(len(random_digits) < num_words):\n",
    "        random_digits = np.append(random_digits, np.random.randint(low = lowNum, high = highNum, size = num_words-len(random_digits)))\n",
    "        random_digits = np.unique(random_digits)\n",
    "    return random_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a61a423",
   "metadata": {},
   "source": [
    "## Wikipedia Search Below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4c8a17",
   "metadata": {},
   "source": [
    "Start at user given wikipedia page, randomly click a certain number of links from there and scrape 5 sentences from the final page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12217cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtains text from a wikipedia url\n",
    "def get_wiki_text(url):\n",
    "    # Requesting data from url, finding specialized tags for this particular website\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    #Attaching soup object to page text, obtaining text in paragraphs\n",
    "    soup = bs4.BeautifulSoup(res.text, \"lxml\")\n",
    "    text = \"\"\n",
    "    \n",
    "    # Problematic structure: fails to look for list items which make up substantial amount of wikipedia pages\n",
    "    for paragraph in soup.find_all('p'):\n",
    "        text+= paragraph.text\n",
    "        \n",
    "    # Formatting the string so that it looks normal\n",
    "    text = re.sub(r'\\[.*\\]', '', text)\n",
    "    text= re.sub(r'\\s+', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca9b474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterates through wikipedia pages\n",
    "def wiki_search(topic, pages):\n",
    "    textSoup, title = looping_wiki_search(topic, int(pages))\n",
    "    return textSoup, title \n",
    "\n",
    "# Randomly selecting the next topic to be searched for\n",
    "def select_next_topic(text: [str])-> str:\n",
    "    random_number = np.random.randint(1,len(text))\n",
    "    return text[random_number]\n",
    "\n",
    "# Replaces hyphens with an underscore for url purposes, removes all punctuation that could break url\n",
    "def replace_punctuation(text):\n",
    "    specific_case = text.strip()\n",
    "    specific_case = specific_case.replace('-',' ')\n",
    "    specific_case = specific_case.replace(' ', '_')\n",
    "    \n",
    "    # Remove all punctuation\n",
    "    pattern = re.compile(r'\\W')\n",
    "    specific_case = re.sub(pattern, '', specific_case)\n",
    "    \n",
    "    return specific_case\n",
    "    \n",
    "    \n",
    "# Loop through connected topics on wikipedia to find a \"landing page\", then return the text of that landing page. By default, returns the page of 'topic'\n",
    "def looping_wiki_search(topic: str, neighbor_pages = 0):\n",
    "    searchText = \"\"\n",
    "    url = construct_wiki_url(topic)\n",
    "    original = construct_wiki_url(topic)\n",
    "    for i in np.arange(neighbor_pages+1):\n",
    "        print(i)\n",
    "        \n",
    "        #Putting in a try-catch in the case that we have reached a nonexistent page, taking back to original which we\n",
    "        # Know actually exists\n",
    "        try:\n",
    "            searchText = get_wiki_text(url).split(\" \")\n",
    "        except requests.HTTPError:\n",
    "            searchText = get_wiki_text(original).split(\" \")\n",
    "        \n",
    "        \n",
    "        #So long as there are still pages left to proccess\n",
    "        if(i < neighbor_pages):\n",
    "            \n",
    "            # Selecting next topic\n",
    "            topic = select_next_topic(searchText)\n",
    "\n",
    "            # Replacing the punctuation of the next topic\n",
    "            topic = replace_punctuation(topic)\n",
    "\n",
    "            # Moving to next URL\n",
    "            url = construct_wiki_url(topic)\n",
    "        \n",
    "    return searchText, topic\n",
    "\n",
    "\n",
    "# Takes a given user topic and constructs a valid wikipedia url\n",
    "def construct_wiki_url(url_topic : str):\n",
    "    return ('https://en.wikipedia.org/wiki/' + url_topic.lower().strip().replace(\" \",\"_\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907cbeb9",
   "metadata": {},
   "source": [
    "## Last Scrape for Song Game: Scraping Genius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae84dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lyricsgenius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf50765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes artist and song name in terms of string and obtains lyrics\n",
    "def obtain_lyrics(artist: str, song: str) -> [str]:\n",
    "    genius = lyricsgenius.Genius(\"NDltUrlbSis8n9o1FEyGUE_ruIlngdDmXwoQdvrkX0hh3le3LKF8XalcHXOetm3x\")\n",
    "    artist = genius.search_artist(artist, max_songs=3, sort=\"title\")\n",
    "    song = genius.search_song(song, artist.name)\n",
    "    song_lyrics =  song.lyrics\n",
    "    \n",
    "    #String proccessing\n",
    "    song_lyrics = re.sub(r'\\[.*\\]', '', song_lyrics)\n",
    "    song_lyrics  = re.sub(r'\\n+', ' ', song_lyrics)\n",
    "    song_lyrics = song_lyrics.split(\" \")\n",
    "    \n",
    "    #Making sure songs do not exceed maximum threshold\n",
    "    if(len(song_lyrics) > 510):\n",
    "        song_lyrics = song_lyrics[:350]\n",
    "    return song_lyrics\n",
    "\n",
    "# Master Controls obtaining and proccessing lyrics\n",
    "# To do: preserve tags, ie [verse 1], and keep word replacement model from touching it \n",
    "def lyrics(artistName, songName):\n",
    "    lyrics = obtain_lyrics(artistName, songName) \n",
    "    return lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338232ae",
   "metadata": {},
   "source": [
    "# Madlibs games gameplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a078fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the madlibs game, song edition and returns the resulting text (replace words and return funny texts)\n",
    "def madlibs_song_gameplay(artist_name: str, song_name: str, level: int, nlp) -> str:\n",
    "    # Some songs are too long, so expect this and auto replace with Africa by Toto\n",
    "    lyrics = obtain_lyrics(artist_name, song_name)\n",
    "    # We only need to return modifications \n",
    "    try:\n",
    "        text, indices = replace_words_at_random(nlp, lyrics, level)\n",
    "    except RuntimeError:\n",
    "        print(len(lyrics))\n",
    "        lyrics = obtain_lyrics(\"Toto\", \"Africa\")\n",
    "        text, indices = replace_words_at_random(nlp, lyrics, level)    \n",
    "    return text.replace(\"\\'\", \"\")\n",
    "\n",
    "# Runs the madlibs wikipedia version (returns messed up text)\n",
    "def madlibs_wiki_gameplay(topic: str, pages: int, level: int, nlp) -> str:\n",
    "    text, title = wiki_search(topic, pages)\n",
    "    \n",
    "    # Insurance against text too big for language model\n",
    "    if(len(text) > 350):\n",
    "        text = text[:350]\n",
    "    new_text, indices = replace_words_at_random(nlp, text, level)\n",
    "    return new_text\n",
    "\n",
    "# Runs the madlibs famous quotes version (returns messed up text)\n",
    "def madlibs_quotes_gameplay(nlp, level: int) -> str:\n",
    "    # Scraping quotes website to obtain our text\n",
    "    text = get_text()\n",
    "    table = pd.DataFrame().assign(quote = text)\n",
    "    table = table_process(table)\n",
    "    \n",
    "    #Selecting a random quote\n",
    "    quotenum = np.random.randint(table.shape[0])\n",
    "    row  = table.iloc[quotenum]\n",
    "    quote = row.get(\"quote\").split(\" \")\n",
    "    author = row.get(\"author\")\n",
    "    # Randomly replacing words\n",
    "    new_quote, indices = replace_words_at_random(nlp, quote, level)\n",
    "    return new_quote + \" \" +  author"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611f02d0",
   "metadata": {},
   "source": [
    "# Guess the Text Gameplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0edc3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the madlibs game, song edition and returns the resulting text (replace words and return funny texts)\n",
    "def guessText_song_gameplay(artist_name: str, song_name: str, level: int, nlp) -> str:\n",
    "    # Some songs are too long, so expect this and auto replace with Africa by Toto\n",
    "    lyrics = obtain_lyrics(artist_name, song_name)\n",
    "    ret = \" \".join(lyrics)\n",
    "    # We only need to return modifications \n",
    "    try:\n",
    "        text, indices = replace_words_at_random(nlp, lyrics, level)\n",
    "    except RuntimeError:\n",
    "        print(len(lyrics))\n",
    "        lyrics = obtain_lyrics(\"Toto\", \"Africa\")\n",
    "        text, indices = replace_words_at_random(nlp, lyrics, level)    \n",
    "    return text.replace(\"\\'\", \"\"), ret\n",
    "\n",
    "# Runs the madlibs famous quotes version (returns messed up text)\n",
    "def guessText_quotes_gameplay(nlp, level: int) -> str:\n",
    "    # Scraping quotes website to obtain our text\n",
    "    text = get_text()\n",
    "    table = pd.DataFrame().assign(quote = text)\n",
    "    table = table_process(table)\n",
    "    \n",
    "    #Selecting a random quote\n",
    "    quotenum = np.random.randint(table.shape[0])\n",
    "    row  = table.iloc[quotenum]\n",
    "    quote = row.get(\"quote\").split(\" \")\n",
    "    author = row.get(\"author\")\n",
    "    # Randomly replacing words\n",
    "    new_quote, indices = replace_words_at_random(nlp, quote, level)\n",
    "    return new_quote + \" \" +  author\n",
    "\n",
    "# Runs the madlibs wikipedia version (returns messed up text)\n",
    "def guessText_wiki_gameplay(topic: str, pages: int, level: int, nlp) -> str:\n",
    "    text, title = wiki_search(topic, pages)\n",
    "    ret = \" \".join(text)\n",
    "    # Insurance against text too big for language model\n",
    "    if(len(text) > 350):\n",
    "        text = text[:350]\n",
    "    new_text, indices = replace_words_at_random(nlp, text, level)\n",
    "    return new_text, ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab92c8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the madlibs wikipedia version (returns messed up text)\n",
    "def guess_original_wiki_gameplay(topic: str, pages: int, level: int, nlp) -> str:\n",
    "    text, title = wiki_search(topic, pages)\n",
    "    # Insurance against text too big for language model\n",
    "    if(len(text) > 350):\n",
    "        text = text[:350]\n",
    "    new_text, indices = replace_words_at_random(nlp, text, level)\n",
    "    return new_text, title\n",
    "\n",
    "# Runs the madlibs game, song edition and returns the resulting text (replace words and return funny texts)\n",
    "def favorite_song_gameplay(artist_name: str, song_name: str, level: int, nlp) -> str:\n",
    "    # Some songs are too long, so expect this and auto replace with Africa by Toto\n",
    "    lyrics = obtain_lyrics(artist_name, song_name)\n",
    "    # We only need to return modifications \n",
    "    try:\n",
    "        text, indices = replace_words_at_random(nlp, lyrics, level)\n",
    "    except RuntimeError:\n",
    "        lyrics = obtain_lyrics(\"Toto\", \"Africa\")\n",
    "        text, indices = replace_words_at_random(nlp, lyrics, level)    \n",
    "    return text.replace(\"\\'\", \"\"), indices"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
