{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb35dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from hrvanalysis import get_time_domain_features, get_csi_cvi_features\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import feature_extractor\n",
    "import model_evaluation\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91960373",
   "metadata": {},
   "source": [
    "# 1. Explore data\n",
    "- Record indicating difficulty in high accuracy: b02 hour 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66f37ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour = 3\n",
    "cwt, apn, group = feature_extractor.extract_cwt(\n",
    "    file='b02', fs_new=1, smooth=True, cwt_width=40, \n",
    "    diagPlot=True, xlm=[150, 180]) #[60 * (hour - 1), 60 * hour])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2673742",
   "metadata": {},
   "source": [
    "# 2. Determine frequency threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeadbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_features(file_names):\n",
    "    df = pd.DataFrame()\n",
    "    fs_new = 2.4 # optimized from hyper-parameter tuning\n",
    "    \n",
    "    for file in file_names:\n",
    "        print(file)\n",
    "        with open(f'../data/raw/{file}.pkl', 'rb') as f:\n",
    "            res = pickle.load(f)\n",
    "            hr = res['hr']\n",
    "            t_hr = res['t'] # in minute\n",
    "            apn = res['apn']\n",
    "            group = file[0].upper() \n",
    "            \n",
    "        t_hr, hr_smth = feature_extractor.smooth_hr(t_hr, hr)\n",
    "        \n",
    "        # Resample data for frequency-domain analysis\n",
    "        t_interp = np.arange(t_hr[0], t_hr[-1], 1 / fs_new / 60)\n",
    "        hr_interp = np.interp(t_interp, t_hr, hr_smth)\n",
    "        \n",
    "        # Extract features from each segment\n",
    "        for minute in range(len(apn) - 4):\n",
    "            fea_dict = {}\n",
    "            idx_1min = (t_hr > minute + 2) & (t_hr < minute + 3)\n",
    "            idx_5min = (t_hr > minute) & (t_hr < minute + 5)\n",
    "            data_1min, data_5min = hr_smth[idx_1min], hr_smth[idx_5min]\n",
    "            \n",
    "            hr_interp_1min = hr_interp[(t_interp > minute + 2) & (t_interp < minute + 3)]\n",
    "            hr_interp_5min = hr_interp[(t_interp > minute) & (t_interp < minute + 5)]\n",
    "            \n",
    "            # Discard segment if less than 30 heart beats detected\n",
    "            if len(data_1min) < 30: \n",
    "                continue\n",
    "                \n",
    "            # Frequency-domain features\n",
    "            freq, psd = signal.periodogram(hr_interp_5min, fs=fs_new)\n",
    "            psd[freq > 0.1] = 0\n",
    "            \n",
    "            # Label information\n",
    "            fea_dict.update({\n",
    "                'apn': apn[minute + 2],\n",
    "                'group': group,\n",
    "                'file': file,\n",
    "                'psd': psd,\n",
    "                'freq': freq\n",
    "            })\n",
    "            df = df.append(fea_dict, ignore_index=True)\n",
    "                    \n",
    "    df['apn'] = df['apn'].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a0047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../resources/File_train.csv')\n",
    "df = freq_features(train_df['file'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad91713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal freq threshold for data_5min\n",
    "for thres in np.arange(0.005, 0.03, 0.0025):\n",
    "    for idx in range(len(df)):\n",
    "        psd = df.loc[idx, 'psd']\n",
    "        freq = df.loc[idx, 'freq']\n",
    "        df.loc[idx, 'peak'] = psd.max()\n",
    "        df.loc[idx, 'f_peak'] = freq[np.argmax(psd)]\n",
    "        df.loc[idx, 'area_total'] = psd.sum()\n",
    "        df.loc[idx, 'area_lf'] = psd[freq < thres].sum()\n",
    "        df.loc[idx, 'area_hf'] = psd[freq > thres].sum(),\n",
    "        df.loc[idx, 'area_ratio'] = psd[freq > thres].sum() / psd[freq < thres].sum()\n",
    "        \n",
    "    feature_col = ['peak', 'f_peak', 'area_total', 'area_lf', 'area_hf', 'area_ratio']\n",
    "    logreg = LogisticRegression(solver='lbfgs', max_iter=1e6)\n",
    "    acc_train, acc_val, _ = model_evaluation.model_evaluation_CV(logreg, df, train_df, feature_col, normalize=True, n=4)\n",
    "    print(f'Thres={thres}: {acc_train:.3f} for training, {acc_val:.3f} for validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c05fc6",
   "metadata": {},
   "source": [
    "# 3. Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79396070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file):\n",
    "    fs_new = 2.4 # optimized from hyper-parameter tuning\n",
    "    thres = 0.015\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    with open(f'../data/raw/{file}.pkl', 'rb') as f:\n",
    "        res = pickle.load(f)\n",
    "        hr = res['hr']\n",
    "        t_hr = res['t'] # in minute\n",
    "        apn = res['apn']\n",
    "        group = file[0].upper() \n",
    "\n",
    "    t_hr, hr_smth = feature_extractor.smooth_hr(t_hr, hr)\n",
    "\n",
    "    # Resample data for frequency-domain analysis\n",
    "    t_interp = np.arange(t_hr[0], t_hr[-1], 1 / fs_new / 60)\n",
    "    hr_interp = np.interp(t_interp, t_hr, hr_smth)\n",
    "\n",
    "    # Extract features from each segment\n",
    "    for minute in range(len(apn) - 4):\n",
    "        fea_dict = {}\n",
    "        idx_1min = (t_hr > minute + 2) & (t_hr < minute + 3)\n",
    "        idx_5min = (t_hr > minute) & (t_hr < minute + 5)\n",
    "        data_1min, data_5min = hr_smth[idx_1min], hr_smth[idx_5min]\n",
    "\n",
    "        hr_interp_1min = hr_interp[(t_interp > minute + 2) & (t_interp < minute + 3)]\n",
    "        hr_interp_5min = hr_interp[(t_interp > minute) & (t_interp < minute + 5)]\n",
    "\n",
    "        # Discard segment if less than 30 heart beats detected\n",
    "        if len(data_1min) < 30: \n",
    "            continue\n",
    "\n",
    "        # Time-domain features for data_1min\n",
    "        md = np.median(data_1min)\n",
    "        fea_dict.update({\n",
    "            'md_1min': md,\n",
    "            'min_r_1min': data_1min.min() - md,\n",
    "            'max_r_1min': data_1min.max() - md,\n",
    "            'p25_r_1min': np.percentile(data_1min, 0.25) - md,\n",
    "            'p75_r_1min': np.percentile(data_1min, 0.75) - md,\n",
    "            'mean_r_1min': data_1min.mean() - md,\n",
    "            'std_1min': data_1min.std(),\n",
    "            'acf1_1min': pd.Series(hr_interp_1min).autocorr(12),\n",
    "            'acf2_1min': pd.Series(hr_interp_1min).autocorr(24),\n",
    "        })\n",
    "\n",
    "        # Time-domain features for data_5min\n",
    "        md = np.median(data_5min)\n",
    "        fea_dict.update({\n",
    "            'md_5min': md,\n",
    "            'min_r_5min': data_5min.min() - md,\n",
    "            'max_r_5min': data_5min.max() - md,\n",
    "            'p25_r_5min': np.percentile(data_5min, 0.25) - md,\n",
    "            'p75_r_5min': np.percentile(data_5min, 0.75) - md,\n",
    "            'mean_r_5min': data_5min.mean() - md,\n",
    "            'std_5min': data_5min.std(),\n",
    "            'acf1_5min': pd.Series(hr_interp_5min).autocorr(12),\n",
    "            'acf2_5min': pd.Series(hr_interp_5min).autocorr(24),\n",
    "        })\n",
    "\n",
    "        # Heart rate variability for data_1min\n",
    "        nn_intervals = (np.diff(t_hr[idx_1min]) * 1000 * 60).astype(int) # Unit in ms\n",
    "        time_domain_features = get_time_domain_features(nn_intervals)\n",
    "        time_domain_features = {f'{key}_1min': value for key, value in time_domain_features.items()}\n",
    "        nonlinear_features = get_csi_cvi_features(nn_intervals)\n",
    "        nonlinear_features = {f'{key}_1min': value for key, value in nonlinear_features.items()}\n",
    "        fea_dict.update(time_domain_features)\n",
    "        fea_dict.update(nonlinear_features)\n",
    "\n",
    "        # Heart rate variability for data_5min\n",
    "        nn_intervals = (np.diff(t_hr[idx_5min]) * 1000 * 60).astype(int) # Unit in ms\n",
    "        time_domain_features = get_time_domain_features(nn_intervals)\n",
    "        time_domain_features = {f'{key}_5min': value for key, value in time_domain_features.items()}\n",
    "        nonlinear_features = get_csi_cvi_features(nn_intervals)\n",
    "        nonlinear_features = {f'{key}_5min': value for key, value in nonlinear_features.items()}\n",
    "        fea_dict.update(time_domain_features)\n",
    "        fea_dict.update(nonlinear_features)\n",
    "\n",
    "        # Frequency-domain features\n",
    "        freqs, psd = signal.periodogram(hr_interp_5min, fs=fs_new)\n",
    "        psd[freqs > 0.1] = 0\n",
    "        fea_dict.update({\n",
    "            'peak': psd.max(),\n",
    "            'f_peak': freqs[np.argmax(psd)],\n",
    "            'area_total': psd.sum(),\n",
    "            'area_lf': psd[freqs < thres].sum(),\n",
    "            'area_hf': psd[freqs > thres].sum(),\n",
    "            'area_ratio': psd[freqs > thres].sum() / psd[freqs < thres].sum(),\n",
    "        })\n",
    "\n",
    "        # Label information\n",
    "        fea_dict.update({\n",
    "            'apn': apn[minute + 2],\n",
    "            'group': group,\n",
    "            'file': file,\n",
    "        })\n",
    "        df = df.append(fea_dict, ignore_index=True)\n",
    "                    \n",
    "    df['apn'] = df['apn'].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7c877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_all = pd.read_csv('../resources/File_all.csv')\n",
    "for file in file_all['file']:\n",
    "    print(file)\n",
    "    df = extract_features(file)\n",
    "    df.dropna(inplace=True)\n",
    "    df.to_csv(f'../data/feature/{file}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0367c506",
   "metadata": {},
   "source": [
    "# 4. Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a0c550",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_train = pd.read_csv('../resources/File_train.csv')\n",
    "file_train\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for file in file_train['file']:\n",
    "    df = df.append(pd.read_csv(f'../data/feature/{file}.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9316a25",
   "metadata": {},
   "source": [
    "## 4.1 Features with high correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e78984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "df_temp = df.drop(['apn', 'file', 'group'], axis=1)\n",
    "df_temp = (df_temp - df_temp.mean()) / df_temp.std()\n",
    "\n",
    "# Calculate correlations\n",
    "corr = df_temp.corr()\n",
    "corr = corr.where(np.tril(np.ones(corr.shape), k=-1).astype(np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98508f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(16, 14))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(corr.abs(), cmap=cmap, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "plt.savefig('../archive/Feature_correlation.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f9e4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [column for column in corr.columns if any(corr.abs()[column] > 0.98)]\n",
    "df = df.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06932192",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../features/feature_selection.pkl', 'wb') as f:\n",
    "    pickle.dump(list(df.drop(['apn', 'file', 'group'], axis=1).columns), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df51dc18",
   "metadata": {},
   "source": [
    "## 4.2 Features with low importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ed8c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['apn', 'file', 'group'], axis=1)\n",
    "X = (X - X.mean()) / X.std()\n",
    "y = df['apn']\n",
    "\n",
    "# feature extraction\n",
    "model = ExtraTreesClassifier(n_estimators=10)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67ca8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(data=model.feature_importances_, index=X.columns, columns=['importance'])\n",
    "ax = temp.sort_values('importance').plot.barh(figsize=(10,10))\n",
    "plt.savefig('../archive/Feature_importance.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66be9b0",
   "metadata": {},
   "source": [
    "# Other frequency-domain features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf7a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'b02'\n",
    "fs_new = 2.4\n",
    "b, a = signal.butter(3, 0.1)\n",
    "\n",
    "with open('features/HR_' + file + '.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "with open('data/processed/' + file + '.pkl', 'rb') as f:\n",
    "    apn = pickle.load(f)['apn']\n",
    "    group = util.ecg_diagnose(apn) if file[0] == 'x' else file[0].upper()   \n",
    "\n",
    "# Remove outliers    \n",
    "idx_valid = (data['hr'] < 2) & (data['hr'] > 0.5)\n",
    "hr_raw, t_raw = data['hr'][idx_valid], data['t'][idx_valid]\n",
    "\n",
    "# Smooth data\n",
    "hr_raw = signal.filtfilt(b, a, hr_raw)\n",
    "\n",
    "# Resample data for frequency-domain analysis\n",
    "t = np.arange(t_raw[0], t_raw[-1], 1 / fs_new / 60)\n",
    "hr = np.interp(t, t_raw, hr_raw)\n",
    "\n",
    "minute = 0\n",
    "idx = (t > minute) & (t < minute + 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9376a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STFT\n",
    "f, t_stft, Zxx = signal.stft(\n",
    "    hr[idx], fs=fs_new, window='hann', \n",
    "    nperseg=24*6, noverlap=None, nfft=None, detrend=False, \n",
    "    return_onesided=True, boundary='zeros', padded=True)\n",
    "plt.pcolormesh(t_stft, f, np.abs(Zxx))\n",
    "plt.title('STFT Magnitude')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylim([0, 0.1])\n",
    "plt.show()\n",
    "\n",
    "# spectrogram\n",
    "f, t_sg, Sxx = signal.spectrogram(\n",
    "    hr[idx], fs=fs_new,\n",
    "    window='hann', nperseg=24*6, noverlap=None, \n",
    "    nfft=None, detrend=False, return_onesided=True, \n",
    "    scaling='density', mode='psd')\n",
    "plt.pcolormesh(t_sg, f, Sxx)\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.xlim([0, 300])\n",
    "plt.ylim([0, 0.1])\n",
    "plt.show()\n",
    "\n",
    "# MFCC\n",
    "mfccs = librosa.feature.mfcc(y=hr, sr=fs_new, n_mfcc=40)\n",
    "print(mfccs.shape)\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(mfccs, x_axis='time')\n",
    "# plt.xlim([0, 5])\n",
    "plt.colorbar()\n",
    "plt.title('MFCC')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
