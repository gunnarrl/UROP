{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3e4b0ef",
   "metadata": {},
   "source": [
    "# <div align=\"center\">Stochastic and Mini Batch Gradient Descent</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f8efce",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88da39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "import math\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use(['ggplot'])\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize = (16,8))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f33b908",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"\n",
    "    \n",
    "    Abstract optimizer base class.\n",
    "\n",
    "    Note: this is the parent class of all optimizers, not an actual optimizer\n",
    "          that can be used for training models.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate, iterations, epochs, epsilon):\n",
    "        \"\"\"\n",
    "        Constructor of base class\n",
    "        \n",
    "        Parameters:\n",
    "            learning_rate (float)\n",
    "            iterations (int)\n",
    "            epochs (int)\n",
    "            epsilon (float) - acceptable error\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.epochs = epochs\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def cal_cost(self, theta, X, y):\n",
    "        '''\n",
    "        Calculates the cost for given X and Y.\n",
    "        \n",
    "        Parameters:\n",
    "            theta = Vector of thetas \n",
    "            X     = Row of X's\n",
    "            y     = Actual y's\n",
    "            \n",
    "        Returns:\n",
    "            float (cost value)\n",
    "\n",
    "        '''\n",
    "\n",
    "        m = len(y)\n",
    "\n",
    "        predictions = X.dot(theta)\n",
    "        cost = 1/(2*m) * np.sum(np.square(predictions-y))\n",
    "        \n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ff4b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent.\n",
    "    \n",
    "    Note: this is the implementation of sgd by randomly choosing an element\n",
    "          from X, this is not the way that we've learned in class\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate = 0.01, iterations = 100, epochs = 10, epsilon = 0.00001):\n",
    "        \"\"\"\n",
    "        Constructor of sgd, calles parents (super) consructor\n",
    "        \n",
    "        Parameters:\n",
    "            learning_rate (float)\n",
    "            iterations (int)\n",
    "            epochs (int)\n",
    "            epsilon (float) - acceptable error\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        \n",
    "        \"\"\"\n",
    "        super(SGD, self).__init__(learning_rate, iterations, epochs, epsilon)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Fit function which is the actual stochastic gradient's algorithm\n",
    "        \n",
    "        Parameters:\n",
    "            X    = Matrix of X with added bias units\n",
    "            y    = Vector of y\n",
    "            \n",
    "        Returns:\n",
    "            Returns the self\n",
    "        '''\n",
    "\n",
    "        m = len(y)\n",
    "        self.theta = np.zeros((X.shape[1], 1))\n",
    "        self.cost_history = []\n",
    "\n",
    "        for e in range(self.epochs):            \n",
    "            for it in range(self.iterations):\n",
    "                cost = 0.0\n",
    "                \n",
    "                for i in range(m):\n",
    "                    rand_ind = np.random.randint(0,m)\n",
    "                    X_i = X[rand_ind,:].reshape(1,X.shape[1])\n",
    "                    y_i = y[rand_ind].reshape(1,1)\n",
    "                    prediction = np.dot(X_i, self.theta)\n",
    "\n",
    "                    self.theta = self.theta -(1/m) * self.learning_rate * ( X_i.T.dot((prediction - y_i)))\n",
    "                    cost += self.cal_cost(self.theta, X_i, y_i)\n",
    "                \n",
    "                self.cost_history = np.append(self.cost_history, cost)                \n",
    "                \n",
    "                if cost <= self.epsilon:\n",
    "                    return self\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the value after the model has been trained.\n",
    "\n",
    "        Parameters:\n",
    "            x : array-like, shape = [n_samples, n_features]\n",
    "                Test samples\n",
    "\n",
    "        Returns:\n",
    "            Predicted value\n",
    "\n",
    "        \"\"\"\n",
    "        return np.dot(X, self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3bf74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBGD(Optimizer):\n",
    "    \"\"\"\n",
    "    Mini batch gradient descent.\n",
    "    \n",
    "    Note: this also covers stochastic descent we learned in class (with batch_size=1)\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate = 0.01, iterations = 100, epochs = 10, batch_size = 10, epsilon = 0.00001):\n",
    "        \"\"\"\n",
    "        Constructor of mini batch gd, calles parents (super) consructor\n",
    "        \n",
    "        Parameters:\n",
    "            learning_rate (float)\n",
    "            iterations (int)\n",
    "            epochs (int)\n",
    "            epsilon (float) - acceptable error\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        \n",
    "        \"\"\"\n",
    "        super(MBGD, self).__init__(learning_rate, iterations, epochs, epsilon)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Fit function which is the actual stochastic gradient's algorithm\n",
    "        \n",
    "        Parameters:\n",
    "            X    = Matrix of X without added bias units\n",
    "            y    = Vector of y\n",
    "            \n",
    "        Returns:\n",
    "            Returns the self\n",
    "        '''\n",
    "\n",
    "        m = len(y)\n",
    "        self.theta = np.zeros((X.shape[1]+1, 1))\n",
    "        self.cost_history = []\n",
    "        n_batches = math.floor(m / self.batch_size)\n",
    "\n",
    "        for e in range(self.epochs):\n",
    "            \n",
    "            for it in range(self.iterations):\n",
    "                cost = 0.0\n",
    "                indices = np.random.permutation(m)\n",
    "                X = X[indices]\n",
    "                y = y[indices]\n",
    "                \n",
    "                for i in range(0, m, self.batch_size):\n",
    "                    X_i = X[i:i + self.batch_size]\n",
    "                    y_i = y[i:i + self.batch_size]\n",
    "                    X_i = np.c_[np.ones(len(X_i)),X_i]\n",
    "                    prediction = np.dot(X_i, self.theta)\n",
    "\n",
    "                    self.theta = self.theta - (1/m) * self.learning_rate * ( X_i.T.dot((prediction - y_i)))\n",
    "                    cost += self.cal_cost(self.theta, X_i, y_i)\n",
    "                    \n",
    "                self.cost_history = np.append(self.cost_history, cost)\n",
    "                \n",
    "                if cost <= self.epsilon:\n",
    "                    return self\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the value after the model has been trained.\n",
    "\n",
    "        Parameters:\n",
    "            x : array-like, shape = [n_samples, n_features]\n",
    "                Test samples\n",
    "\n",
    "        Returns:\n",
    "            Predicted value\n",
    "\n",
    "        \"\"\"\n",
    "        X = np.c_[np.ones(len(X)),X]\n",
    "        \n",
    "        return np.dot(X, self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e112ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculate mean squared error\n",
    "    \n",
    "    Parameters:\n",
    "        actual (nd.array) - actual values\n",
    "        predicted (nd.array) - predicted values\n",
    "    \n",
    "    Return:\n",
    "        float(mse)\n",
    "    \"\"\"\n",
    "    return (np.square(actual - predicted)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693aab98",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fac9b3",
   "metadata": {},
   "source": [
    "# Data points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a52d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of data points \n",
    "n = 10000\n",
    "\n",
    "# Datapoints generation\n",
    "X = np.random.random(n).reshape((n, 1))\n",
    "y = np.sqrt(1+X)\n",
    "\n",
    "# Data plotting\n",
    "plt.plot(X, y, 'ro', label ='Original data')\n",
    "plt.title('Generated data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1379050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/ Test/ Validation datasets construction\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=22)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27321ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For stochastic gradient descent only\n",
    "\n",
    "int = np.ones(shape=y_train.shape[0])[..., None]\n",
    "X_train_with_ones = np.concatenate((int, X_train), 1)\n",
    "\n",
    "int = np.ones(shape=y_test.shape[0])[..., None]\n",
    "X_test_with_ones = np.concatenate((int, X_test), 1)\n",
    "\n",
    "int = np.ones(shape=y_val.shape[0])[..., None]\n",
    "X_val_with_ones = np.concatenate((int, X_val), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279ca3f5",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783ae4af",
   "metadata": {},
   "source": [
    "# Parameter Tuning and Actual Predicting for SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ca7b88",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea1e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr =0.01\n",
    "n_iter = 100\n",
    "\n",
    "epochs = [1, 5, 10]\n",
    "mse_e = []\n",
    "\n",
    "for i in range(len(epochs)):\n",
    "    sgd = SGD(learning_rate=lr, iterations=n_iter, epochs=epochs[i], epsilon=0.0000001)\n",
    "    sgd.fit(X_train_with_ones, y_train)\n",
    "    \n",
    "    prediction = sgd.predict(X_val_with_ones)\n",
    "    \n",
    "    mse_e.append(mse(y_val, prediction ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d3bdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch_index = np.argmin(mse_e)\n",
    "\n",
    "best_epoch = epochs[best_epoch_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc963f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(learning_rate=0.01, iterations=100, epochs=best_epoch, epsilon=0.00001)\n",
    "\n",
    "sgd.fit(X_train_with_ones, y_train)\n",
    "\n",
    "predicted_values_sgd = sgd.predict(X_test_with_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5b4070",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE in Test dataset: ', mse(y_test, predicted_values_sgd))\n",
    "\n",
    "plt.plot(X_test, y_test, 'ro', label ='test data')\n",
    "plt.plot(X_test, predicted_values_sgd, label ='Fitted line', color='green')\n",
    "plt.title('Stochastic Gradient Descent approximation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d46ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "ax.set_ylabel('{J(Theta)}',rotation=0)\n",
    "ax.set_xlabel('{Iterations}')\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "_=ax.plot(range(len(sgd.cost_history)),sgd.cost_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d3965d",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22adae93",
   "metadata": {},
   "source": [
    "# Parameter Tuning and Actual Predicting for MBGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a221d954",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a7b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr =0.001\n",
    "n_iter = 1000\n",
    "batch_size = 100\n",
    "\n",
    "epochs = [1, 5, 10]\n",
    "mse_e = []\n",
    "\n",
    "for i in range(len(epochs)):\n",
    "    mbgd = MBGD(learning_rate=lr, iterations=n_iter, epochs=epochs[i], epsilon=0.0000001, batch_size=batch_size)\n",
    "    mbgd.fit(X_train, y_train)\n",
    "    \n",
    "    prediction = mbgd.predict(X_val)\n",
    "    \n",
    "    mse_e.append(mse(y_val, prediction ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3691639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch_index = np.argmin(mse_e)\n",
    "\n",
    "best_epoch = epochs[best_epoch_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aced64",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbgd = MBGD(learning_rate=0.001, iterations=1000, epochs=best_epoch, epsilon=0.0000001, batch_size=100)\n",
    "\n",
    "mbgd.fit(X_train, y_train)\n",
    "\n",
    "predicted_values_mbgd = mbgd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaac4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE in Test dataset: ', mse(y_test, predicted_values_mbgd))\n",
    "\n",
    "plt.plot(X_test, y_test, 'ro', label ='test data')\n",
    "plt.plot(X_test, predicted_values_mbgd, label ='Fitted line', color='green')\n",
    "plt.title('Mini Batch Gradient Descent approximation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f5f60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "ax.set_ylabel('{J(Theta)}',rotation=0)\n",
    "ax.set_xlabel('{Iterations}')\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "_=ax.plot(range(len(mbgd.cost_history)),mbgd.cost_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d90c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 10))\n",
    "\n",
    "plt.plot(X_test, y_test, 'ro', label ='Original data', alpha=0.7)\n",
    "\n",
    "plt.plot(X_test, predicted_values_sgd, label = 'SGD', alpha = 0.7)\n",
    "plt.plot(X_test, predicted_values_mbgd, label = 'MBGD', alpha = 0.9)\n",
    "\n",
    "plt.title('All approximations')\n",
    "plt.xlabel('Test data')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
