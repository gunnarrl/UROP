{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b4c9c0",
   "metadata": {},
   "source": [
    "# Face recognition using OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc0b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import imutils\n",
    "import cv2\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import sys\n",
    "sys.path.append('../') \n",
    "from utils import show_side_by_side, load_image_as_array, show_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c31f0a",
   "metadata": {},
   "source": [
    "### Load the OpenCV face detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b36cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "protoPath = '../opencv_face_recognition/face_detector/deploy.prototxt'\n",
    "modelPath = '../opencv_face_recognition/face_detector/res10_300x300_ssd_iter_140000_fp16.caffemodel'\n",
    "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e6bf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = '../opencv_face_recognition/openface/nn4.small2.v1.t7'\n",
    "embedder = cv2.dnn.readNetFromTorch(embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf099841",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_per_person = 20\n",
    "names = ['Tony_Blair', 'Serena_Williams']\n",
    "\n",
    "confidence = 0.5\n",
    "mean_subtract_values = (104, 177, 123)\n",
    "embeddings_file = '../classifier_data/face_embeddings.p'\n",
    "recogniser_file = '../classifier_data/recogniser.p'\n",
    "labels_file = '../classifier_data/labels.p'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6eed4c",
   "metadata": {},
   "source": [
    "### Create a sample dataset\n",
    "\n",
    "We'll get 10 images for 2 known people, and 10 from a random selection of other people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92db1c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfw_dir = '../lfw'\n",
    "\n",
    "X, y = [], []\n",
    "for name in names:\n",
    "    X_name, y_name = [], []\n",
    "    images = os.listdir(os.path.join(lfw_dir, name))\n",
    "    for i in range(num_images_per_person):\n",
    "        filename = os.path.join(lfw_dir, name, images[i]) \n",
    "        X.append(filename)\n",
    "        y.append(name)\n",
    "                    \n",
    "df_names = pd.read_csv(os.path.join(lfw_dir, 'lfw-names.txt'), delim_whitespace=True, names=['name', 'count'])\n",
    "df_names = df_names[~df_names['name'].isin(names)]\n",
    "df_names = df_names.sort_values(by=['count'], ascending=False).reset_index()\n",
    "\n",
    "for i in range(num_images_per_person):\n",
    "    unknown_idx = randint(0, len(df_names) - 1 )\n",
    "    name = df_names.iloc[unknown_idx]['name']\n",
    "    df_names = df_names[df_names['name'] != name]\n",
    "    images = os.listdir(os.path.join(lfw_dir, name))\n",
    "    image_idx = randint(0, len(images) - 1)\n",
    "    X.append(os.path.join(lfw_dir, name, images[image_idx]))\n",
    "    y.append('Unknown')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ed1b0",
   "metadata": {},
   "source": [
    "* Loop through the images and create a blob for each one.\n",
    "\n",
    "Note that we normalise the image by subtracting the mean values from red, green and blue. The helps with variations in illumination between images and helps the CNN work (see https://github.com/opencv/opencv/tree/master/samples/dnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d37acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_face_embeddings_from_image(image):\n",
    "    vec = []\n",
    "    #image = cv2.imread(image_path)\n",
    "    imageBlob = cv2.dnn.blobFromImage(\n",
    "        cv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
    "        mean_subtract_values, swapRB=False, crop=False)\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    # apply OpenCV's deep learning-based face detector to localize faces in the input image\n",
    "    detector.setInput(imageBlob)\n",
    "    predictions = detector.forward()\n",
    "    # We need one face to continue\n",
    "    if len(predictions) > 0:\n",
    "        # Find the bounding box with the largest probability\n",
    "        i = np.argmax(predictions[0, 0, :, 2])\n",
    "        conf = predictions[0, 0, i, 2]\n",
    " \n",
    "        # Find the bounding box with the highest confidence\n",
    "        if conf > confidence:\n",
    "            # Find the bounding box and extract the face\n",
    "            box = predictions[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            start_x, start_y, end_x, end_y = box.astype(\"int\")\n",
    "            face = image[start_y:end_y, start_x:end_x]\n",
    " \n",
    "            # Ensure the face width and height are sufficiently large\n",
    "            if face.shape[0] >= 20 and face.shape[1] > 20:\n",
    "                # construct a blob for the face ROI, then pass the blob through our face embedding model \n",
    "                # to obtain the 128-d vector\n",
    "                faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
    "                    (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "                embedder.setInput(faceBlob)\n",
    "                vec = embedder.forward()\n",
    "    return (start_x, start_y, end_x, end_y), vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7b80a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {'names': [], 'embeddings': []}\n",
    "for i, (image_path, name) in enumerate(zip(X, y)):\n",
    "    image = load_image_as_array(image_path)\n",
    "    print('Processing image {}'.format(image_path))\n",
    "    _, face_embedding_vector = get_face_embeddings_from_image(image)\n",
    "    if face_embedding_vector != []:\n",
    "        embeddings['names'].append(name)\n",
    "        embeddings['embeddings'].append(face_embedding_vector[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c3452d",
   "metadata": {},
   "source": [
    "* Check how many embeddings we've created\n",
    "\n",
    "Note that some images in the LFW dataset fail embeddings due to low confidence from OpenCV or the face size is too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6972b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Created {} embeddings.'.format(len(embeddings['names'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99db270f",
   "metadata": {},
   "source": [
    "* Save our embeddings using Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aaaddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(embeddings, open(embeddings_file, 'wb' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d65cbf2",
   "metadata": {},
   "source": [
    "### Let's train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6799ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pickle.load(open(embeddings_file, 'rb' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ee97e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(embeddings['names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675fa562",
   "metadata": {},
   "outputs": [],
   "source": [
    "recogniser = SVC(C=1.0, kernel=\"linear\", probability=True)\n",
    "recogniser.fit(embeddings['embeddings'], labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919519d5",
   "metadata": {},
   "source": [
    "* And let's save our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1521fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(recogniser, open(recogniser_file, 'wb'))\n",
    "pickle.dump(le, open(labels_file, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ec3578",
   "metadata": {},
   "source": [
    "### Now let's recognise faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c1b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "embedder = cv2.dnn.readNetFromTorch(embedding_model)\n",
    "recogniser = pickle.load(open(recogniser_file, 'rb'))\n",
    "le = pickle.load(open(labels_file, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a2de50",
   "metadata": {},
   "source": [
    "* Pick a random image from our dataset\n",
    "\n",
    "Note that we might pick one we trained with, but both of these people have a lot of images to choose from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf8086",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_images_per_person):\n",
    "    name = names[randint(0, len(names) - 1)]\n",
    "    images = os.listdir(os.path.join(lfw_dir, name))\n",
    "    filename = os.path.join(lfw_dir, name, images[randint(0, len(images) - 1)])\n",
    "    print('Trying to recognise faces in {}'.format(filename))\n",
    "    image = load_image_as_array(filename)\n",
    "\n",
    "    # imageBlob = cv2.dnn.blobFromImage(\n",
    "    #     cv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
    "    #     mean_subtract_values, swapRB=False, crop=False)\n",
    "\n",
    "    # detector.setInput(imageBlob)\n",
    "    # detections = detector.forward()\n",
    "\n",
    "    box, vec = get_face_embeddings_from_image(image)\n",
    "\n",
    "    predictions = recogniser.predict_proba(vec)[0]\n",
    "    j = np.argmax(predictions)\n",
    "    proba = predictions[j]\n",
    "    name = le.classes_[j]\n",
    "\n",
    "    start_x, start_y, end_x, end_y = box\n",
    "    text = \"{}: {:.2f}%\".format(name, proba * 100)\n",
    "    y = start_y - 10 if start_y - 10 > 10 else start_y + 10\n",
    "    cv2.rectangle(image, (start_x, start_y), (end_x, end_y),\n",
    "        (0, 0, 255), 2)\n",
    "    cv2.putText(image, text, (start_x, y),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "\n",
    "    show_image(image)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
