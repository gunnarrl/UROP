{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbf9b2f0",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network for traffic Sign Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e672dad6",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a1fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "import csv\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn .utils import shuffle\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c85d38",
   "metadata": {},
   "source": [
    "## Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58691240",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "with open('signnames.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    labelNameDict = {rows[0]:rows[1] for rows in reader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef30cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = 'train.p'\n",
    "testing_file = 'test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b77759f",
   "metadata": {},
   "source": [
    "### Dataset Summary & Exploration\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "* 'features' is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "* 'labels' is a 2D array containing the label/class id of the traffic sign. The file signnames.csv contains id -> name mappings for each id.\n",
    "* 'sizes' is a list containing tuples, (width, height) representing the the original width and height the image.\n",
    "* 'coords' is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. THESE COORDINATES ASSUME THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455ff4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = len(X_train)\n",
    "n_test = len(X_test)\n",
    "image_shape = X_train.shape[1:]\n",
    "uniq_classes, uniq_indexes, uniq_class_counts=np.unique(y_train, return_index=True, return_counts=True)\n",
    "n_classes = len(uniq_classes)\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6f8ba6",
   "metadata": {},
   "source": [
    "### Utility Funcitons\n",
    "\n",
    "Also plot the sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16dfa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_images():\n",
    "    gs = gridspec.GridSpec(10, 5)\n",
    "    plt.figure(figsize=(18,20))\n",
    "    i=0\n",
    "    for index in uniq_indexes:\n",
    "        ax = plt.subplot(gs[i])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        img = X_train[index]\n",
    "        plt.subplot(10,5,i+1)\n",
    "        label=str(y_train[index])\n",
    "        plt.text(0, -2, label)\n",
    "        plt.text(5, -2, labelNameDict[label])\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        i=i+1\n",
    "    plt.show()\n",
    "\n",
    "def plot_data_with_labels():\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.bar(uniq_classes, uniq_class_counts)\n",
    "    plt.xlabel('label')\n",
    "    plt.xticks(uniq_classes)\n",
    "    plt.ylabel('no. of samples')\n",
    "    for a,b in zip(uniq_classes, uniq_class_counts): \n",
    "        plt.text(a, b+50, str(b))\n",
    "    plt.show()\n",
    "    \n",
    "plot_data_with_labels()\n",
    "print ('Sample Image for each label')\n",
    "plot_sample_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bbca0c",
   "metadata": {},
   "source": [
    "### Image Preprocessing\n",
    "\n",
    "Preprocessing includes normalization using opencv minmax normalization between 50 and 200. Final values reached after some tests. followed by median bluring because\n",
    "\n",
    "1. Normalization will mitigate differences due to light condition across the data set and will make the pixel intesity consistant.\n",
    "\n",
    "2. Bluring is done to remove noise/unwanted features. Median blur was chossen because it works good in removing salt and pepper noice and produced the best results when tested in comparison with averaging, gaussian blur or bilateralFilter.\n",
    "\n",
    "Sample images before and after preprocessing plotted below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758d9e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess the data\n",
    "def normalize_and_blur(img):\n",
    "    img = cv2.normalize(img, img, 50, 200, cv2.NORM_MINMAX)\n",
    "    cv2.medianBlur(img,3)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f5816",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train)):\n",
    "    normalize_and_blur(X_train[i])\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    normalize_and_blur(X_train[i])\n",
    "\n",
    "print(\"done\")\n",
    "plot_sample_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c3f3cd",
   "metadata": {},
   "source": [
    "### Generate additional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d9a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate data additional data \n",
    "def transform_image(img,ang_range,shear_range,trans_range):\n",
    "    ang_rot = np.random.uniform(ang_range)-ang_range/2\n",
    "    rows,cols,ch = img.shape\n",
    "    Rot_M = cv2.getRotationMatrix2D((cols/2,rows/2),ang_rot,1)\n",
    "    tr_x = trans_range*np.random.uniform()-trans_range/2\n",
    "    tr_y = trans_range*np.random.uniform()-trans_range/2\n",
    "    Trans_M = np.float32([[1,0,tr_x],[0,1,tr_y]])\n",
    "    pts1 = np.float32([[5,5],[20,5],[5,20]])\n",
    "    pt1 = 5+shear_range*np.random.uniform()-shear_range/2\n",
    "    pt2 = 20+shear_range*np.random.uniform()-shear_range/2\n",
    "    pts2 = np.float32([[pt1,5],[pt2,pt1],[5,pt2]])\n",
    "    shear_M = cv2.getAffineTransform(pts1,pts2)\n",
    "    img = cv2.warpAffine(img,Rot_M,(cols,rows))\n",
    "    img = cv2.warpAffine(img,Trans_M,(cols,rows))\n",
    "    img = cv2.warpAffine(img,shear_M,(cols,rows))\n",
    "    return img\n",
    "\n",
    "# generate more data from images:\n",
    "#new_images = []\n",
    "#for cl,a,b in zip(uniq_classes, uniq_indexes, uniq_class_counts):\n",
    "#    maxSamples = max(uniq_class_counts)\n",
    "#    diff = maxSamples//b\n",
    "#    k=b\n",
    "#    for n in range(diff):\n",
    "#        i = a\n",
    "#        j = 0\n",
    "#        while j < b:\n",
    "#            img = transform_image(X_train[i],20,10,5)\n",
    "#            new_images.append(img)\n",
    "#            y_train = np.append(y_train, cl)\n",
    "#            j = j+1\n",
    "#            i = i+1\n",
    "#            k = k+1\n",
    "#            if  k >= maxSamples:\n",
    "#               break\n",
    "#        if  k >= maxSamples:\n",
    "#                break\n",
    "                \n",
    "#X_train = np.concatenate((X_train, new_images), axis=0)\n",
    "#uniq_classes, uniq_indexes, uniq_class_counts=np.unique(y_train, return_index=True, return_counts=True)\n",
    "#plot_data_with_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf26119",
   "metadata": {},
   "source": [
    "### Split data into train and validation sets\n",
    "\n",
    "From the downloded data 76% is training data and 24% test data.\n",
    "\n",
    "The training data is further split into training and validation data sets in a 70:30 ratio i.e. 70% of all training data is used for actual taining and 30% of it used for validation.\n",
    "\n",
    "Training and validation data is setup using train_test_split from sklearn.\n",
    "\n",
    "Used a test size of 0.3 because with 0.2 validation set had less then 8000 images and testing set had most. I think Validation accuracy will not be very accurate if the classifier had already seen most of the data. So this division seemed right.\n",
    "\n",
    "random_state is set to zero so that the same set is not returned in successive runs. This seems more appropriate as the classifier needs to be trained on different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f1c5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train , test_size=0.3, random_state=0)\n",
    "\n",
    "print(\"Number of training examples =\", len(X_train))\n",
    "print(\"Number of validation examples =\", len(X_validation))\n",
    "print(\"Update image Shape: {}\".format(X_train[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a86277",
   "metadata": {},
   "source": [
    "### Define CNN architecture\n",
    "\n",
    "\n",
    "Started with leNet and resources provided in the chapter and read about architectures like laNet, AlexNet, VGGNet GoogLeNet etc.\n",
    "\n",
    "In comparison to others VGGNet like architecture appeard to be most suitable for image recognition without being too complex. Also, it was performing the best when tested on this task against laNet and AlexNet.\n",
    "\n",
    "The final architecure is similar to VGGNet but not so deep with two conv-pool layers followed by two fully connected layers.\n",
    "\n",
    "Two conv-pool layers are used because of small image size and because adding more layers with SAME padding(to not reduce the image size too quickly) had no sizable improvement in performance.\n",
    "\n",
    "Testing showed that increasing depth/number of filter in a consistent manner with small steps in conv layers before a polling operation produced better results compared to bigger/suddend changes in conv layer. In fact the accuracy goes down if the depth is increased too rapidly.\n",
    "\n",
    "\n",
    "Ref:\n",
    "Feature Evaluation of Deep Convolutional Neural Networks for Object Recognition and Detection\n",
    "Hirokatsu Kataoka, Kenji Iwata, Yutaka Satoh\n",
    "https://arxiv.org/abs/1509.07627"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b42297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convNet(x):    \n",
    "    # Hyperparameters\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    strides = 1\n",
    "    filter_height  = 3\n",
    "    filter_width = 3\n",
    "    \n",
    "    # Layer 1: Conv1. Input = 32x32x3. Output = 30x30x6.\n",
    "    W = tf.Variable(tf.truncated_normal(shape=[filter_height, filter_width, 3, 6], mean = mu, stddev = sigma))\n",
    "    b = tf.Variable(tf.zeros(6))\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    x = tf.nn.relu(x)\n",
    "    # Layer 1: Conv2. Input = 30x30x6. Output = 28x28x9.\n",
    "    W = tf.Variable(tf.truncated_normal(shape=[filter_height, filter_width, 6, 9], mean = mu, stddev = sigma))\n",
    "    b = tf.Variable(tf.zeros(9))\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "    # Layer 1: Pooling. Input = 28x28x9. Output = 14x14x9.\n",
    "    x = tf.nn.max_pool(x,ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='VALID')\n",
    "    \n",
    "    \n",
    "    #Layer 2: Conv1. Input = 14x14x9. Output = 12x12x12.\n",
    "    W = tf.Variable(tf.truncated_normal(shape=[filter_height, filter_width, 9, 12], mean = mu, stddev = sigma))\n",
    "    b = tf.Variable(tf.zeros(12))\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    x = tf.nn.relu(x)\n",
    "    #Layer 2: Conv2. Input = 12x12x12. Output = 10x10x16.\n",
    "    W = tf.Variable(tf.truncated_normal(shape=[filter_height, filter_width, 12, 16], mean = mu, stddev = sigma))\n",
    "    b = tf.Variable(tf.zeros(16))\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "    # Layer 2: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    x = tf.nn.max_pool(x,ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='VALID')\n",
    "    \n",
    "    \n",
    "    # Flatten. Input = 5x5x16. Output = 400.\n",
    "    x = flatten(x)\n",
    "    \n",
    "      \n",
    "    # Layer 3: Fully Connected. Input = 400. Output = 220.\n",
    "    W =tf.Variable(tf.truncated_normal([400, 220], mean = mu, stddev = sigma))\n",
    "    b = tf.Variable(tf.zeros(220))\n",
    "    x = tf.add(tf.matmul(x, W), b)\n",
    "    x = tf.nn.relu(x)\n",
    "    # Layer 4: Fully Connected. Input = 84. Output = 43.\n",
    "    W =tf.Variable(tf.truncated_normal([220, 43], mean = mu, stddev = sigma))\n",
    "    b = tf.Variable(tf.zeros(43))\n",
    "    logits = tf.add(tf.matmul(x, W), b)\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee0704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"convnet_architecture.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7166e92d",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "<a href=\"http://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network\">Good read about various paramter tuning</a>\n",
    "\n",
    "1. started with the batch_size=1000 and epocs=10 with lerrning rate of 0.1. This gave very low initial accuracy so incremently reduced the learning rate and tested. Finally stopped at 0.001 as the best inital accuray achived by this.\n",
    "\n",
    "2. With above it was observed that I got high initial accuracy which increased up to 6th or 7th interation and than started going down and finally finally went up on 10th. So started reducing the Batch size and increasing EPOCS.\n",
    "\n",
    "3. Reached the final values above after some trial and error. Final EPOCS value was taken after the accuracy flattened for around 5-10 iterations and there was't any considerable improvements with more.\n",
    "\n",
    "3. The training data is shuffled after every iteration to avoid overfitting.\n",
    "\n",
    "4. Adam Optimizer was used after reading <a href=\"http://sebastianruder.com/optimizing-gradient-descent/\">ref</a> and because it produced best results after testing in comparison with other optimizers in tenserflow.\n",
    "\n",
    "Ref:\n",
    "\n",
    "From Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, Ping Tak Peter Tang. On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima.\n",
    "https://arxiv.org/abs/1609.04836\n",
    "\n",
    "Yoshua Bengio. Practical recommendations for gradient-based training of deep architectures\n",
    "https://arxiv.org/abs/1206.5533\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabbc675",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train your model\n",
    "\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 400\n",
    "rate = 0.001\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 43)\n",
    "\n",
    "logits = convNet(x)\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy, total_loss = 0,0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        loss, accuracy = sess.run([loss_operation, accuracy_operation], feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "        total_loss += (loss *  len(batch_x))\n",
    "    return total_accuracy/num_examples, total_loss/num_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eed541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_map = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    print(\"Training...\")\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        validation_accuracy, validation_loss = evaluate(X_validation, y_validation)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation loss = {:.3f}\".format(validation_loss))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        accuracy_map.append(round(validation_accuracy*100,2))\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.xticks(range(1, EPOCHS+1))\n",
    "    plt.xlabel('EPOC')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.plot(range(1,EPOCHS+1), accuracy_map,'-o')\n",
    "    for a,b in list(zip(range(1,EPOCHS+1), accuracy_map))[0::3]:\n",
    "        plt.text(a, b-1, str(b))\n",
    "    try:\n",
    "        saver\n",
    "    except NameError:\n",
    "        saver = tf.train.Saver()\n",
    "    saver.save(sess, 'model')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef246d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    new_saver = tf.train.import_meta_graph('model.meta')\n",
    "    new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    tf.trainable_variables()\n",
    "    test_accuracy,test_loss = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a236aaba",
   "metadata": {},
   "source": [
    "### Test a Model on New Images\n",
    "\n",
    "Used several pictures of traffic signs found on the web not seen by the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ad45c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load new images.\n",
    "def plot_images(images, x, y,labels):\n",
    "    gs = gridspec.GridSpec(x, y)\n",
    "    i=0\n",
    "    for index in range(len(images)):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        img = images[index]\n",
    "        plt.subplot(x,y,i+1)\n",
    "        label=str(labels[index])\n",
    "        plt.text(0, -2, label)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        i=i+1\n",
    "    plt.show()\n",
    "\n",
    "for i in range(1,17):\n",
    "    image = cv2.imread('test'+str(i)+'.jpg',cv2.IMREAD_UNCHANGED)\n",
    "    image = cv2.resize(image,(32, 32), interpolation = cv2.INTER_AREA)\n",
    "    cv2.imwrite('test_sampled'+str(i)+'.jpg',image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebc039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_images=[]\n",
    "new_labels=['14','31','12','16','1','25','32','32','8','18','0','40','13','1','29','18']\n",
    "\n",
    "for i in range(1,17):\n",
    "    img = mpimg.imread('test_sampled'+str(i)+'.jpg')\n",
    "    img = normalize_and_blur(img)\n",
    "    new_images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca23a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_images[:11]\n",
    "#new_labels[:11]\n",
    "plot_images(new_images[:11], 3, 5,new_labels[:11])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph('model.meta')\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    #tf.trainable_variables()\n",
    "    predictions = sess.run(tf.argmax(logits, 1), feed_dict={x: new_images[:11]})\n",
    "    print (\"Actual labels    : \" + '['+' '.join(new_labels[:11])+']')\n",
    "    print (\"predicted labels : \" + str(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0499e2bd",
   "metadata": {},
   "source": [
    "### Run the predictions.\n",
    "\n",
    "I have chossen these images becuse they have multiple signs some of them seen in training set other not. This will be interesting to see what the model gives priority to.\n",
    "\n",
    "While one image is similar but have diffent color. As the model is trained on color images this will show if the model can extend to different colors.\n",
    "\n",
    "Selected candidate images are plotted below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae109dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_images[11:16]\n",
    "#new_labels[11:16]\n",
    "plot_images(new_images[11:16], 1, 5,new_labels[11:16])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph('model.meta')\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    #tf.trainable_variables()\n",
    "    predictions = sess.run(tf.argmax(logits, 1), feed_dict={x: new_images[11:16]})\n",
    "    print (\"Actual labels    : \" + '['+' '.join(new_labels[11:16])+']')\n",
    "    print (\"predicted labels : \" + str(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c613a3be",
   "metadata": {},
   "source": [
    "### Visualize the softmax probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae3066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "soft_max = None\n",
    "with tf.Session() as sess: \n",
    "    saver = tf.train.import_meta_graph('model.meta')\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    soft_max = sess.run(tf.nn.softmax(logits), {x:new_images[11:16]})\n",
    "    print ('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e2902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, nrows=2,figsize=(15,15))\n",
    "ax1, ax2, ax3, ax4, ax5, ax6= axes.ravel()\n",
    "ax1.scatter(np.arange(0,43), soft_max[0])\n",
    "ax1.grid(True)\n",
    "ax1.set_yticks(np.arange(-0.1,1.1,0.1))\n",
    "ax2.scatter(np.arange(0,43), soft_max[1])\n",
    "ax2.grid(True)\n",
    "ax2.set_yticks(np.arange(-0.1,1.1,0.1))\n",
    "ax3.scatter(np.arange(0,43), soft_max[2])\n",
    "ax3.grid(True)\n",
    "ax3.set_yticks(np.arange(-0.1,1.1,0.1))\n",
    "ax4.scatter(np.arange(0,43), soft_max[3])\n",
    "ax4.grid(True)\n",
    "ax4.set_yticks(np.arange(-0.1,1.1,0.1))\n",
    "ax5.scatter(np.arange(0,43), soft_max[4])\n",
    "ax5.grid(True)\n",
    "ax5.set_yticks(np.arange(-0.1,1.1,0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86654e86",
   "metadata": {},
   "source": [
    "### Use the model's softmax probabilities to visualize the **certainty** of its predictions, [`tf.nn.top_k`](https://www.tensorflow.org/versions/r0.12/api_docs/python/nn.html#top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4429c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = None\n",
    "with tf.Session() as sess: \n",
    "    saver = tf.train.import_meta_graph('model.meta')\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    values,indices = sess.run(tf.nn.top_k(soft_max, k=5))\n",
    "    print (values)\n",
    "    print (indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480ee812",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, nrows=2,figsize=(15,15))\n",
    "ax1, ax2, ax3, ax4, ax5, ax6 = axes.ravel()\n",
    "\n",
    "ax1.scatter(indices[0], values[0])\n",
    "ax1.grid(True)\n",
    "ax1.set_yticks(np.arange(-0.1,1.1,0.1))\n",
    "ax2.scatter(indices[1], values[1])\n",
    "ax2.grid(True)\n",
    "ax2.set_yticks(np.arange(-0.1,1.1,0.1))\n",
    "ax3.scatter(indices[2], values[2])\n",
    "ax3.grid(True)\n",
    "ax3.set_yticks(np.arange(-0.1,1.1,0.1))\n",
    "ax4.scatter(indices[3], values[3])\n",
    "ax4.grid(True)\n",
    "ax4.set_yticks(np.arange(-0.1,1.1,0.1))\n",
    "ax5.scatter(indices[4], values[4])\n",
    "ax5.grid(True)\n",
    "ax5.set_yticks(np.arange(-0.1,1.1,0.1))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
