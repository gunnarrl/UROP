import keras 
keras.__version__

from __future__ import print_function
import matplotlib.pyplot as plt
import numpy as np
from scipy.misc import imresize
import os
import pickle 

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

plt.style.use('ggplot')

from keras import layers
from keras import models
from keras import optimizers
from keras import callbacks

from keras.utils import plot_model

from keras import preprocessing
from keras.preprocessing import image

#DATASET_DIR = '/Users/Joshua.Newnham/Documents/Data/quickdraw_dataset/sketchrnn_training_data/'
#DATASET_DIR = '/quickdraw_sketches/'
#DATASET_DIR = '/Users/Joshua.Newnham/Dropbox/Method - Personal/Machine Learning with CoreML/TrainingData/Chapter9/cnn-sketch-classifier/data/'
DATASET_DIR = '/Users/Joshua.Newnham/Documents/Data/quickdraw_dataset/sketchrnn_training_data/'
TRAINING_PARTS = 5 

EPOCHS = 1000
BATCH_SIZE = 512 
MAX_SEQ_LEN = 75
CLASSES = 172
NUM_RNN_LAYERS = 3 
NUM_RNN_NODES = 128
NUM_CONV = [48, 64, 96, 128]
CONV_LEN = [5, 5, 3, 3]
DROPOUT = 0.3

def plot_accuracy_loss(history):
    acc = history['acc']
    val_acc = history['val_acc']
    loss = history['loss']
    val_loss = history['val_loss']
    
    print("{} {} {} {}".format(len(acc), len(val_acc), len(loss), len(val_loss)))

    epochs = range(len(acc))

    plt.plot(epochs, acc, 'bo', label='Training acc')
    plt.plot(epochs, val_acc, 'b', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.legend()

    plt.figure()

    plt.plot(epochs, loss, 'bo', label='Training loss')
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.legend()

    plt.show()

def batch(x, y, batch_size=BATCH_SIZE):
    return x.reshape(batch_size, -1, 3), y

def pad_stroke_sequence(x, max_len=MAX_SEQ_LEN):
    padded_x = np.zeros((x.shape[0], max_len, 3), dtype=np.float32)
    for i in range(x.shape[0]):
        X = x[i]
        if X.shape[0] > max_len:
            X = X[:max_len, :]
        elif X.shape[0] < max_len:
            padding = np.array([[0,0,0]] * (max_len-X.shape[0]), dtype=np.float32)            
            X = np.vstack((padding, X))
            
        padded_x[i] = X
        
    return padded_x

def create_model(input_shape=(MAX_SEQ_LEN, 3), 
                 num_conv=NUM_CONV, 
                 conv_len=CONV_LEN, 
                 dropout=DROPOUT, 
                 batch_size=BATCH_SIZE, 
                 num_rnn_layers=NUM_RNN_LAYERS, 
                 num_rnn_nodes=NUM_RNN_NODES, 
                 num_classes=CLASSES):
    
    model = models.Sequential() 
    for i, filters in enumerate(num_conv):
        if i == 0:
            # TODO: feasible to use a TimeDistributed wrapper here? https://keras.io/layers/wrappers/
            model.add(
                layers.Conv1D(filters=filters, 
                              kernel_size=conv_len[i], 
                              activation=None, 
                              strides=1, 
                              padding='same', 
                              name='conv1d_{}'.format(i), input_shape=input_shape))
        else:
            model.add(layers.Dropout(dropout, name="dropout_{}".format(i)))
            model.add(layers.Conv1D(filters=filters, 
                                    kernel_size=conv_len[i], 
                                    activation=None, 
                                    strides=1, 
                                    padding='same', 
                                    name='conv1d_{}'.format(i)))
      
    for i in range(num_rnn_layers):
        model.add(layers.Bidirectional(layers.LSTM(units=num_rnn_nodes, 
                                                   return_sequences=True, 
                                                   recurrent_dropout=dropout), 
                                       name="lstm_{}".format(i)))
    
    model.add(layers.Flatten())
    model.add(layers.Dense(num_classes, activation="softmax"))
    
                      
    return model 

def train(model, 
          train_x_files, train_y_files, valid_x_files, valid_y_files, 
          batch_size=BATCH_SIZE, epochs=EPOCHS,
          max_seq_len=MAX_SEQ_LEN, 
          load_previous_weights=True, model_weights_file="output/quickdraw_weights.h5"):
    
    # load previous weights (if applicable)
    if model_weights_file is not None and os.path.isfile(model_weights_file) and load_previous_weights:
        print("Loading weights from file {}".format(model_weights_file))
        model.load_weights(model_weights_file)
    
    checkpoint = callbacks.ModelCheckpoint(model_weights_file, 
                                           monitor='val_loss', 
                                           verbose=0, 
                                           save_best_only=True, 
                                           save_weights_only=True, 
                                           mode='auto', 
                                           period=1)
    
    # compile model 
    model.compile(
        loss='categorical_crossentropy', 
        optimizer='rmsprop', 
        metrics=['accuracy'])
    
    history_file = "{}_history.pickle".format(model_weights_file.replace(".h5", ""))
    
    if os.path.isfile(history_file):
        with open(history_file, 'rb') as f:
            accumulated_history = pickle.load(f)
    else:
        accumulated_history = {
            'acc': [], 
            'val_acc': [], 
            'loss': [], 
            'val_loss': []
        }                        
    
    for e in range(epochs):
        for i in range(len(train_x_files)):
            # load data for this iteration 
            train_x = np.load(train_x_files[i])
            train_y = np.load(train_y_files[i])
            
            valid_x = np.load(valid_x_files[i])
            valid_y = np.load(valid_y_files[i])
    
            # prepare training and validation data 
            train_x = pad_stroke_sequence(train_x)
            valid_x = pad_stroke_sequence(valid_x)        
    
            history = model.fit(train_x, train_y,
                                batch_size=batch_size, 
                                epochs=1,
                                validation_data=(valid_x, valid_y), 
                                shuffle=True, 
                                callbacks=[checkpoint])
        
            history_acc = history.history['acc']
            if type(history_acc) is not type(list):
                history_acc = [history_acc]
                
            history_val_acc = history.history['val_acc']
            if type(history_val_acc) is not type(list):
                history_val_acc = [history_val_acc]
                
            history_loss = history.history['loss']
            if type(history_loss) is not type(list):
                history_loss = [history_loss]
                
            history_val_loss = history.history['val_loss']                         
            if type(history_val_loss) is not type(list):
                history_val_loss = [history_val_loss]
            
            accumulated_history['acc'] += history_acc
            accumulated_history['val_acc'] += history_val_acc
            accumulated_history['loss'] += history_loss
            accumulated_history['val_loss'] += history_val_loss
            
            with open(history_file, 'wb') as f:
                pickle.dump(accumulated_history, f)
    
    return model, accumulated_history 

# Load files 

train_x_files = [] 
train_y_files = []
valid_x_files = [] 
valid_y_files = []

for part_num in range(1, TRAINING_PARTS+1):
    train_x_files.append(os.path.join(DATASET_DIR, "train_{}_x.npy".format(part_num)))
    train_y_files.append(os.path.join(DATASET_DIR, "train_{}_y.npy".format(part_num)))
    valid_x_files.append(os.path.join(DATASET_DIR, "validation_{}_x.npy".format(part_num)))
    valid_y_files.append(os.path.join(DATASET_DIR, "validation_{}_y.npy".format(part_num)))

train_1_x = np.load(train_x_files[0])
train_1_y = np.load(train_y_files[0])
padded_train_1_x = pad_stroke_sequence(train_1_x)

print("train_1_x {}, train_1_y {}, padded_train_1_x {}".format(
    train_1_x.shape, 
    train_1_y.shape, 
    padded_train_1_x.shape))

valid_1_x = np.load(valid_x_files[0])
valid_1_y = np.load(valid_y_files[0])
padded_valid_1_x = pad_stroke_sequence(valid_1_x)

print("valid_1_x {}, valid_1_y {}, padded_valid_1_x {}".format(
    valid_1_x.shape, 
    valid_1_y.shape, 
    padded_valid_1_x.shape))

model = create_model()
model.summary()

model, training_history = train(model, 
                                train_x_files=train_x_files, 
                                train_y_files=train_y_files, 
                                valid_x_files=valid_x_files, 
                                valid_y_files=valid_y_files, 
                                load_previous_weights=True, 
                                model_weights_file="output/quickdraw_weights.h5")

model, training_history = train(model, 
                                train_x_files=train_x_files, 
                                train_y_files=train_y_files, 
                                valid_x_files=valid_x_files, 
                                valid_y_files=valid_y_files, 
                                load_previous_weights=True, 
                                model_weights_file="output/quickdraw_weights.h5")

with open('{}_history.pickle'.format('output/quickdraw_weights'), 'rb') as f:
    training_history = pickle.load(f)
plot_accuracy_loss(training_history)

import json 

with open('output/quickdraw_arch.json', 'w') as f:
    json_string = model.to_json()
    json.dump(json_string, f)
