{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de960671",
   "metadata": {},
   "source": [
    "# Extrinsic NLP Task for Sentiment Analysis using a Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681a5449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path().absolute().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc3fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import embedeval\n",
    "from embedeval.parsers.word2vec_gensim import load_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bc9081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16615f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedeval.tasks.offense_detection import f1_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7500ad5d",
   "metadata": {},
   "source": [
    "## Loading the Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a587ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_PATH = PROJECT_ROOT / \"tests\" / \"data\" / \"downloads\" / \"cc.de.300.vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62621c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = load_embedding(EMBEDDING_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50e5207",
   "metadata": {},
   "source": [
    "## Prepare Text Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0850c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_file = PROJECT_ROOT / \"src\" / \"embedeval\" / \"tasks\" / \"data\" / \"germeval-2018\" / \"train.txt\"\n",
    "test_dataset_file = PROJECT_ROOT / \"src\" / \"embedeval\" / \"tasks\" / \"data\" / \"germeval-2018\" / \"test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb2a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(corpus_file):\n",
    "    df = pd.read_csv(corpus_file, sep=\"\\t\", names=[\"tweet\", \"label\"], usecols=[0, 1])\n",
    "    df[\"label\"] = df[\"label\"].map({\"OFFENSE\": 1, \"OTHER\": 0})\n",
    "    \n",
    "    # remove words starting with an @ from all tweets\n",
    "    def remove_mentions(text):\n",
    "        return re.sub(r\"@[A-Za-z0-9_]{3,}\", \"\", text)\n",
    "        \n",
    "    df[\"tweet\"] = df[\"tweet\"].apply(remove_mentions)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d1883d",
   "metadata": {},
   "source": [
    "### Load Train and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce4a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_dataset(train_dataset_file)\n",
    "test_df = load_dataset(test_dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788767b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(train_df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914e5e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1beb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(test_df[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30c62a6",
   "metadata": {},
   "source": [
    "### Tokenize Text Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5057ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentence_length(corpus):\n",
    "    word_count = lambda sentence: len(word_tokenize(sentence))\n",
    "    longest_sentence = max(corpus, key=word_count)\n",
    "    length_long_sentence = len(word_tokenize(longest_sentence))\n",
    "    \n",
    "    return length_long_sentence\n",
    "    \n",
    "\n",
    "def tokenize_corpus(corpus):\n",
    "    word_tokenizer = Tokenizer()\n",
    "    word_tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    return word_tokenizer\n",
    "\n",
    "\n",
    "def pad_text(corpus, text, sentence_length):\n",
    "    padded_text = pad_sequences(text, sentence_length, padding='post')\n",
    "    return padded_text\n",
    "\n",
    "\n",
    "def prepare_corpus(corpus, tokenizer, sentence_length):\n",
    "    text = tokenizer.texts_to_sequences(corpus)\n",
    "    padded_text = pad_text(corpus, text, sentence_length)\n",
    "    \n",
    "    return padded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5128de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_corpus = pd.concat([train_df, test_df])[\"tweet\"]\n",
    "tokenizer = tokenize_corpus(entire_corpus)\n",
    "\n",
    "sentence_length = calculate_sentence_length(entire_corpus)\n",
    "\n",
    "train_corpus = prepare_corpus(train_df[\"tweet\"], tokenizer, sentence_length)\n",
    "test_corpus = prepare_corpus(test_df[\"tweet\"], tokenizer, sentence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd5303",
   "metadata": {},
   "source": [
    "### Create Embedding Matrix for Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e31f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(embedding, tokenizer):\n",
    "    # Get the Vocabulary length and add 1 for all unknown words\n",
    "    vocab_length = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    embedding_matrix = np.zeros((vocab_length, 300))\n",
    "    \n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = embedding.get_word_vector(word)\n",
    "        except KeyError:\n",
    "            embedding_vector = None\n",
    "            \n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ded4170",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding_matrix = create_embedding_matrix(embedding, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f6a5ec",
   "metadata": {},
   "source": [
    "## Create Keras Model for Sentimental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758d2e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(\n",
    "    train_embedding_matrix.shape[0], \n",
    "    train_embedding_matrix.shape[1], \n",
    "    weights=[train_embedding_matrix], \n",
    "    input_length=sentence_length, \n",
    "    #trainable=False,\n",
    "    trainable=True\n",
    "))\n",
    "\n",
    "#model.add(GlobalAveragePooling1D())\n",
    "#model.add(Dense(16, activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))     SLOW AF\n",
    "\n",
    "#model.add(Dense(256, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(Dense(256, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc', f1_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfddc7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e058dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_corpus, train_df[\"label\"], validation_split=0.3, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dc9d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "acc = history.history['acc']\n",
    "\n",
    "epochs = list(range(1, 6))\n",
    "\n",
    "plt.plot(epochs, loss, 'ko', label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.plot(epochs, acc, 'yo', label='Training Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e057fd",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247dc601",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, f1_score = model.evaluate(test_corpus, test_df[\"label\"], verbose=0)\n",
    "\n",
    "loss, accuracy, f1_score"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
