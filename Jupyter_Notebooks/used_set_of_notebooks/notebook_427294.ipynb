{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deddbbf1",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a358a855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras import optimizers, applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Dense, Dropout, GlobalAveragePooling2D, Input\n",
    "\n",
    "# Set seeds to make the experiment more reproducible.\n",
    "from tensorflow import set_random_seed\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    set_random_seed(0)\n",
    "seed_everything()\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb6c460",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6343cada",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')\n",
    "test = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\n",
    "print('Number of train samples: ', train.shape[0])\n",
    "print('Number of test samples: ', test.shape[0])\n",
    "\n",
    "# Preprocecss data\n",
    "train[\"id_code\"] = train[\"id_code\"].apply(lambda x: x + \".png\")\n",
    "test[\"id_code\"] = test[\"id_code\"].apply(lambda x: x + \".png\")\n",
    "train['diagnosis'] = train['diagnosis'].astype('str')\n",
    "display(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a99ad1b",
   "metadata": {},
   "source": [
    "# Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8085c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 30\n",
    "WARMUP_EPOCHS = 2\n",
    "LEARNING_RATE = 1e-4\n",
    "WARMUP_LEARNING_RATE = 1e-3\n",
    "HEIGHT = 512\n",
    "WIDTH = 512\n",
    "CANAL = 3\n",
    "N_CLASSES = train['diagnosis'].nunique()\n",
    "ES_PATIENCE = 5\n",
    "RLROP_PATIENCE = 3\n",
    "DECAY_DROP = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb460a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kappa(y_true, y_pred, n_classes=5):\n",
    "    y_trues = K.cast(K.argmax(y_true), K.floatx())\n",
    "    y_preds = K.cast(K.argmax(y_pred), K.floatx())\n",
    "    n_samples = K.cast(K.shape(y_true)[0], K.floatx())\n",
    "    distance = K.sum(K.abs(y_trues - y_preds))\n",
    "    max_distance = n_classes - 1\n",
    "    \n",
    "    kappa_score = 1 - ((distance**2) / (n_samples * (max_distance**2)))\n",
    "\n",
    "    return kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9babc91b",
   "metadata": {},
   "source": [
    "# Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fe9c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen=ImageDataGenerator(rescale=1./255, \n",
    "                                 validation_split=0.2,\n",
    "                                 horizontal_flip=True,\n",
    "                                 vertical_flip=True)\n",
    "\n",
    "train_generator=train_datagen.flow_from_dataframe(\n",
    "    dataframe=train,\n",
    "    directory=\"../input/aptos2019-blindness-detection/train_images/\",\n",
    "    x_col=\"id_code\",\n",
    "    y_col=\"diagnosis\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(HEIGHT, WIDTH),\n",
    "    subset='training')\n",
    "\n",
    "valid_generator=train_datagen.flow_from_dataframe(\n",
    "    dataframe=train,\n",
    "    directory=\"../input/aptos2019-blindness-detection/train_images/\",\n",
    "    x_col=\"id_code\",\n",
    "    y_col=\"diagnosis\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",    \n",
    "    target_size=(HEIGHT, WIDTH),\n",
    "    subset='validation')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(  \n",
    "        dataframe=test,\n",
    "        directory = \"../input/aptos2019-blindness-detection/test_images/\",\n",
    "        x_col=\"id_code\",\n",
    "        target_size=(HEIGHT, WIDTH),\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        class_mode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cc91ca",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b7a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_out):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    base_model = applications.ResNet50(weights=None, \n",
    "                                       include_top=False,\n",
    "                                       input_tensor=input_tensor)\n",
    "    base_model.load_weights('../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(2048, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    final_output = Dense(n_out, activation='softmax', name='final_output')(x)\n",
    "    model = Model(input_tensor, final_output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547ed671",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(input_shape=(HEIGHT, WIDTH, CANAL), n_out=N_CLASSES)\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "for i in range(-5, 0):\n",
    "    model.layers[i].trainable = True\n",
    "    \n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(train['diagnosis'].astype('int').values), train['diagnosis'].astype('int').values)\n",
    "\n",
    "metric_list = [\"accuracy\", kappa]\n",
    "optimizer = optimizers.Adam(lr=WARMUP_LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\",  metrics=metric_list)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889d205c",
   "metadata": {},
   "source": [
    "# Train top layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee95287",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID = valid_generator.n//valid_generator.batch_size\n",
    "\n",
    "history_warmup = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                              validation_data=valid_generator,\n",
    "                              validation_steps=STEP_SIZE_VALID,\n",
    "                              epochs=WARMUP_EPOCHS,\n",
    "                              class_weight=class_weights,\n",
    "                              verbose=1).history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044ec262",
   "metadata": {},
   "source": [
    "# Fine-tune the complete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65fb9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=ES_PATIENCE, restore_best_weights=True, verbose=1)\n",
    "rlrop = ReduceLROnPlateau(monitor='val_loss', mode='min', patience=RLROP_PATIENCE, factor=DECAY_DROP, min_lr=1e-6, verbose=1)\n",
    "\n",
    "callback_list = [es, rlrop]\n",
    "optimizer = optimizers.Adam(lr=LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\",  metrics=metric_list)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8774af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_finetunning = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                              validation_data=valid_generator,\n",
    "                              validation_steps=STEP_SIZE_VALID,\n",
    "                              epochs=EPOCHS,\n",
    "                              callbacks=callback_list,\n",
    "                              class_weight=class_weights,\n",
    "                              verbose=1).history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bcd01d",
   "metadata": {},
   "source": [
    "# Model loss graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfa266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'loss': history_warmup['loss'] + history_finetunning['loss'], \n",
    "           'val_loss': history_warmup['val_loss'] + history_finetunning['val_loss'], \n",
    "           'acc': history_warmup['acc'] + history_finetunning['acc'], \n",
    "           'val_acc': history_warmup['val_acc'] + history_finetunning['val_acc'], \n",
    "           'kappa': history_warmup['kappa'] + history_finetunning['kappa'], \n",
    "           'val_kappa': history_warmup['val_kappa'] + history_finetunning['val_kappa']}\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex='col', figsize=(20, 18))\n",
    "\n",
    "ax1.plot(history['loss'], label='Train loss')\n",
    "ax1.plot(history['val_loss'], label='Validation loss')\n",
    "ax1.legend(loc='best')\n",
    "ax1.set_title('Loss')\n",
    "\n",
    "ax2.plot(history['acc'], label='Train accuracy')\n",
    "ax2.plot(history['val_acc'], label='Validation accuracy')\n",
    "ax2.legend(loc='best')\n",
    "ax2.set_title('Accuracy')\n",
    "\n",
    "ax3.plot(history['kappa'], label='Train kappa')\n",
    "ax3.plot(history['val_kappa'], label='Validation kappa')\n",
    "ax3.legend(loc='best')\n",
    "ax3.set_title('Kappa')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622da3e5",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5842212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lastFullTrainPred = np.empty((0, N_CLASSES))\n",
    "lastFullTrainLabels = np.empty((0, N_CLASSES))\n",
    "lastFullValPred = np.empty((0, N_CLASSES))\n",
    "lastFullValLabels = np.empty((0, N_CLASSES))\n",
    "\n",
    "for i in range(STEP_SIZE_TRAIN+1):\n",
    "    im, lbl = next(train_generator)\n",
    "    scores = model.predict(im, batch_size=train_generator.batch_size)\n",
    "    lastFullTrainPred = np.append(lastFullTrainPred, scores, axis=0)\n",
    "    lastFullTrainLabels = np.append(lastFullTrainLabels, lbl, axis=0)\n",
    "\n",
    "for i in range(STEP_SIZE_VALID+1):\n",
    "    im, lbl = next(valid_generator)\n",
    "    scores = model.predict(im, batch_size=valid_generator.batch_size)\n",
    "    lastFullValPred = np.append(lastFullValPred, scores, axis=0)\n",
    "    lastFullValLabels = np.append(lastFullValLabels, lbl, axis=0)\n",
    "\n",
    "train_preds = [np.argmax(pred) for pred in lastFullTrainPred]\n",
    "train_labels = [np.argmax(label) for label in lastFullTrainLabels]\n",
    "validation_preds = [np.argmax(pred) for pred in lastFullValPred]\n",
    "validation_labels = [np.argmax(label) for label in lastFullValLabels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10128764",
   "metadata": {},
   "source": [
    "# Threshold optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d8f3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_fixed_threshold(preds, targs, do_plot=True):\n",
    "    best_thr_list = [0 for i in range(preds.shape[1])]\n",
    "    for index in reversed(range(preds.shape[1])):\n",
    "        score = []\n",
    "        thrs = np.arange(0, 1, 0.01)\n",
    "        for thr in thrs:\n",
    "            preds_thr = [index if x[index] > thr else np.argmax(x) for x in preds]\n",
    "            score.append(cohen_kappa_score(targs, preds_thr))\n",
    "        score = np.array(score)\n",
    "        pm = score.argmax()\n",
    "        best_thr, best_score = thrs[pm], score[pm].item()\n",
    "        best_thr_list[index] = best_thr\n",
    "        print(f'thr={best_thr:.3f}', f'F2={best_score:.3f}')\n",
    "        if do_plot:\n",
    "            plt.plot(thrs, score)\n",
    "            plt.vlines(x=best_thr, ymin=score.min(), ymax=score.max())\n",
    "            plt.text(best_thr+0.03, best_score-0.01, ('Kappa[%s]=%.3f'%(index, best_score)), fontsize=14);\n",
    "            plt.show()\n",
    "    return best_thr_list\n",
    "\n",
    "threshold_list = find_best_fixed_threshold(lastFullValPred, validation_labels, do_plot=True)\n",
    "threshold_list[0] = 0 # In last instance assign label 0\n",
    "\n",
    "train_preds_opt = [0 for i in range(lastFullTrainPred.shape[0])]\n",
    "for idx, thr in enumerate(threshold_list):\n",
    "    for idx2, pred in enumerate(lastFullTrainPred):\n",
    "        if pred[idx] > thr:\n",
    "            train_preds_opt[idx2] = idx\n",
    "\n",
    "validation_preds_opt = [0 for i in range(lastFullValPred.shape[0])]\n",
    "for idx, thr in enumerate(threshold_list):\n",
    "    for idx2, pred in enumerate(lastFullValPred):\n",
    "        if pred[idx] > thr:\n",
    "            validation_preds_opt[idx2] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828e37e6",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a2158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex='col', figsize=(24, 7))\n",
    "labels = ['0 - No DR', '1 - Mild', '2 - Moderate', '3 - Severe', '4 - Proliferative DR']\n",
    "train_cnf_matrix = confusion_matrix(train_labels, train_preds)\n",
    "validation_cnf_matrix = confusion_matrix(validation_labels, validation_preds)\n",
    "\n",
    "train_cnf_matrix_norm = train_cnf_matrix.astype('float') / train_cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "validation_cnf_matrix_norm = validation_cnf_matrix.astype('float') / validation_cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "train_df_cm = pd.DataFrame(train_cnf_matrix_norm, index=labels, columns=labels)\n",
    "validation_df_cm = pd.DataFrame(validation_cnf_matrix_norm, index=labels, columns=labels)\n",
    "\n",
    "sns.heatmap(train_df_cm, annot=True, fmt='.2f', cmap=\"Blues\",ax=ax1).set_title('Train')\n",
    "sns.heatmap(validation_df_cm, annot=True, fmt='.2f', cmap=\"Blues\",ax=ax2).set_title('Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6172851",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex='col', figsize=(24, 7))\n",
    "labels = ['0 - No DR', '1 - Mild', '2 - Moderate', '3 - Severe', '4 - Proliferative DR']\n",
    "train_cnf_matrix = confusion_matrix(train_labels, train_preds_opt)\n",
    "validation_cnf_matrix = confusion_matrix(validation_labels, validation_preds_opt)\n",
    "\n",
    "train_cnf_matrix_norm = train_cnf_matrix.astype('float') / train_cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "validation_cnf_matrix_norm = validation_cnf_matrix.astype('float') / validation_cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "train_df_cm = pd.DataFrame(train_cnf_matrix_norm, index=labels, columns=labels)\n",
    "validation_df_cm = pd.DataFrame(validation_cnf_matrix_norm, index=labels, columns=labels)\n",
    "\n",
    "sns.heatmap(train_df_cm, annot=True, fmt='.2f', cmap=\"Blues\",ax=ax1).set_title('Train optimized')\n",
    "sns.heatmap(validation_df_cm, annot=True, fmt='.2f', cmap=\"Blues\",ax=ax2).set_title('Validation optimized')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4198f1",
   "metadata": {},
   "source": [
    "## Quadratic Weighted Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57002094",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Cohen Kappa score: %.3f\" % cohen_kappa_score(train_preds, train_labels, weights='quadratic'))\n",
    "print(\"Validation Cohen Kappa score: %.3f\" % cohen_kappa_score(validation_preds, validation_labels, weights='quadratic'))\n",
    "print(\"Complete set Cohen Kappa score: %.3f\" % cohen_kappa_score(train_preds+validation_preds, train_labels+validation_labels, weights='quadratic'))\n",
    "print(\"Train optimized Cohen Kappa score: %.3f\" % cohen_kappa_score(train_preds_opt, train_labels, weights='quadratic'))\n",
    "print(\"Validation optimized Cohen Kappa score: %.3f\" % cohen_kappa_score(validation_preds_opt, validation_labels, weights='quadratic'))\n",
    "print(\"Complete optimized set Cohen Kappa score: %.3f\" % cohen_kappa_score(train_preds_opt+validation_preds_opt, train_labels+validation_labels, weights='quadratic'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2fe4d2",
   "metadata": {},
   "source": [
    "# Apply model to test set and output predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3d2b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.reset()\n",
    "STEP_SIZE_TEST = test_generator.n//test_generator.batch_size\n",
    "preds = model.predict_generator(test_generator, steps=STEP_SIZE_TEST)\n",
    "predictions = [np.argmax(pred) for pred in preds]\n",
    "\n",
    "predictions_opt = [0 for i in range(preds.shape[0])]\n",
    "for idx, thr in enumerate(threshold_list):\n",
    "    for idx2, pred in enumerate(preds):\n",
    "        if pred[idx] > thr:\n",
    "            predictions_opt[idx2] = idx\n",
    "\n",
    "filenames = test_generator.filenames\n",
    "results = pd.DataFrame({'id_code':filenames, 'diagnosis':predictions})\n",
    "results['id_code'] = results['id_code'].map(lambda x: str(x)[:-4])\n",
    "\n",
    "results_opt = pd.DataFrame({'id_code':filenames, 'diagnosis':predictions_opt})\n",
    "results_opt['id_code'] = results_opt['id_code'].map(lambda x: str(x)[:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06efb301",
   "metadata": {},
   "source": [
    "# Predictions class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b11105",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex='col', figsize=(24, 8.7))\n",
    "sns.countplot(x=\"diagnosis\", data=results, palette=\"GnBu_d\", ax=ax1)\n",
    "sns.countplot(x=\"diagnosis\", data=results_opt, palette=\"GnBu_d\", ax=ax2)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8547e7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('submission.csv',index=False)\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fb9c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_opt.to_csv('submission_opt.csv',index=False)\n",
    "results_opt.head(10)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
