{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b43e522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab599de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import essential modules\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c2cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful utils funcion to load data\n",
    "\n",
    "def process_read_dataframe(df: pd.DataFrame):\n",
    "    bins = [0.25, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    "    # group\n",
    "    df[\"unique_id_question_body\"] = df[\"question_body\"].astype(\"category\").cat.codes\n",
    "    df[\"unique_id_question_body\"] = df[\"category\"].str.cat(df[\"unique_id_question_body\"].astype(\"str\"), sep=\"_\")\n",
    "    df[\"host_stem\"] = df[\"host\"].str.split(\".\").apply(lambda x: \".\".join(x[-2:]))\n",
    "    group_columns = [\"category\", \"host_stem\", \"unique_id_question_body\"]\n",
    "    df[group_columns] = df[group_columns].astype(\"category\")\n",
    "\n",
    "    # corpus\n",
    "    columns = [\"question_title\", \"question_body\", \"answer\"]\n",
    "    for col in columns:\n",
    "        df[f\"count_{col}\"] = df[col].str.split(\" \").apply(lambda x: len(x)).astype(np.int32)\n",
    "\n",
    "    df[\"count_question_title_body\"] = (df[\"count_question_title\"] + df[\"count_question_body\"]).astype(np.int32)\n",
    "    df[\"count_question_title_body_answer\"] = (df[\"count_question_title_body\"] + df[\"count_answer\"]).astype(np.int32)\n",
    "    stats_columns = [f\"count_{col}\" for col in columns] + [\n",
    "        \"count_question_title_body\", \"count_question_title_body_answer\"]\n",
    "\n",
    "    df_stats = df[stats_columns].describe(bins)\n",
    "    df_stats_split = df.groupby(\"category\")[stats_columns].apply(lambda x: x.describe(bins)).unstack(0).T\n",
    "\n",
    "    # concat\n",
    "    # df[\"question_title_body\"] = df[\"question_title\"].str.cat(others=df[\"question_body\"], sep=\" \")\n",
    "    # columns = columns + [\"question_title_body\"]\n",
    "    return df[columns], df[group_columns], df_stats, df_stats_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6a17bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folder\n",
    "data_dir = '../input/google-quest-challenge'\n",
    "index_name = 'qa_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ca937e",
   "metadata": {},
   "source": [
    "## Tasks to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3abe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = [\n",
    "    \"question_asker_intent_understanding\", \"question_body_critical\", \"question_conversational\",\n",
    "    \"question_expect_short_answer\", \"question_fact_seeking\", \"question_has_commonly_accepted_answer\",\n",
    "    \"question_interestingness_others\", \"question_interestingness_self\", \"question_multi_intent\",\n",
    "    \"question_not_really_a_question\", \"question_opinion_seeking\", \"question_type_choice\", \"question_type_compare\",\n",
    "    \"question_type_consequence\", \"question_type_definition\", \"question_type_entity\", \"question_type_instructions\",\n",
    "    \"question_type_procedure\", \"question_type_reason_explanation\", \"question_type_spelling\",\n",
    "    \"question_well_written\", \"answer_helpful\", \"answer_level_of_information\", \"answer_plausible\",\n",
    "    \"answer_relevance\", \"answer_satisfaction\", \"answer_type_instructions\", \"answer_type_procedure\",\n",
    "    \"answer_type_reason_explanation\", \"answer_well_written\"\n",
    "]\n",
    "\n",
    "# split by questioin and answer\n",
    "output_categories_question = list(filter(lambda x: x.startswith(\"question_\"), target_columns))\n",
    "output_categories_answer = list(filter(lambda x: x.startswith(\"answer_\"), target_columns))\n",
    "output_categories = output_categories_question + output_categories_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b713341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "df_train = pd.read_csv(os.path.join(data_dir, \"train.csv\")).set_index(index_name)\n",
    "\n",
    "# labels\n",
    "df_train[target_columns] = df_train[target_columns].astype(np.float32)\n",
    "train_y = df_train[output_categories]\n",
    "train_x, train_groups, train_stats, train_stats_split = process_read_dataframe(df_train)\n",
    "train_x.shape  # training data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a8038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for the ratio of duplicated questions\n",
    "df_train[\"question_body\"].nunique() / df_train.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd75b301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency of duplicated questions, ~1750 question bodies don't have any dupplicates, and over 1250 with duplicated once, and some even have 12.\n",
    "train_groups[\"unique_id_question_body\"].value_counts().value_counts().sort_index().plot(kind=\"bar\", legend=\"distribution of duplicated distribution\", grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14c615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby(\"category\").apply(lambda x: x[\"question_body\"].nunique() / len(x))  # duplicated among different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cd5136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stats of setence length by words of question and answer pair: [\"question_title\", \"question_body\", \"answer\"]\n",
    "train_stats.T  # half of the context of question_body and answer are less 100 words (93, 91 words on question_body and answer,respectively). \n",
    "\n",
    "# 95% of them are less than 440 words, this provide a good insights to set the input max_lengh for language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed354755",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats_split  # Further split by category. There are five categories: CULTURE, LIFE_ARTS, SCIENCE, STACKOVERFLOW and TECHNOLOGY."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68e570c",
   "metadata": {},
   "source": [
    "The distribution on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd755ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eace2f99",
   "metadata": {},
   "source": [
    "## Unique values in Values\n",
    "\n",
    "Although these labels are averaged be several scorer, most labels are just a few unique values. This provide the insights for postprocessing after prediciont."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747c74f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.nunique()  # observe how many unique values within labels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a5411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.mean().rename('mean of score').sort_values().plot(kind='barh', figsize=(18, 15), grid=True, fontsize=14, legend='Mean value of each target')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e87a1d",
   "metadata": {},
   "source": [
    "Here is . It can be foresee that on the bottom side, such as `question_type_consequnce`, `question_not_really_a_question`, `question_type_spelling` would suffer in modeling as their minor representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c84bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.corr().abs().describe([.99]).T  #labels has very weak correaltion among each other. The max 1.0 is correlation with label theirselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a435e4e1",
   "metadata": {},
   "source": [
    "## Test stats\n",
    "\n",
    "Also load test set to exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbeaac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(os.path.join(data_dir, \"test.csv\")).set_index(index_name)\n",
    "test_x, test_groups, test_stats, test_stats_split = process_read_dataframe(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b8b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8824010",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae5799b",
   "metadata": {},
   "source": [
    "## Distribution difference between training and test set\n",
    "\n",
    "It appears that the mean paragraph length of training set are shorter than test set except question title. Mean length of answer are shorter 10% than test set have. However, once split data into the provided cateories such as `CULTURE`, `LIFE_ARTS`, `SCIENCE`, `STACKOVERFLOW` and `TECHNOLOGY`, then training corpus are longer at `LIFE_ARTS` and `STACKOVERFLOW`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22607512",
   "metadata": {},
   "outputs": [],
   "source": [
    "((train_stats.T['mean'] - test_stats.T['mean']) / train_stats.T['mean']).rename(\"length dfifference (%)\").plot(\n",
    "    kind='barh', grid=True, fontsize=14, legend='paragragh length differencec between train and test')  # seems training is generally shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa83c79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ((train_stats_split['mean'] - test_stats_split['mean']) / train_stats_split['mean']).reset_index()\n",
    "df.columns = ['field'] + df.columns.tolist()[1:]\n",
    "\n",
    "g = sns.catplot(x=\"category\", y=\"mean\", hue=\"field\", data=df, height=6, kind=\"bar\", palette=\"muted\", )\n",
    "g.despine(left=True)\n",
    "g.set_ylabels(\"breakdown paragraph length by category\")\n",
    "g.set_xticklabels(rotation=45)  # upside longer on training set, and downside on longer on test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aff6ec",
   "metadata": {},
   "source": [
    "## To be continued with training models"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
