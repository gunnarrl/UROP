import pandas as pd
import numpy as np

print(pd.__version__)
print(np.__version__)

# 판다스를 통해 데이터를 로드해 온다.
# 여기에서는 캐글의 타이타닉 데이터를 사용한다. 
# 데이터 다운로드 : https://www.kaggle.com/c/titanic/data 
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

print(train.shape)
print(test.shape)

train.dtypes

test.dtypes

train.columns

# 수치형 데이터에 대한 정보를 보여준다.
# 데이터의 수량과 최대값, 최소값, 평균값, 중간값 등을 확인할 수 있다.
train.describe()

# 오브젝트 타입의 데이터만 따로 추출해 본다.
# 이 데이터 중 카테고리 형태의 데이터가 무엇인지 보고 인코딩 해준다.
# 원핫인코딩 뿐만 아니라 자연어처리(NLP)에서 배웠던 TF, TF-IDF의 인코딩도 해줄 수 있으며 
# 어떤 인코딩이 적합할지 생각해 본다.
obj_df = train.select_dtypes(include=['object']).copy()
obj_df.head()

# 어느 데이터든 누락 된 데이터가 있으면 출력하도록 했다.
# Cabin이 누락 된 데이터가 가장 많다.
# 결측치 다루는 법은 따로 다룰 것이다.
obj_df[obj_df.isnull().any(axis=1)].head(5)

# 카테고리 데이터로 적합한지 확인
obj_df["Cabin"].value_counts().head(5)

# 처리 전과 비교해 보기 위해 데이터를 복사
train_c_df = train.copy()
test_c_df = test.copy()

train['Sex'].value_counts()

train.loc[train["Sex"] == "male", "Sex"] = 0
train.loc[train["Sex"] == "female", "Sex"] = 1

test.loc[test["Sex"] == "male", "Sex"] = 0
test.loc[test["Sex"] == "female", "Sex"] = 1

train['Sex'] = train['Sex'].apply(lambda s: 1 if s == 'female' else 0)
test['Sex'] = test['Sex'].apply(lambda s: 1 if s == 'female' else 0)

train.head()

# 카테고리 데이터를 인코딩 해준다.
from sklearn.preprocessing import LabelEncoder

# 성별을 0과 1로 인코딩
def gender_to_int(data):
    le = LabelEncoder()
    le.fit(["male","female"])
    data["Gender"] = le.transform(data["Sex"]) 
    return data

train_c_df = gender_to_int(train_c_df)
test_c_df = gender_to_int(test_c_df)
train_c_df.head()

train['Embarked'].value_counts()

train_c_df["Embarked_C"] = train_c_df["Embarked"] == "C"
train_c_df["Embarked_S"] = train_c_df["Embarked"] == "S"
train_c_df["Embarked_Q"] = train_c_df["Embarked"] == "Q"

print(train.shape)
print(train_c_df.shape)

train_c_df[["Embarked", "Embarked_C", "Embarked_S", "Embarked_Q"]].head(10)

# 기계가 데이터를 이해할 수 있도록 
# 카테고리 데이터를 one-hot-encoding 해준다.
def dummy_data(data, columns):
    for column in columns:
        data = pd.concat([data, pd.get_dummies(data[column], prefix = column)], axis=1)
        data = data.drop(column, axis=1)
    return data


dummy_columns = ["Sex", "Pclass", "Embarked"]
train_dummy = dummy_data(train, dummy_columns)
test_dummy = dummy_data(test, dummy_columns)

print('원핫인코딩 전 shape')
print(train.shape)
print(test.shape)

print('get_dummies로 원핫인코딩 후 shape')
print(train_dummy.shape)
print(test_dummy.shape)

train_dummy.head()

# 사용하지 않을 컬럼을 제거해 피처로 사용할 컬럼만 남겨둔다.
def drop_not_concerned(data, columns):
    return data.drop(columns, axis=1)

not_concerned_columns = ["PassengerId", "Name", "Ticket", "Cabin"]
X_train = drop_not_concerned(train_dummy, not_concerned_columns)
X_train = X_train.drop('Survived', axis=1)
X_test = drop_not_concerned(test_dummy, not_concerned_columns)

X_train.head()

from sklearn.preprocessing import MinMaxScaler

def normalize_age(data):
    scaler = MinMaxScaler()
    data["Age"] = scaler.fit_transform(data["Age"].values.reshape(-1,1))
    return data


def normalize_fare(data):
    scaler = MinMaxScaler()
    data["Fare"] = scaler.fit_transform(data["Fare"].values.reshape(-1,1))
    return data

train_data = normalize_age(train)
test_data = normalize_age(test)

train_data = normalize_fare(train)
test_data = normalize_fare(test)

train_data.head()
