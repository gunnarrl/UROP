{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c753433c",
   "metadata": {},
   "source": [
    "# Import models \n",
    "Introduction for this part: first we import useful models and related csv data as dataframe. Then we clean the data and select predictors and target label. Finally we use CNN models to train our data and test them on test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae92b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import pandas as pd\n",
    "\n",
    "# Load the original data\n",
    "data_ori = pd.read_csv('tmdb_5000_movies.csv')\n",
    "# show data detail\n",
    "data_ori.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b483913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show columns dtypes\n",
    "data_ori.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3966dfe6",
   "metadata": {},
   "source": [
    "# Transfer json object to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115ac875",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = data_ori\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# changing the genres column from json to string\n",
    "movies['genres']=movies['genres'].apply(json.loads)\n",
    "for index,i in zip(movies.index,movies['genres']):\n",
    "    list1=[]\n",
    "    for j in range(len(i)):\n",
    "        list1.append((i[j]['name']))# the key 'name' contains the name of the genre\n",
    "    movies.loc[index,'genres']=str(list1)\n",
    "    \n",
    "# changing the keywords column from json to string\n",
    "movies['keywords']=movies['keywords'].apply(json.loads)\n",
    "for index,i in zip(movies.index,movies['keywords']):\n",
    "    list1=[]\n",
    "    for j in range(len(i)):\n",
    "        list1.append((i[j]['name']))\n",
    "    movies.loc[index,'keywords']=str(list1)\n",
    "    \n",
    "## changing the production_companies column from json to string\n",
    "movies['production_companies']=movies['production_companies'].apply(json.loads)\n",
    "for index,i in zip(movies.index,movies['production_companies']):\n",
    "    list1=[]\n",
    "    for j in range(len(i)):\n",
    "        list1.append((i[j]['name']))\n",
    "    movies.loc[index,'production_companies']=str(list1)\n",
    "    \n",
    "# changing the production_countries column from json to string    \n",
    "movies['production_countries']=movies['production_countries'].apply(json.loads)\n",
    "for index,i in zip(movies.index,movies['production_countries']):\n",
    "    list1=[]\n",
    "    for j in range(len(i)):\n",
    "        list1.append((i[j]['name']))\n",
    "    movies.loc[index,'production_countries']=str(list1)\n",
    "    \n",
    "# changing the spoken_languages column from json to string    \n",
    "movies['spoken_languages']=movies['spoken_languages'].apply(json.loads)\n",
    "for index,i in zip(movies.index,movies['spoken_languages']):\n",
    "    list1=[]\n",
    "    for j in range(len(i)):\n",
    "        list1.append((i[j]['name']))\n",
    "    movies.loc[index,'production_countries']=str(list1)   \n",
    "\n",
    "data_ori.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2d8759",
   "metadata": {},
   "source": [
    "# Get feaures and labels ready\n",
    "Here we intent to focus on columns in float type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fb5d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are multiple methods, I go with my original way which is cumbersome but striaght forward\n",
    "\n",
    "data_ori.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c2a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = data_ori.columns\n",
    "type(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0009924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(data_ori)\n",
    "type(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273cf1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove colmuns that is not float\n",
    "for col in columns:\n",
    "    if type(col) != float:\n",
    "        columns.remove(col)\n",
    "\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65c39d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we see some columns of type object are stil there, I don't know why. We try another method\n",
    "data=data_ori.select_dtypes(exclude='object')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eba322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we choose columns that we want\n",
    "features=['popularity','revenue','vote_average']\n",
    "label=['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dd596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we change budget to float\n",
    "data['budget']=data['budget']+0.0\n",
    "data['revenue']=data['revenue']+0.0\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76c9993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we change label values according to their original value\n",
    "def change(i):\n",
    "    if i<=5.6:\n",
    "        i=0.0\n",
    "    if i>5.6 and i<=6.2:\n",
    "        i=1.0\n",
    "    if i>6.2 and i<=6.8:\n",
    "        i=2.0\n",
    "    if i>6.8:\n",
    "        i=3.0\n",
    "    return i\n",
    "data[label]=data[['vote_average']].applymap(lambda x : change(x))\n",
    "data[label].head(1)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae91fd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[label].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bf41b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data = data[features]\n",
    "feature_data.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4873f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data=data[label]\n",
    "label_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31632f",
   "metadata": {},
   "source": [
    "# Split data and write new dataframes to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8150ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split shuffled data\n",
    "trainingfeatures= feature_data[:4000]\n",
    "traininglabels=label_data[:4000]\n",
    "testingfeatures= feature_data[4000:]\n",
    "testinglabels=label_data[4000:]\n",
    "dfs = [trainingfeatures,traininglabels,testingfeatures,testinglabels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54d8b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv files\n",
    "names=['trainingfeatures','traininglabels','testingfeatures','testinglabels']\n",
    "for i in range(len(dfs)):\n",
    "    df = dfs[i]\n",
    "    name= names[i]+'.csv'\n",
    "    df.to_csv(path_or_buf=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4b0db6",
   "metadata": {},
   "source": [
    "# Read training data and testing data\n",
    "Here we trainsform dataframe as np float arrays for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5eed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get datasets\n",
    "x_train = pd.read_csv('trainingfeatures.csv').drop(columns=['Unnamed: 0'])\n",
    "y_train = pd.read_csv('traininglabels.csv').drop(columns=['Unnamed: 0'])\n",
    "x_test = pd.read_csv('testingfeatures.csv').drop(columns=['Unnamed: 0'])\n",
    "y_test = pd.read_csv('testinglabels.csv').drop(columns=['Unnamed: 0'])\n",
    "temp_x_train=[]\n",
    "\n",
    "for row in x_train.iterrows():\n",
    "    index, data = row\n",
    "    temp_x_train.append(data.tolist())\n",
    "temp_y_train=[]\n",
    "\n",
    "for row in y_train.iterrows():\n",
    "    index, data = row\n",
    "    temp_y_train.append(data.tolist())\n",
    "\n",
    "temp_x_test=[]\n",
    "\n",
    "for row in x_test.iterrows():\n",
    "    index, data = row\n",
    "    temp_x_test.append(data.tolist())\n",
    "temp_y_test=[]\n",
    "\n",
    "for row in y_test.iterrows():\n",
    "    index, data = row\n",
    "    temp_y_test.append(data.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fc257e",
   "metadata": {},
   "source": [
    "# Start neuron network with 4 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f14e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.05\n",
    "num_steps = 10000\n",
    "batch_size = 1000\n",
    "display_step = 500\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 100 # 1st layer number of neurons\n",
    "n_hidden_2 = 50 # 2nd layer number of neurons\n",
    "n_hidden_3 = 20 # 3rd layer number of neurons\n",
    "num_input =  3 #  data input \n",
    "num_classes = 4 #  total classes (0-3 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2,n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_3, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "# Create model\n",
    "def neural_net(x):\n",
    "    # Hidden_1 fully connected layer \n",
    "    layer_1 = tf.nn.softmax(tf.add(tf.matmul(x, weights['h1']), biases['b1']))\n",
    "    # Hidden_2 fully connected layer \n",
    "    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']))\n",
    "    #Hidden_3 fully connected layer  \n",
    "    layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, weights['h3']), biases['b3']))\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_3, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "tf.summary.scalar('loss',loss_op)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "tf.summary.scalar('acuracy',accuracy)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "    \n",
    "x= np.array(temp_x_train)\n",
    "\n",
    "y=np.array(keras.utils.to_categorical(y_train))\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "\n",
    "# Define a batch method\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    merge_summary= tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter('C:/Users/BoyangWei.LAPTOP-SRSNTDRH/7390/TensorFlow/files/1')\n",
    "    writer.add_graph(sess.graph)\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x,batch_y=(x,y)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
    "            summary=sess.run(merge_summary,feed_dict={X: batch_x,Y: batch_y})\n",
    "            writer.add_summary(summary,step)\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \",Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    pred = logits  \n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Test ccuracy:\", accuracy.eval({X: np.array(temp_x_test), Y: np.array(keras.utils.to_categorical(y_test))}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a10c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92853d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951d5a13",
   "metadata": {},
   "source": [
    "# Another way to declear a neuron net work with 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fbc0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10000\n",
    "BATCH_SIZE = 1000\n",
    "display_step = 500\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "# using two numpy arrays\n",
    "features, labels = (X, Y)\n",
    "# make a simple model\n",
    "def Neuron(x):\n",
    "    net = tf.layers.dense(x, 100, activation=tf.tanh) # pass the first value \n",
    "    #from iter.get_next() as input\n",
    "    net = tf.layers.dense(net, 50, activation=tf.nn.relu)\n",
    "    net = tf.layers.dense(net, 20, activation=tf.nn.softmax)\n",
    "    prediction = tf.layers.dense(net, 4)\n",
    "    return prediction\n",
    "prediction = Neuron(X)\n",
    "\n",
    "# loss fuction\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=prediction, labels=Y)) \n",
    "\n",
    "# prepare accuracy\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# optimaze function\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# start to train and test\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    for i in range(EPOCHS):\n",
    "        _, loss_value,acc_value = sess.run([train_op, loss,accuracy],feed_dict={X: x, Y: y})\n",
    "        if i% display_step == 0:\n",
    "            print(\"Iter: {}, Loss: {:.4f}\".format(i+1, loss_value)) \n",
    "            print(\"Accurancy: \" +str(acc_value))\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    print(\"Test accuracy: \"+ str(accuracy.eval({X: np.array(temp_x_test), Y: np.array(keras.utils.to_categorical(y_test))})))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724877e6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "After training the data many times and tuning of hyper-parameters, we find that training accuracy actually approaches a certain value which is around 0.45. However, model proferms better in testing datasets. Even though the testing accuracy is not satisfying. All in all, there is no strong relationships amoung features and labels. It is easy to understand since a high budget does not mean a great popularity. And sometimes a high voted movie fails in revenue because of its topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71b0447",
   "metadata": {},
   "source": [
    "MIT License\n",
    "\n",
    "Copyright (c) 2018 BoyangWei\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a2c6b5",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/3.0/us/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/3.0/us/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/3.0/us/\">Creative Commons Attribution 3.0 United States License</a>."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
