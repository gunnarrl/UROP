{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e0c036f",
   "metadata": {},
   "source": [
    "# Autoencoder (Semi-supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f83ef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212f9fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(seed_value)\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "from keras import backend as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0965154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "# plt.style.use('fivethirtyeight')\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "VAL_SPLITS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import plot_confusion_matrix\n",
    "from cv_utils import run_cv_f1\n",
    "from cv_utils import plot_cv_roc\n",
    "from cv_utils import plot_cv_roc_prc\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e341da",
   "metadata": {},
   "source": [
    "For this part of the project, we will only work with the training set, that we will split again into train and validation to perform the hyperparameter tuning.\n",
    "\n",
    "We will save the test set for the final part, when we have already tuned our hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66461ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH,'df_train.csv'))\n",
    "df.drop(columns= df.columns[0:2],inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505bf2bf",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31088398",
   "metadata": {},
   "source": [
    "Although we are always using cross validation with `VAL_SPLITS` folds, (in general, 4), here we are gonna set only one split in order to explore how the Autoencoder works and get intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d989df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=1,test_size=0.15,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664356fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we want to select a subset of features\n",
    "# df_ = df[['Class','V9','V14','V16','V2','V3','V17']]\n",
    "df_ = df[['Class','V4','V14','V16','V12','V3','V17']]\n",
    "X = df_.drop(columns='Class').to_numpy()\n",
    "y = df_['Class'].to_numpy()\n",
    "\n",
    "for idx_t, idx_v in cv.split(X,y):\n",
    "    X_train = X[idx_t]\n",
    "    y_train = y[idx_t]\n",
    "    X_val = X[idx_v]\n",
    "    y_val = y[idx_v]\n",
    "    \n",
    "    # Now we need to erase the FRAUD cases on the TRAINING set  \n",
    "    X_train_normal = X_train[y_train==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a523fc69",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b686f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the size of our encoded representations\n",
    "ENCODED_DIM = 2\n",
    "INPUT_DIM = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcde5ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "def create_encoder(input_dim, encoded_dim):\n",
    "    encoder = Sequential([\n",
    "        Dense(32, input_shape=(input_dim,)),\n",
    "        LeakyReLU(),\n",
    "        Dense(16),\n",
    "        LeakyReLU(),\n",
    "        Dense(8),\n",
    "        LeakyReLU(),\n",
    "        Dense(encoded_dim)\n",
    "    ], name='encoder')\n",
    "    return encoder\n",
    "\n",
    "def create_decoder(input_dim, encoded_dim):\n",
    "    decoder = Sequential([\n",
    "    Dense(8, input_shape=(encoded_dim,) ),\n",
    "    LeakyReLU(),\n",
    "    Dense(16),\n",
    "    LeakyReLU(),\n",
    "    Dense(8),\n",
    "    LeakyReLU(),\n",
    "    Dense(input_dim)\n",
    "],name='decoder')\n",
    "    return decoder\n",
    "\n",
    "\n",
    "\n",
    "def create_autoencoder(input_dim, encoded_dim, return_encoder = True):\n",
    "    encoder = create_encoder(input_dim,encoded_dim)\n",
    "    decoder = create_decoder(input_dim,encoded_dim)\n",
    "    inp = Input(shape=(INPUT_DIM,),name='Input_Layer')\n",
    "\n",
    "    # a layer instance is callable on a tensor, and returns a tensor\n",
    "    x_enc = encoder(inp)\n",
    "    x_out = decoder(x_enc)\n",
    "\n",
    "\n",
    "    # This creates a model that includes\n",
    "    # the Input layer and three Dense layers\n",
    "    autoencoder = Model(inputs=inp, outputs=x_out)\n",
    "    if return_encoder:\n",
    "        return autoencoder, encoder\n",
    "    else:\n",
    "        return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1007628",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder, encoder = create_autoencoder(INPUT_DIM,ENCODED_DIM)\n",
    "print('ENCODER SUMMARY\\n')\n",
    "print(encoder.summary())\n",
    "print('AUTOENCODER SUMMARY\\n')\n",
    "print(autoencoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b7353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam',\n",
    "              loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b88854d",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e477e7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(x=X_train_normal, y= X_train_normal,\n",
    "                   batch_size=512,epochs=40, validation_split=0.1)  # starts training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690dd446",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b222ed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_enc = encoder.predict(X_val)\n",
    "X_enc_normal = X_enc[y_val==0]\n",
    "X_enc_fraud = X_enc[y_val==1]\n",
    "sns.scatterplot(x = X_enc_normal[:,0], y = X_enc_normal[:,1] ,label='Normal', alpha=0.5)\n",
    "sns.scatterplot(x = X_enc_fraud[:,0], y = X_enc_fraud[:,1] ,label='Fraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b809550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_out = autoencoder.predict(X_val)\n",
    "print(X_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c95c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50307a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.sum((X_out-X_val)**2,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74311eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0,np.max(distances),40)\n",
    "sns.distplot(distances[y_val==0],label='Normal',kde=False, \n",
    "             bins=bins, norm_hist=True, axlabel='Distance')\n",
    "sns.distplot(distances[y_val==1],label='Fraud',kde=False, bins=bins, norm_hist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e0a151",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0,100,40)\n",
    "sns.distplot(distances[y_val==0],label='Normal',kde=False, \n",
    "             bins=bins, norm_hist=True, axlabel='Distance')\n",
    "sns.distplot(distances[y_val==1],label='Fraud',kde=False, bins=bins, norm_hist=True)\n",
    "plt.xlim((0,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7e566f",
   "metadata": {},
   "source": [
    "## Validating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3200ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9e4176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_autoencoder(X,autoencoder,threshold):\n",
    "    \"\"\"\n",
    "    Classifier based in the autoencoder.\n",
    "    A datapoint is a nomaly if the distance of the original points\n",
    "    and the result of the autoencoder is greater than the threshold.\n",
    "    \"\"\"\n",
    "    X_out = autoencoder.predict(X)\n",
    "    distances = np.sum((X_out-X)**2,axis=1).reshape((-1,1))\n",
    "    # y_pred = 1 if it is anomaly\n",
    "    y_pred = 1.*(distances > threshold )\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed850718",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=VAL_SPLITS,test_size=0.15,random_state=0)\n",
    "# Thresholds to validate\n",
    "thresholds = np.linspace(0,100,100)\n",
    "# List with the f1 of all the thresholds at each validation fold\n",
    "f1_all = [] \n",
    "for i,(idx_t, idx_v) in enumerate(cv.split(X,y)):\n",
    "    X_train = X[idx_t]\n",
    "    y_train = y[idx_t]\n",
    "    X_val = X[idx_v]\n",
    "    y_val = y[idx_v]\n",
    "    # Now we need to erase the FRAUD cases on the TRAINING set  \n",
    "    X_train_normal = X_train[y_train==0]\n",
    "    \n",
    "    # Train the autoencoder\n",
    "    autoencoder, encoder = create_autoencoder(INPUT_DIM,ENCODED_DIM)\n",
    "    autoencoder.compile(optimizer='adam',\n",
    "              loss='mean_squared_error')\n",
    "    autoencoder.fit(x=X_train_normal, y= X_train_normal,\n",
    "                   batch_size=512,epochs=30, shuffle=True,\n",
    "                   verbose=0)  # starts training\n",
    "    \n",
    "    # Plot of the validation set in the embedding space\n",
    "    X_enc = encoder.predict(X_val)\n",
    "    X_enc_normal = X_enc[y_val==0]\n",
    "    X_enc_fraud = X_enc[y_val==1]\n",
    "    sns.scatterplot(x = X_enc_normal[:,0], y = X_enc_normal[:,1] ,label='Normal', alpha=0.5)\n",
    "    sns.scatterplot(x = X_enc_fraud[:,0], y = X_enc_fraud[:,1] ,label='Fraud')\n",
    "    plt.show()\n",
    "    \n",
    "    # Transformation of the points through the autoencoder \n",
    "    # and calculate the predictions\n",
    "    y_preds=clf_autoencoder(X_val,autoencoder,thresholds)\n",
    "    \n",
    "    metrics_f1 = np.array([ f1_score(y_val,y_pred) for y_pred in y_preds.T  ])\n",
    "    f1_all.append(metrics_f1)\n",
    "    \n",
    "    # Save the models into files for future use\n",
    "    autoencoder.save('models_autoencoder/autoencoder_fold_'+str(i+1)+'.h5')\n",
    "    encoder.save('models_autoencoder/encoder_fold_'+str(i+1)+'.h5')\n",
    "    del(autoencoder,encoder)\n",
    "\n",
    "f1_mean = np.mean(f1_all,axis=0) \n",
    "# Plot of F1-Threshold curves\n",
    "for i,f1_fold in enumerate(f1_all):\n",
    "    sns.lineplot(thresholds,f1_fold, label='Fold '+str(i+1))\n",
    "sns.scatterplot(thresholds,f1_mean,label='Mean')\n",
    "plt.show()\n",
    "f1_opt = f1_mean.max()\n",
    "threshold_opt = thresholds[np.argmax(f1_mean)]\n",
    "print('F1 max = {:.3f} at threshold = {:.3f}'.format(f1_opt,threshold_opt))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
