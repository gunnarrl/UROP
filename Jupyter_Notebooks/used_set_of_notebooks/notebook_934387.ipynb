{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f6e49a",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "SVM hypothesis and cost function can be derived from logistic regression hypothesis. For logistic regression hypothesis is defined as:\n",
    "$$\n",
    "h_{\\theta} = \\frac{1}{1+e^{-\\theta^{T}x}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "* if $y=1$, we want $h_{\\theta}(x) \\approx 1$, $\\theta^T x \\gg 0 $\n",
    "* if $y=1$, we want $h_{\\theta}(x) \\approx 0$, $\\theta^T x \\ll 0 $\n",
    "\n",
    "Cost for a single example can be written as:\n",
    "\n",
    "$$\n",
    "-\\bigg(y log\\big(h_{\\theta}(x)\\big) + (1 - y) log\\big(1 - h_{\\theta}(x)\\big)\\bigg) \\\\=\\\\ -y log\\frac{1}{1+e^{-\\theta^{T}x}} -(1-y)log(1 - \\frac{1}{1+e^{-\\theta^{T}x}})\n",
    "$$\n",
    "\n",
    "And finally, from here, the whole optimization process for regularized logistic regression is defined as:\n",
    "$$\n",
    "\\underset{\\theta}{\\min} \\frac{1}{m}\\bigg[\\sum_{i=1}^{m}y^{(i)}\\big( -log h_{\\theta}(x^{(i)})\\big) + (1 - y^{(i)})\\big( -log(1 - h_{\\theta}(x^{(i)}))\\big) \\bigg] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}\n",
    "$$\n",
    "where $\\lambda$ is the regularization parameter.\n",
    "\n",
    "For SVMs the regularization parameter is $C$ and is defined as the inverse of $\\lambda$. This parameter is now applied to the left portion of optimization term.\n",
    "\n",
    "Instead of logarithmic cost, SVM's cost is linear (is shown in figure below the code in the next cell) and optimization for SVM is as follows:\n",
    "$$\n",
    "\\underset{\\theta}{\\min} C\\bigg[\\sum_{i=1}^{m}y^{(i)}\\big(cost_{1}(\\theta^T x^{(i)})\\big) + (1 - y^{(i)})\\big(cost_{0}(\\theta^T x^{(i)})\\big) \\bigg] + \\frac{1}{2}\\sum_{j=1}^{n}\\theta_{j}^{2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258035d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "%matplotlib inline\n",
    "from sklearn.svm import SVC\n",
    "from scipy.io import loadmat\n",
    "from sympy import Symbol, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e357c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain\n",
    "x1 = -3\n",
    "x2 = 3\n",
    "x = np.linspace(x1, x2, 100)\n",
    "\n",
    "# logreg cost\n",
    "c_lr = lambda x: - np.log(1/(1 + np.exp(-x)))\n",
    "c_lr_vals = c_lr(x)\n",
    "\n",
    "# 1st derivative of logreg cost\n",
    "h = 0.01\n",
    "diff1_approx = lambda x :(c_lr(x+h)-c_lr(x-h))/(2*h)\n",
    "diff1_act = lambda x: -1/(1+np.exp(x))\n",
    "\n",
    "# svm cost        \n",
    "k = -0.645 # manually configured slope\n",
    "c_svm = [k * x - k if x<=1 else 0 for x in x]\n",
    "    \n",
    "# visualize   \n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(x, c_lr(x), label='logistic regression cost')\n",
    "plt.plot(x, diff1_act(x), label='theoretical derivative of logreg cost')\n",
    "plt.plot(x, diff1_approx(x), marker='x', markevery=5, linestyle='None', label='numerical derivative of logreg cost')\n",
    "plt.plot(x, c_svm, label='svm cost')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcf5ba1",
   "metadata": {},
   "source": [
    "## 1 Support vector machines - examples\n",
    "\n",
    "### 1.1 Example dataset 1\n",
    "\n",
    "In this example, 2D data should be separated by a linear boundary. Because of a single positive outlier at about (0.1, 4.1), regularization parameter C will drasticly affect the position of the linear boundary.\n",
    "\n",
    "Informally, the C parameter is a positive value that controls the penalty for misclassified training examples; a large C parameter tells the SVM to try to classify **all** the examples correctly. It plays a role similar to $\\frac{1}{\\lambda}$, where $\\lambda$ is the regularization parameter for regularized logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838293fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = loadmat('data/ex6data1.mat')\n",
    "print(dataset1['__header__'])\n",
    "X = dataset1['X']\n",
    "y = dataset1['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91c327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(X, y):\n",
    "    pos = np.where(y==1)[0]\n",
    "    neg = np.where(y==0)[0]\n",
    "    plt.plot(X[pos, 0], X[pos, 1], \n",
    "             marker='+',\n",
    "             color='black',\n",
    "             markersize=7,\n",
    "             linestyle='None',\n",
    "             label='pos examps')\n",
    "\n",
    "    plt.plot(X[neg, 0], X[neg, 1], \n",
    "             marker='o',\n",
    "             color='red',\n",
    "             markersize=7,\n",
    "             linestyle='None',\n",
    "             label='neg examps')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    \n",
    "    \n",
    "def trainSVM(X, y, C, kernel, tol, max_iter):\n",
    "    return SVC(C, kernel=kernel, tol=tol, max_iter=max_iter)\n",
    "\n",
    "def visualizeBoundary(X, y, model):\n",
    "    # make classification predictions over a grid of values\n",
    "    x1plot = np.linspace(np.min(X[:, 0]), np.max(X[:, 0]), num=100)\n",
    "    x2plot = np.linspace(np.min(X[:, 1]), np.max(X[:, 1]), num=100)\n",
    "    preds = np.zeros((len(x1plot), len(x2plot)))\n",
    "    for i, x1 in enumerate(x1plot):\n",
    "        for j, x2 in enumerate(x2plot):\n",
    "            preds[i,j] = model.predict(np.array([x1, x2]).reshape(1, -1))\n",
    "    \n",
    "    #X1, X2 = np.meshgrid(x1plot, x2plot)\n",
    "    contr = plt.contour(x1plot, x2plot, preds.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf78de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotData(X, y)\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d2eed2",
   "metadata": {},
   "source": [
    "#### C = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858e5e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a model\n",
    "C = 1\n",
    "kernel = 'linear'\n",
    "tol = 1e-3\n",
    "max_iter=1000\n",
    "model = trainSVM(X, y, C, kernel, tol, max_iter)\n",
    "model.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc69ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the SVM boundary\n",
    "plotData(X, y)\n",
    "visualizeBoundary(X, y, model)\n",
    "plt.axis([np.min(X[:, 0])-0.2, np.max(X[:, 0])+0.2,\n",
    "         np.min(X[:, 1])-0.2, np.max(X[:, 1])+0.2])\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4b2efa",
   "metadata": {},
   "source": [
    "#### Changing the C parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609b4c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = [0.1, 1, 10, 100]\n",
    "for i, C in enumerate(Cs):\n",
    "    kernel = 'linear'\n",
    "    tol = 1e-2\n",
    "    max_iter=1000\n",
    "    model = trainSVM(X, y, C, kernel, tol, max_iter)\n",
    "    model.fit(X, y.ravel())\n",
    "    \n",
    "    plt.figure(figsize=(12,9))\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plotData(X, y)\n",
    "    plt.axis([np.min(X[:, 0])-0.2, np.max(X[:, 0])+0.2,\n",
    "             np.min(X[:, 1])-0.2, np.max(X[:, 1])+0.2])\n",
    "    visualizeBoundary(X, y, model)\n",
    "    plt.title(f'C = {C}')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5091a1e3",
   "metadata": {},
   "source": [
    "### 1.2 SVM for non-linearly separable data\n",
    "\n",
    "#### 1.2.1 Gaussian kernel\n",
    "\n",
    "To find non-linear decision boundaries with the SVM, input features have to be mapped to higher dimension (in order to separate the data linearly with a hyperplane). \n",
    "\n",
    "The most used mapping function is Gaussian kernel or radial basis function. Gaussian kernel is similarity function that measures the 'distance' between a pair of examples, $(x^{(i)}, x^{(j)})$. The Gaussian kernel is parametrized by a bandwidth parameter, $\\sigma$, which determines how fast the similarity metrics decreases to 0 as the examples are further apart. \n",
    "$$\n",
    "K_{gasussian}(x^{(i)}, x^{(j)}) = exp(-\\frac{||{x^{(i)} - x^{(j)}||}^2}{2\\sigma^{2}} = exp\\bigg(-\\frac{\\sum_{k=1}^{n}(x_{k}^{(i)} - x_{k}^{(j)})^2)}{2\\sigma^2}\\bigg)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4795ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussianKernel(x1, x2, sigma):\n",
    "    x1 = x1.flatten()\n",
    "    x2 = x2.flatten()\n",
    "    \n",
    "    return np.exp(-np.sum((x1-x2)**2)/(2*sigma**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145ad567",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([1, 2, 1])\n",
    "x2 = np.array([0, 4, -1])\n",
    "sigma = 2\n",
    "\n",
    "sim = gaussianKernel(x1, x2, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cc8615",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Gaussian kernel between x1={x1} and x2={x2} with sigma={sigma} is {sim}')\n",
    "print(f'Expected value ~ 0.324652')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3e61bb",
   "metadata": {},
   "source": [
    "#### 1.2.2 Example dataset 2\n",
    "\n",
    "This dataset contains data without linear decision boundary that separates the positive and negative examples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef97d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = loadmat('data/ex6data2.mat')\n",
    "print(dataset2['__header__'])\n",
    "X = dataset2['X']\n",
    "y = dataset2['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b90980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plotData(X, y)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940ebe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rbf is ootb sklearn gaussian kernel\n",
    "# instead of sigma it takes gamma=1/sigma^2 as parameter\n",
    "C = 1\n",
    "sigma = 0.1\n",
    "model = SVC(C=C,\n",
    "            kernel='rbf',\n",
    "            gamma=sigma**(-2),\n",
    "           )\n",
    "model.fit(X, y.ravel())\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plotData(X, y)\n",
    "plt.axis([np.min(X[:, 0])-0.05, np.max(X[:, 0])+0.05,\n",
    "         np.min(X[:, 1])-0.05, np.max(X[:, 1])+0.05])\n",
    "visualizeBoundary(X, y, model)\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653d3141",
   "metadata": {},
   "source": [
    "#### 1.2.2 Example dataset 2\n",
    "\n",
    "In the provided dataset, *ex6data3.mat*, the given variables are $X$,\n",
    "$y$, $Xval$, $yval$.\n",
    "The task is to use the cross validation set to determine the best C and $\\sigma$ parameter to use. \n",
    "\n",
    "For both C and $\\sigma$, suggested values are 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30. All possible pairs of values for C and $\\sigma$ should be examined, where the total number of models for suggested list is 64 (8^2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ad5b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = loadmat('data/ex6data3.mat')\n",
    "print(dataset3['__header__'])\n",
    "X = dataset3['X']\n",
    "y = dataset3['y']\n",
    "Xval = dataset3['Xval']\n",
    "yval = dataset3['yval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ac093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plotData(X, y)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34306b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvParams(X, y, Xval, yval):\n",
    "    params = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30]\n",
    "    \n",
    "    opt_accuracy = 0\n",
    "    opt_C, opt_sigma = 0, 0\n",
    "    \n",
    "    for i, C in enumerate(params):\n",
    "        for j, sigma in enumerate(params):\n",
    "            model = SVC(C, kernel='rbf', gamma=1/(sigma**2))\n",
    "            model.fit(X, y.ravel())\n",
    "            predictions = model.predict(Xval)\n",
    "            accuracy = np.mean(predictions.ravel() == yval.ravel())\n",
    "            if accuracy > opt_accuracy:\n",
    "                opt_accuracy = accuracy\n",
    "                opt_C = C\n",
    "                opt_sigma = sigma\n",
    "    return opt_C, opt_sigma, opt_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f77e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "C, sigma, acc = cvParams(X, y, Xval, yval)\n",
    "model = SVC(C, kernel='rbf', gamma=1/(sigma**2))\n",
    "model.fit(X, y.ravel())\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plotData(X, y)\n",
    "plt.axis([np.min(X[:, 0])-0.05, np.max(X[:, 0])+0.05,\n",
    "         np.min(X[:, 1])-0.05, np.max(X[:, 1])+0.05])\n",
    "visualizeBoundary(X, y, model)\n",
    "plt.legend(loc='lower left')\n",
    "plt.title(f'accuracy = {acc*100}%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3071ef1",
   "metadata": {},
   "source": [
    "## 2 Spam classification\n",
    "\n",
    "TBA"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
