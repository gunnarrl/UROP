{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9a2897f",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986c5cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import shutil\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import set_random_seed\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers, applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Dropout, GlobalAveragePooling2D, Input\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback, LearningRateScheduler\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    set_random_seed(0)\n",
    "\n",
    "seed = 0\n",
    "seed_everything(seed)\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append(os.path.abspath('../input/efficientnet/efficientnet-master/efficientnet-master/'))\n",
    "from efficientnet import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a0ecf6",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91fca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_out_set = pd.read_csv('../input/aptos-data-split/hold-out.csv')\n",
    "X_train = hold_out_set[hold_out_set['set'] == 'train']\n",
    "X_val = hold_out_set[hold_out_set['set'] == 'validation']\n",
    "test = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\n",
    "print('Number of train samples: ', X_train.shape[0])\n",
    "print('Number of validation samples: ', X_val.shape[0])\n",
    "print('Number of test samples: ', test.shape[0])\n",
    "\n",
    "# Preprocecss data\n",
    "X_train[\"id_code\"] = X_train[\"id_code\"].apply(lambda x: x + \".png\")\n",
    "X_val[\"id_code\"] = X_val[\"id_code\"].apply(lambda x: x + \".png\")\n",
    "test[\"id_code\"] = test[\"id_code\"].apply(lambda x: x + \".png\")\n",
    "display(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e244e10",
   "metadata": {},
   "source": [
    "# Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cce503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 30\n",
    "WARMUP_EPOCHS = 5\n",
    "LEARNING_RATE = 1e-4\n",
    "WARMUP_LEARNING_RATE = 1e-3\n",
    "HEIGHT = 456\n",
    "WIDTH = 456\n",
    "CHANNELS = 3\n",
    "ES_PATIENCE = 5\n",
    "RLROP_PATIENCE = 3\n",
    "DECAY_DROP = 0.5\n",
    "LR_WARMUP_EPOCHS_1st = 2\n",
    "LR_WARMUP_EPOCHS_2nd = 5\n",
    "STEP_SIZE = (5 * (len(X_train) // BATCH_SIZE)) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49be38ed",
   "metadata": {},
   "source": [
    "# Pre-procecess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37bc2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_base_path = '../input/aptos2019-blindness-detection/train_images/'\n",
    "test_base_path = '../input/aptos2019-blindness-detection/test_images/'\n",
    "train_dest_path = 'base_dir/train_images/'\n",
    "validation_dest_path = 'base_dir/validation_images/'\n",
    "test_dest_path =  'base_dir/test_images/'\n",
    "\n",
    "# Making sure directories don't exist\n",
    "if os.path.exists(train_dest_path):\n",
    "    shutil.rmtree(train_dest_path)\n",
    "if os.path.exists(validation_dest_path):\n",
    "    shutil.rmtree(validation_dest_path)\n",
    "if os.path.exists(test_dest_path):\n",
    "    shutil.rmtree(test_dest_path)\n",
    "    \n",
    "# Creating train, validation and test directories\n",
    "os.makedirs(train_dest_path)\n",
    "os.makedirs(validation_dest_path)\n",
    "os.makedirs(test_dest_path)\n",
    "\n",
    "def crop_image(img, tol=7):\n",
    "    if img.ndim ==2:\n",
    "        mask = img>tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    elif img.ndim==3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img>tol\n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "            \n",
    "        return img\n",
    "\n",
    "def circle_crop(img):\n",
    "    img = crop_image(img)\n",
    "\n",
    "    height, width, depth = img.shape\n",
    "    largest_side = np.max((height, width))\n",
    "    img = cv2.resize(img, (largest_side, largest_side))\n",
    "\n",
    "    height, width, depth = img.shape\n",
    "\n",
    "    x = width//2\n",
    "    y = height//2\n",
    "    r = np.amin((x, y))\n",
    "\n",
    "    circle_img = np.zeros((height, width), np.uint8)\n",
    "    cv2.circle(circle_img, (x, y), int(r), 1, thickness=-1)\n",
    "    img = cv2.bitwise_and(img, img, mask=circle_img)\n",
    "    img = crop_image(img)\n",
    "\n",
    "    return img\n",
    "    \n",
    "def preprocess_image(base_path, save_path, image_id, HEIGHT, WIDTH, sigmaX=10):\n",
    "    image = cv2.imread(base_path + image_id)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = circle_crop(image)\n",
    "    image = cv2.resize(image, (HEIGHT, WIDTH))\n",
    "    image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0,0), sigmaX), -4 , 128)\n",
    "    cv2.imwrite(save_path + image_id, image)\n",
    "    \n",
    "# Pre-procecss train set\n",
    "for i, image_id in enumerate(X_train['id_code']):\n",
    "    preprocess_image(train_base_path, train_dest_path, image_id, HEIGHT, WIDTH)\n",
    "    \n",
    "# Pre-procecss validation set\n",
    "for i, image_id in enumerate(X_val['id_code']):\n",
    "    preprocess_image(train_base_path, validation_dest_path, image_id, HEIGHT, WIDTH)\n",
    "    \n",
    "# Pre-procecss test set\n",
    "for i, image_id in enumerate(test['id_code']):\n",
    "    preprocess_image(test_base_path, test_dest_path, image_id, HEIGHT, WIDTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e772a30",
   "metadata": {},
   "source": [
    "# Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5096edb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen=ImageDataGenerator(rescale=1./255, \n",
    "                           rotation_range=360,\n",
    "                           horizontal_flip=True,\n",
    "                           vertical_flip=True,\n",
    "                           zoom_range=[0.75,1],\n",
    "                           fill_mode='constant',\n",
    "                           cval=0)\n",
    "\n",
    "train_generator=datagen.flow_from_dataframe(\n",
    "                          dataframe=X_train,\n",
    "                          directory=train_dest_path,\n",
    "                          x_col=\"id_code\",\n",
    "                          y_col=\"diagnosis\",\n",
    "                          class_mode=\"raw\",\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          target_size=(HEIGHT, WIDTH),\n",
    "                          seed=seed)\n",
    "\n",
    "valid_generator=datagen.flow_from_dataframe(\n",
    "                        dataframe=X_val,\n",
    "                        directory=validation_dest_path,\n",
    "                        x_col=\"id_code\",\n",
    "                        y_col=\"diagnosis\",\n",
    "                        class_mode=\"raw\",\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        target_size=(HEIGHT, WIDTH),\n",
    "                        seed=seed)\n",
    "\n",
    "test_generator=datagen.flow_from_dataframe(  \n",
    "                          dataframe=test,\n",
    "                          directory=test_dest_path,\n",
    "                          x_col=\"id_code\",\n",
    "                          batch_size=1,\n",
    "                          class_mode=None,\n",
    "                          shuffle=False,\n",
    "                          target_size=(HEIGHT, WIDTH),\n",
    "                          seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80d606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency.\n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore\n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where\n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored\n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on\n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or\n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    # Example for CIFAR-10 w/ batch size 100:\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    # References\n",
    "      - [Cyclical Learning Rates for Training Neural Networks](\n",
    "      https://arxiv.org/abs/1506.01186)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                base_lr=0.001,\n",
    "                max_lr=0.006,\n",
    "                step_size=2000.,\n",
    "                mode='triangular',\n",
    "                gamma=1.,\n",
    "                scale_fn=None,\n",
    "                scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range']:\n",
    "            raise KeyError(\"mode must be one of 'triangular', \"\"'triangular2', or 'exp_range'\")\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1 / (2.**(x - 1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma ** x\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        if new_base_lr is not None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr is not None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size is not None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "\n",
    "    def clr(self):\n",
    "        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n",
    "        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr - self.base_lr) * \\\n",
    "                np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr - self.base_lr) * \\\n",
    "                np.maximum(0, (1 - x)) * self.scale_fn(self.clr_iterations)\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "\n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "        self.history.setdefault(\n",
    "            'lr', []).append(\n",
    "            K.get_value(\n",
    "                self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34981f1d",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78f15c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    base_model = EfficientNetB5(weights=None, \n",
    "                                include_top=False,\n",
    "                                input_tensor=input_tensor)\n",
    "    base_model.load_weights('../input/efficientnet-keras-weights-b0b5/efficientnet-b5_imagenet_1000_notop.h5')\n",
    "\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(2048, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    final_output = Dense(1, activation='linear', name='final_output')(x)\n",
    "    model = Model(input_tensor, final_output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f356e592",
   "metadata": {},
   "source": [
    "# Train top layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b48ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(input_shape=(HEIGHT, WIDTH, CHANNELS))\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "for i in range(-5, 0):\n",
    "    model.layers[i].trainable = True\n",
    "\n",
    "metric_list = [\"accuracy\"]\n",
    "optimizer = optimizers.Adam(lr=WARMUP_LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=metric_list)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bbbeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID = valid_generator.n//valid_generator.batch_size\n",
    "\n",
    "history_warmup = model.fit_generator(generator=train_generator,\n",
    "                                     steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                                     validation_data=valid_generator,\n",
    "                                     validation_steps=STEP_SIZE_VALID,\n",
    "                                     epochs=WARMUP_EPOCHS,\n",
    "                                     verbose=2).history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad446ea",
   "metadata": {},
   "source": [
    "# Fine-tune the complete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad2b8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=ES_PATIENCE, restore_best_weights=True, verbose=1)\n",
    "cyclic_lr = CyclicLR(base_lr=(LEARNING_RATE / 20),\n",
    "                     max_lr=(LEARNING_RATE * 2),\n",
    "                     step_size=STEP_SIZE,\n",
    "                     mode='triangular2')\n",
    "\n",
    "callback_list = [es, cyclic_lr]\n",
    "optimizer = optimizers.Adam(lr=LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=metric_list)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4246aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                              validation_data=valid_generator,\n",
    "                              validation_steps=STEP_SIZE_VALID,\n",
    "                              epochs=EPOCHS,\n",
    "                              callbacks=callback_list,\n",
    "                              verbose=2).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f793d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 4))\n",
    "\n",
    "ax.plot(cyclic_lr.history['lr'])\n",
    "ax.set_title('Fine-tune learning rates')\n",
    "\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Learning rate')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ea4e4b",
   "metadata": {},
   "source": [
    "# Model loss graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07f030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex='col', figsize=(20, 14))\n",
    "\n",
    "ax1.plot(history['loss'], label='Train loss')\n",
    "ax1.plot(history['val_loss'], label='Validation loss')\n",
    "ax1.legend(loc='best')\n",
    "ax1.set_title('Loss')\n",
    "\n",
    "ax2.plot(history['acc'], label='Train accuracy')\n",
    "ax2.plot(history['val_acc'], label='Validation accuracy')\n",
    "ax2.legend(loc='best')\n",
    "ax2.set_title('Accuracy')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dae6fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty arays to keep the predictions and labels\n",
    "df_preds = pd.DataFrame(columns=['label', 'pred', 'set'])\n",
    "train_generator.reset()\n",
    "valid_generator.reset()\n",
    "\n",
    "# Add train predictions and labels\n",
    "for i in range(STEP_SIZE_TRAIN + 1):\n",
    "    im, lbl = next(train_generator)\n",
    "    preds = model.predict(im, batch_size=train_generator.batch_size)\n",
    "    for index in range(len(preds)):\n",
    "        df_preds.loc[len(df_preds)] = [lbl[index], preds[index][0], 'train']\n",
    "\n",
    "# Add validation predictions and labels\n",
    "for i in range(STEP_SIZE_VALID + 1):\n",
    "    im, lbl = next(valid_generator)\n",
    "    preds = model.predict(im, batch_size=valid_generator.batch_size)\n",
    "    for index in range(len(preds)):\n",
    "        df_preds.loc[len(df_preds)] = [lbl[index], preds[index][0], 'validation']\n",
    "\n",
    "df_preds['label'] = df_preds['label'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b7d672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(x):\n",
    "    if x < 0.5:\n",
    "        return 0\n",
    "    elif x < 1.5:\n",
    "        return 1\n",
    "    elif x < 2.5:\n",
    "        return 2\n",
    "    elif x < 3.5:\n",
    "        return 3\n",
    "    return 4\n",
    "\n",
    "# Classify predictions\n",
    "df_preds['predictions'] = df_preds['pred'].apply(lambda x: classify(x))\n",
    "\n",
    "train_preds = df_preds[df_preds['set'] == 'train']\n",
    "validation_preds = df_preds[df_preds['set'] == 'validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b0a414",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185cc3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['0 - No DR', '1 - Mild', '2 - Moderate', '3 - Severe', '4 - Proliferative DR']\n",
    "def evaluate_model(train, validation):\n",
    "    train_labels, train_preds = train\n",
    "    validation_labels, validation_preds = validation\n",
    "    print(\"Train        Cohen Kappa score: %.3f\" % cohen_kappa_score(train_preds, train_labels, weights='quadratic'))\n",
    "    print(\"Validation   Cohen Kappa score: %.3f\" % cohen_kappa_score(validation_preds, validation_labels, weights='quadratic'))\n",
    "    print(\"Complete set Cohen Kappa score: %.3f\" % cohen_kappa_score(np.append(train_preds, validation_preds), np.append(train_labels, validation_labels), weights='quadratic'))\n",
    "    print(' \\t\\t\\t\\t   TRAIN')\n",
    "    print(classification_report(train_labels, train_preds, target_names=labels))\n",
    "    print(' \\t\\t\\t\\t VALIDATION')\n",
    "    print(classification_report(validation_labels, validation_preds, target_names=labels))\n",
    "    \n",
    "evaluate_model((train_preds['label'], train_preds['predictions']), (validation_preds['label'], validation_preds['predictions']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a53339",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5ed93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(train, validation, labels=labels):\n",
    "    train_labels, train_preds = train\n",
    "    validation_labels, validation_preds = validation\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, sharex='col', figsize=(24, 7))\n",
    "    train_cnf_matrix = confusion_matrix(train_labels, train_preds)\n",
    "    validation_cnf_matrix = confusion_matrix(validation_labels, validation_preds)\n",
    "\n",
    "    train_cnf_matrix_norm = train_cnf_matrix.astype('float') / train_cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    validation_cnf_matrix_norm = validation_cnf_matrix.astype('float') / validation_cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    train_df_cm = pd.DataFrame(train_cnf_matrix_norm, index=labels, columns=labels)\n",
    "    validation_df_cm = pd.DataFrame(validation_cnf_matrix_norm, index=labels, columns=labels)\n",
    "\n",
    "    sns.heatmap(train_df_cm, annot=True, fmt='.2f', cmap=\"Blues\",ax=ax1).set_title('Train')\n",
    "    sns.heatmap(validation_df_cm, annot=True, fmt='.2f', cmap=sns.cubehelix_palette(8),ax=ax2).set_title('Validation')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix((train_preds['label'], train_preds['predictions']), (validation_preds['label'], validation_preds['predictions']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c3c9c1",
   "metadata": {},
   "source": [
    "## Apply model to test set and output predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3065991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tta(model, generator, steps=10):\n",
    "    step_size = generator.n//generator.batch_size\n",
    "    preds_tta = []\n",
    "    for i in range(steps):\n",
    "        generator.reset()\n",
    "        preds = model.predict_generator(generator, steps=step_size)\n",
    "        preds_tta.append(preds)\n",
    "\n",
    "    return np.mean(preds_tta, axis=0)\n",
    "\n",
    "preds = apply_tta(model, test_generator)\n",
    "predictions = [classify(x) for x in preds]\n",
    "\n",
    "results = pd.DataFrame({'id_code':test['id_code'], 'diagnosis':predictions})\n",
    "results['id_code'] = results['id_code'].map(lambda x: str(x)[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d7b955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning created directories\n",
    "if os.path.exists(train_dest_path):\n",
    "    shutil.rmtree(train_dest_path)\n",
    "if os.path.exists(validation_dest_path):\n",
    "    shutil.rmtree(validation_dest_path)\n",
    "if os.path.exists(test_dest_path):\n",
    "    shutil.rmtree(test_dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed9c31d",
   "metadata": {},
   "source": [
    "# Predictions class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca6157",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(sharex='col', figsize=(24, 8.7))\n",
    "sns.countplot(x=\"diagnosis\", data=results, palette=\"GnBu_d\").set_title('Test')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc42b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('submission.csv', index=False)\n",
    "display(results.head())"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
