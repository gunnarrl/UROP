{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f17380f",
   "metadata": {},
   "source": [
    "# Building systematics maps for 3x2pt with Spark\n",
    "\n",
    "<br>Kernel: desc-pyspark\n",
    "<br>Owner: **S Plaszczynski** \n",
    "<br>Last Verified to Run: **2019-01-10**\n",
    "\n",
    "The goal of this notebook is to show how to build (simply) Healpix maps from the DC2 \n",
    "DPDD output inorder to test for possible 3x2pt systematics.\n",
    "It is illustrated on the current run1.2p production.\n",
    "It also shows how Spark can be used for data analysis (for more details see: https://arxiv.org/abs/1807.03078)\n",
    "Note that the full power of Spark will reveal when more data will be available.\n",
    "\n",
    "The advantages of using Spark are:\n",
    "- one can put the relevant variables in cache\n",
    "- computation automatically optimised (lazy evaluation)\n",
    "- the analysis will scale when more data will be available\n",
    "- Spark is available at NERSC (as this notebook shows). jupyter-dev is limited to running on 4 threads ie 8GB mem. For (much) more memory use the interactive or batch mode, see https://github.com/LSSTDESC/desc-spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c2efcf",
   "metadata": {},
   "source": [
    "# reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3325172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialise our Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(\"spark session started\")\n",
    "\n",
    "#usefull tool to benchmark\n",
    "from time import time\n",
    "class Timer:\n",
    "    \"\"\"\n",
    "    a simple class for printing time (s) since last call\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.t0=time()\n",
    "    \n",
    "    def start(self):\n",
    "        self.t0=time()\n",
    "        \n",
    "    def stop(self):\n",
    "        t1=time()\n",
    "        print(\"{:2.1f}s\".format(t1-self.t0))\n",
    "\n",
    "timer=Timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920ac685",
   "metadata": {},
   "outputs": [],
   "source": [
    "timer.start()\n",
    "df_all=spark.read.parquet(\"/global/cscratch1/sd/plaszczy/Run1.2p/object_catalog/full_catalog.parquet\")\n",
    "df_all.printSchema()\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c7d49",
   "metadata": {},
   "source": [
    "select interesting columns (for this example we will only use the i band)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b55360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build selection by appending to string\n",
    "cols=[\"ra\",\"dec\",\"good\",\"clean\",\"extendedness\",\"blendedness\",\"mag_i_cModel\",\"magerr_i_cModel\",\"snr_i_cModel\",\\\n",
    "      \"psf_fwhm_i\",\"Ixx_i\",\"Iyy_i\",\"Ixy_i\",\"IxxPSF_i\",\"IyyPSF_i\",\"IxyPSF_i\"]\n",
    "print(cols)\n",
    "#use these columns\n",
    "df=df_all.select(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d9d78a",
   "metadata": {},
   "source": [
    "Apply some quality cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6093beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.filter( (df.good==True)& \\\n",
    "            (df.clean==True) & \\\n",
    "             (df.extendedness>0.9) & \\\n",
    "             (df.blendedness < 10**(-0.375)) &\\\n",
    "            (df.mag_i_cModel< 24.5) &\\\n",
    "             (df.snr_i_cModel>10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0f3201",
   "metadata": {},
   "source": [
    "Add a column of healpixels (mapReduce way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df701911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "nside=2048\n",
    "#create the ang2pix user-defined-function. \n",
    "#we use pandas_udf because they are more efficient\n",
    "@pandas_udf('int', PandasUDFType.SCALAR)\n",
    "def Ang2Pix(ra,dec):\n",
    "    return pd.Series(hp.ang2pix(nside,np.radians(90-dec),np.radians(ra)))\n",
    "\n",
    "#add a column of healpix indices\n",
    "df=df.withColumn(\"ipix\",Ang2Pix(\"ra\",\"dec\"))\n",
    "#groupby indices and count the number of elements in each group\n",
    "df_map=df.groupBy(\"ipix\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3485fb36",
   "metadata": {},
   "source": [
    "Drop all Nans and put in cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dff3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "timer.start()\n",
    "df.na.drop().cache()\n",
    "print(\"sample has {}M objects\".format(df.count()/1e6))\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455814cb",
   "metadata": {},
   "source": [
    "## Mean counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae1393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "timer.start()\n",
    "#groupby indices and count the number of elements in each group\n",
    "df_map=df.groupBy(\"ipix\").count()\n",
    "#statistics per pixel\n",
    "df_map.describe(['count']).show() \n",
    "#back to python world\n",
    "map_p=df_map.toPandas()\n",
    "#now data is reduced create the healpy map\n",
    "map_c = np.zeros(hp.nside2npix(nside))\n",
    "map_c[map_p['ipix'].values]=map_p['count'].values\n",
    "#map_c[map_c==0]=hp.UNSEEN\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016fb267",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('jet')\n",
    "hp.gnomview(map_c,rot=[55,-29.8],reso=hp.nside2resol(nside,arcmin=True),max=80,title='counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a4f579",
   "metadata": {},
   "source": [
    "## Sky sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d35649",
   "metadata": {},
   "outputs": [],
   "source": [
    "var=\"magerr_i_cModel\"\n",
    "var_sys=\"avg(\"+var+\")\"\n",
    "df_map=df.groupBy(\"ipix\").mean(var)\n",
    "df_map.describe([var_sys]).show() \n",
    "dfp=df_map.toPandas()\n",
    "map_s = np.zeros(hp.nside2npix(nside))\n",
    "map_s[dfp['ipix'].values]=dfp[var_sys].values\n",
    "hp.gnomview(map_s,rot=[55,-29.8],reso=hp.nside2resol(nside,arcmin=True),title=var_sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cdc50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "var='snr_i_cModel'\n",
    "var_sys=\"avg(\"+var+\")\"\n",
    "df_map=df.groupBy(\"ipix\").mean(var)\n",
    "df_map.describe([var_sys]).show() \n",
    "dfp=df_map.toPandas()\n",
    "map_s = np.zeros(hp.nside2npix(nside))\n",
    "map_s[dfp['ipix'].values]=dfp[var_sys].values\n",
    "hp.gnomview(map_s,rot=[55,-29.8],reso=hp.nside2resol(nside,arcmin=True),min=10,max=500,title=var_sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552b34f3",
   "metadata": {},
   "source": [
    "## Mean seeing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3a1cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "var=\"psf_fwhm_i\"\n",
    "var_sys=\"avg(\"+var+\")\"\n",
    "df_map=df.groupBy(\"ipix\").mean(var)\n",
    "df_map.describe([var_sys]).show() \n",
    "dfp=df_map.toPandas()\n",
    "map_s = np.zeros(hp.nside2npix(nside))\n",
    "map_s[dfp['ipix'].values]=dfp[var_sys].values\n",
    "hp.gnomview(map_s,rot=[55,-29.8],reso=hp.nside2resol(nside,arcmin=True),min=0.45,max=1.,title=var_sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c78665",
   "metadata": {},
   "source": [
    "## Ellipticities \n",
    "\n",
    "\n",
    "compute distorsion (thanks to Javier). Note that we don't have redshifts\n",
    "\n",
    "### Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c7ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "Q11=\"IxxPSF_i\"\n",
    "Q22=\"IyyPSF_i\"\n",
    "Q12=\"IxyPSF_i\"\n",
    "\n",
    "# pre-compute denominator\n",
    "df_shear=df.withColumn(\"denom\",F.col(Q11)+F.col(Q22))\n",
    "#read and img parts of shear\n",
    "df_shear=df_shear.withColumn(\"R_E\",(F.col(Q11)-F.col(Q22))/F.col('denom')).\\\n",
    "        withColumn(\"I_E\",(2*F.col(Q12))/F.col('denom'))\n",
    "# convert to amplitude and phase\n",
    "df_shear=df_shear.withColumn(\"amp_E\",F.hypot(F.col(\"R_E\"),F.col(\"I_E\"))).\\\n",
    "    withColumn(\"phase_E\",F.atan2(F.col(\"R_E\"),F.col(\"I_E\")))\n",
    "df_shear.select(\"R_E\",\"I_E\",\"amp_E\",\"phase_E\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54d131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "var=\"amp_E\"\n",
    "var_sys=\"avg(\"+var+\")\"\n",
    "df_map=df_shear.groupBy(\"ipix\").mean(var)\n",
    "df_map.describe([var_sys]).show() \n",
    "dfp=df_map.toPandas()\n",
    "map_e = np.zeros(hp.nside2npix(nside))\n",
    "map_e[dfp['ipix'].values]=dfp[var_sys].values\n",
    "hp.gnomview(map_e,rot=[55,-29.8],reso=hp.nside2resol(nside,arcmin=True),title=var_sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e756c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "var=\"phase_E\"\n",
    "var_sys=\"avg(\"+var+\")\"\n",
    "df_map=df_shear.groupBy(\"ipix\").mean(var)\n",
    "df_map.describe([var_sys]).show() \n",
    "dfp=df_map.toPandas()\n",
    "map_e = np.zeros(hp.nside2npix(nside))\n",
    "map_e[dfp['ipix'].values]=dfp[var_sys].values\n",
    "hp.gnomview(map_e,rot=[55,-29.8],reso=hp.nside2resol(nside,arcmin=True),title=var_sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03c42b1",
   "metadata": {},
   "source": [
    "# Missing quantities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b429127e",
   "metadata": {},
   "source": [
    "- redshift \n",
    "- airmass\n",
    "- HSM _e1/e2 (ext_shapeHSM_HsmShapeRegauss_e1 and ext_shapeHSM_HsmShapeRegauss_e2 availbale in GCR as native_quantities but not DPDD)\n",
    "- ?"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
