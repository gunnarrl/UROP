{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "324d5fc8",
   "metadata": {},
   "source": [
    "# KNN text classification from ICLR 2018 reviews\n",
    "\n",
    "Using ICLR 2018 reviews from openreview, KNN was used in this notebook, using the decision as label for a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396bd3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast # Library to parse code, since replies were saved as a list format in the csv\n",
    "from collections import Counter\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7be5e9f",
   "metadata": {},
   "source": [
    "#### Path to data that contains csv names for iclr conferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cee681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"iclr_urls.csv\"\n",
    "iclr_conf_data = pd.read_csv(path_to_data) \n",
    "iclr_conf_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78f1f2f",
   "metadata": {},
   "source": [
    "#### Showing conference where data is available from the iclr_ulrs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7883ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "iclr_conf_data['conference']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501da8c7",
   "metadata": {},
   "source": [
    "#### Data for iclr 2018 is divided between the conference and its workshops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb8b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "iclr_conf_data['conference'].iloc[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d030693a",
   "metadata": {},
   "source": [
    "#### Using ICLR 2018 conference data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5395266",
   "metadata": {},
   "outputs": [],
   "source": [
    "iclr18_dataset = pd.read_csv(iclr_conf_data['conference'].iloc[6]+\".csv\").dropna()\n",
    "iclr18_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8d02d",
   "metadata": {},
   "source": [
    "#### 911 papers are avaible from the openreview website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19939895",
   "metadata": {},
   "outputs": [],
   "source": [
    "replies = iclr18_dataset['replies']\n",
    "print(\"Number of papers: \" + str(len(replies))) \n",
    "replies[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1118224a",
   "metadata": {},
   "source": [
    "#### There are only 4 possible classes to tag reviews,classes differ a lot in its numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37838adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_options = Counter(iclr18_dataset['decision'])\n",
    "decision_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84107c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_labels = {\"Accept (Oral)\": 0,\n",
    "         \"Invite to Workshop Track\": 1,\n",
    "         \"Reject\": 2,\n",
    "         \"Accept (Poster)\": 3}\n",
    "decision_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfdda6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_reviews_and_decision(df_venue, labels):\n",
    "  \"\"\"Creates and lists of lists containing a review an its decision\n",
    "  \n",
    "  Args:\n",
    "    df_venue (pandas dataframe): Contains data from a venue where each row represents\n",
    "    a paper that have title, authors, emails, decision, abstract, pdf and replies fields.\n",
    "    labels (dictionary): Maps a string to a number that encodes the  decision as an integer\n",
    "  Returns:\n",
    "    A lists of pairs where the first elements are the decision encoded with the labels\n",
    "    and the second elements is the review.\n",
    "    \n",
    "  \"\"\"\n",
    "\n",
    "#   print(df_venue.head())\n",
    "  \n",
    "  decision_review = []\n",
    "  for index, row in df_venue.iterrows():\n",
    "    comments = ast.literal_eval(row['replies'])\n",
    "    for comment in comments:\n",
    "      if comment[1][0] == \"rating\": # They called reviews as rating\n",
    "        decision_review.append([labels[row['decision']],comment[1][1]])\n",
    "  return decision_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c263ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_review = match_reviews_and_decision(iclr18_dataset, decision_labels)\n",
    "len(decision_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb008c9",
   "metadata": {},
   "source": [
    "#### Defining cleaning method includes punctuation marks, stop words &amp; digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fbb733",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    processed = re.sub(r\"\\d+\",\"\",normalized)\n",
    "    y = processed.split()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665bdf51",
   "metadata": {},
   "source": [
    "#### Cleaning the reviews and extracting tf-idf features using scikit-learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c700a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df_venue):\n",
    "  \"\"\"Does the preprocessing applying the clean function and the\n",
    "  TfidVectorizer using the english stopwords\n",
    "  \n",
    "  Args:\n",
    "    df_venue (pandas dataframe): Contains data from a venue where each row represents\n",
    "    a paper that have title, authors, emails, decision, abstract, pdf and replies fields.\n",
    "    \n",
    "  Returns:\n",
    "    X (sparse matrix, [n_samples, n_features]): Tf-idf-weighted document-term matrix.\n",
    "    y (lists of integers): labels encoded as integers.\n",
    "\n",
    "  \"\"\"\n",
    "  \n",
    "  train_clean_sentences = []\n",
    "  y = np.array([y[0] for y in decision_review])\n",
    "\n",
    "  for line in decision_review:\n",
    "    line = line[1].strip()\n",
    "    cleaned = clean(line)\n",
    "    cleaned = ' '.join(cleaned)\n",
    "    train_clean_sentences.append(cleaned)\n",
    "    \n",
    "  vectorizer = TfidfVectorizer(stop_words='english')\n",
    "  \n",
    "  X = vectorizer.fit_transform(train_clean_sentences)\n",
    "  \n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bfa8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    y_true = [int(x) for x in y_true]\n",
    "    y_pred = [int(x) for x in y_pred]\n",
    "    \n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562841c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = preprocess_data(decision_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfffb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ece607",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf75f83",
   "metadata": {},
   "source": [
    "#### Setting k to 4 and printing the confusion matrix for 10 k folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0663619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelknn = KNeighborsClassifier(n_neighbors=4)\n",
    "\n",
    "scores = []\n",
    "cv = KFold(n_splits=8, random_state=420, shuffle=True)\n",
    "class_names = np.array([\"AcceptOral\", \"invitedWorkshop\", \"Reject\",\"AcceptPoster\"])\n",
    "\n",
    "for train_index, test_index in cv.split(X):\n",
    "  X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "  modelknn.fit(X_train,y_train)\n",
    "  scores.append(modelknn.score(X_test, y_test))    \n",
    "  np.set_printoptions(precision=2)\n",
    "  y_pred = modelknn.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "  # Plot non-normalized confusion matrix\n",
    "  plot_confusion_matrix(y_test, y_pred, classes=class_names, title=\"Confusion matrix, without normalization\")\n",
    "  # Plot normalized confusion matrix\n",
    "  plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True, title=\"Normalized confusion matrix\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf145071",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42220316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 10-fold cross validation\n",
    "scores = cross_val_score(modelknn, X, y, cv=10)\n",
    "print(\"Cross-validated scores:\", scores)\n",
    "print(\"Avg score: \", sum(scores)/len(scores))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
