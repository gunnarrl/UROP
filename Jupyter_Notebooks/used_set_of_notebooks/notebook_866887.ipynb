{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1735191b",
   "metadata": {},
   "source": [
    "# Back Propagation Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da92b5cc",
   "metadata": {},
   "source": [
    "We do backprop in some concrete examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd62107f",
   "metadata": {},
   "source": [
    "## Import the package and set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bab65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from math import exp, log\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e707e16",
   "metadata": {},
   "source": [
    "## The Network\n",
    "\n",
    "The network is: \n",
    "\n",
    "$$ x = A^0 = \n",
    "\\begin{bmatrix}\n",
    "  0.35 & 0.9\n",
    "\\end{bmatrix}$$\n",
    "$$ W^1  = \n",
    "\\begin{bmatrix}\n",
    "  0.1 & 0.4 \\\\\n",
    "  0.8 & 0.6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$ W^2  = \n",
    "\\begin{bmatrix}\n",
    "  0.3  \\\\\n",
    "  0.9 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "![Network 1](./BP_example.png \"Network 1\")\n",
    "\n",
    "\n",
    "Or simply, \n",
    "$$ \n",
    "A^0 \\stackrel{W^1}{\\Longrightarrow} Z^1 \\stackrel{\\sigma}{\\Longrightarrow} \n",
    "A^1 \\stackrel{W^2}{\\Longrightarrow} Z^2 \\stackrel{\\sigma}{\\Longrightarrow} \n",
    "A^2\n",
    "$$\n",
    "and $x = A^0$, $y = A^2$, ground truth $t = 0.5$ \n",
    "\n",
    "All activation functions are sigmoid. \n",
    "\n",
    "Note that \n",
    "1. there is no biases in the network. \n",
    "2. $x$ is a row vector.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ae70b9",
   "metadata": {},
   "source": [
    "### Implementation in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb86c4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _acti(z): \n",
    "    return 1.0 / (1.0 + exp(-z))\n",
    "\n",
    "def acti(zs):\n",
    "    return 1.0/(1.0 + np.exp(-zs))\n",
    "\n",
    "def acti_old(zs):\n",
    "    s = zs.shape\n",
    "    dup = zs.reshape(-1)\n",
    "    return np.fromiter( (_acti(elem) for elem in dup), zs.dtype ).reshape(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe21a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _grad(z):\n",
    "    return _acti(z) * (1 - _acti(z))\n",
    "\n",
    "\n",
    "def grad(zs):\n",
    "    s = zs.size\n",
    "    return np.multiply(acti(zs),(-acti(zs) + 1.0))\n",
    "\n",
    "def grad_old(zs):\n",
    "    return np.fromiter( (_grad(elem) for elem in zs), zs.dtype )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c65bacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, t): \n",
    "    return 0.5*(y-t).T*(y-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445f5b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(*vals, prec = 8,):\n",
    "    for val in vals:\n",
    "        if isinstance(val, str):\n",
    "            print(val, end=' ')\n",
    "        else: # assume numeric\n",
    "            print(val.round(prec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa6f64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the above functions\n",
    "a = np.array([1, 2])\n",
    "b = np.array([3, 4])\n",
    "print(a * b)\n",
    "a = np.matrix([[1, -1], [2, 3]])\n",
    "#acti(a)\n",
    "grad(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabfa997",
   "metadata": {},
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eaa721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the model. We use numpy.matrix as numpy 1d array cannot do transpose()\n",
    "A0 = np.matrix([0.35, 0.9])\n",
    "W1 = np.matrix([[0.1, 0.4], [0.8, 0.6]])\n",
    "W2 = np.array([[0.3], [0.9]])\n",
    "t = np.array([0.5])\n",
    "\n",
    "# forward pass\n",
    "Z1 = np.dot(A0, W1)\n",
    "A1 = acti(Z1)\n",
    "Z2 = np.dot(A1, W2)\n",
    "A2 = acti(Z2)\n",
    "y = A2\n",
    "\n",
    "# print\n",
    "p('Z1, A1 = ', Z1, A1)\n",
    "p('Z2, A2 = ', Z2, A2)\n",
    "p('loss=', loss(y, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86db7e53",
   "metadata": {},
   "source": [
    "#### Compute the Gradients on Parameters\n",
    "\n",
    "Overview: \n",
    "1. Compute $\\delta_{Z_2}$\n",
    "   * From $\\delta_{Z_2}$, compute $\\delta_{W_2}$\n",
    "2. Compute $\\delta_{Z_1}$ from $\\delta_{Z_2}$\n",
    "   * From $\\delta_{Z_1}$, compute $\\delta_{W_1}$\n",
    "  \n",
    "The forward and backward passes are very clear on the vectorized version of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98f2be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_Z2 = np.dot(grad(Z2), (y - t))\n",
    "p(d_Z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ea57e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_W2 = np.dot(A1.T, d_Z2)\n",
    "p(d_W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3b8f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(d_Z2, W2.T).A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8306e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_Z1 = np.multiply(np.dot(d_Z2, W2.T), grad(Z1))\n",
    "d_Z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6e07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_W1 = np.dot(A0.T, d_Z1)\n",
    "p(d_W1, prec = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a15dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p(W1 - d_W1, prec = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd99910",
   "metadata": {},
   "source": [
    "## PyTorch implementation\n",
    "\n",
    "Now we use PyTorch's autograd to verify our computation above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78349688",
   "metadata": {},
   "outputs": [],
   "source": [
    "tT = torch.tensor(t, requires_grad=False) # ground truth\n",
    "tx = torch.tensor(A0, requires_grad=False) # input x\n",
    "tW1= torch.tensor(W1, requires_grad=True) \n",
    "tW2 = torch.tensor(W2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6796954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "igma = torch.sigmoid\n",
    "# our network (and its output Y for input X)\n",
    "Z1 = tx.mm(tW1)\n",
    "A1 = sigma(Z1)\n",
    "Z2 = A1.mm(tW2)\n",
    "A2 = sigma(Z2)\n",
    "Y = A2\n",
    "loss2 = (Y-tT).pow(2).sum()/2.0 # just sum all\n",
    "print(loss2.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305d4a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2.backward()\n",
    "print(tW1.grad)\n",
    "print(tW2.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e859e0fc",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Modify the example to support biases in the network. "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
