{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "016af37f",
   "metadata": {},
   "source": [
    "Use of California Census data to build a model of House Prices<br>\n",
    "Model Should learn from the data and predict the median housing prices of any districts given all the predictors.<br>\n",
    "This is a supervised regression task using batch learning technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8b046b",
   "metadata": {},
   "source": [
    "### Selecting A performance Measure -\n",
    "\n",
    "We used Root Mean Squared Error(RMSE) as our performance measure <br><br>\n",
    "\\begin{equation*}\n",
    "RMSE(X, h) = \\sqrt{\\frac 1m \\sum_{i = 1}^m (h(x^{(i)}) - y^{(i)})^2}\n",
    "\\end{equation*}\n",
    "- $m$ is the number of observations in the dataset\n",
    "- $x^{(i)}$ is the feature vector of the $i^{th}$ instance of the dataset, $y^{(i)}$ is its label\n",
    "- $X$ is the matrix contating predictor values all the observations ni the dataset (excluding the label)\n",
    "- $h$ is the estimated prediction function of aur learning algorithm.\n",
    "\n",
    "We call $RMSE(X,h)$ as cost function measured on our obesrvations using the hypothesis $h$. <br>\n",
    "We could also have used Mean Absolute Error(MAE) if our dataset had many outliers. Basically RMSE uses L2 norm and MAE the L1 norm, the higher the norm index the more focus is on the larger values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8259c5fe",
   "metadata": {},
   "source": [
    "### Getting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da463989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://github.com/ageron/handson-ml2/raw/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3184266",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOUSING_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db04e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to fetch the data\n",
    "def fetch_housing_data(housing_url = HOUSING_URL, housing_path = HOUSING_PATH):\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url,tgz_path) #Dowloads the data from housing_url to tgz_path\n",
    "    housing_tgz = tarfile.open(tgz_path) \n",
    "    housing_tgz.extractall(path = housing_path)#extracts and saves the data into housing_path\n",
    "    housing_tgz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff63efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    "import pandas as pd\n",
    "\n",
    "#function to load the data\n",
    "def load_housing_data(housing_path = HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d476bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def3aa93",
   "metadata": {},
   "source": [
    "### Inspect the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04300ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3d522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aafd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a quick discription of data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca42798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary of the categorical variable\n",
    "df['ocean_proximity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1ab64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary of the numerical datatypes\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aefbd2",
   "metadata": {},
   "source": [
    "The 25%, 50% and the 75% rows shows the value below which a given percentage of obseervation in a group of observations fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "df.hist(bins = 50, figsize = (20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3615a557",
   "metadata": {},
   "source": [
    "### Creating a test Set\n",
    "We pick some instances randomly typically around 20% of the data and set them aside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc137c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#function for test train split\n",
    "def split_test_train(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) *test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719915f3",
   "metadata": {},
   "source": [
    "This method of train test split works but we will get a new split when we run the algorithm again and over time our ml model will see the entire dataset.<br>\n",
    "Other method is to use random seed or save the test data and both the method will not if we fetch the updated data <br>\n",
    "The work-around is to use a **hash funstion** and calculate the hash of eash instance's identifier and if its hash value is below 20% of the maximum hash value it will be in the test set otherwise ot will be in the training data using this we will not have any instance in the test set which was earlier in the training set and the test train split will maintain the desirable ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f218d24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zlib import crc32\n",
    "\n",
    "def test_set_check(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n",
    "\n",
    "def split_train_test_by_id(data, test_ratio, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbc3239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding index column to the dataframe\n",
    "df_with_index = df.reset_index()\n",
    "train_set, test_set = split_train_test_by_id(df_with_index, 0.2, \"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f200e64",
   "metadata": {},
   "source": [
    "it is needed to be made sure that if row is needed to be the unique identifier then the new data gets appended to the end of the data det and no row gets deleted, otherwise we can use most stable features of the dataset as the unique identitfiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241b88c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can also use sklearn inbuilt train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(df, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d264d279",
   "metadata": {},
   "source": [
    "We can use diffrent method of sampling to devide our dataset. So far we have only considered simple random sampling but it may give sampling biases sometimes. For example, if the dataset contains 53% of one class and 47% of other class we can devide the data into different strata so that the sample is representatiove of the original population, this is called ***stratified random sampling***. <br>\n",
    "In our example we will do stratified random sampling on median income by first binning the income into different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b7e8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning the median incomes into different categories\n",
    "df['income_cat'] = pd.cut(df['median_income'],\n",
    "                         bins = [0., 1.5, 3.0, 4.5, 6, np.inf],\n",
    "                         labels = [1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0207b090",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['income_cat'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c3c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing stratified sampling based on median income categories\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1,test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(df, df['income_cat']):\n",
    "    strat_train_set = df.loc[train_index]\n",
    "    strat_test_set = df.loc[test_index]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d39f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# propotion of each median income category in train set\n",
    "strat_train_set['income_cat'].value_counts() / len(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c3d12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# propotion of each median income category in entire set\n",
    "df['income_cat'].value_counts() / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa1af9f",
   "metadata": {},
   "source": [
    "We can see that the test set is representative of the population wrt median income category attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3257f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the income_cat attribute to get the data back in the same state\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
