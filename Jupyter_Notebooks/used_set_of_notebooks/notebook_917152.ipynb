{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aba2b5d",
   "metadata": {},
   "source": [
    "# Datathon Week 1\n",
    "\n",
    "Having the NYC problem we are going to explore multiple approaches on how to handle the data to answer some questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2223addf",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    " * What is the most used transportation method?\n",
    " * How uber behavior vs other transportation methods (in use)?\n",
    " * In which nta the people uses more uber?\n",
    " * \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15708282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_folder = '../Dataset'\n",
    "\n",
    "dataset_names = ['demographics', 'geographic', 'green_trips', 'mta_trips',\n",
    "            'uber_trips_2014', 'uber_trips_2015', 'weather', 'yellow_trips', 'zones']\n",
    "datasets = {}\n",
    "for dataset_name in dataset_names:\n",
    "    filename = f\"{dataset_name}.csv\"\n",
    "    filepath = os.path.join(data_folder, filename)\n",
    "    datasets[dataset_name] = pd.read_csv(filepath, dtype = { 'line_name': str }) if dataset_name == 'mta_trips' else pd.read_csv(filepath)\n",
    "    print(f\"loaded: {filepath}\")\n",
    "\n",
    "print('All datasets loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766e9ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_explore = 'demographics'\n",
    "\n",
    "datasets[name_to_explore].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d564620",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['uber_trips_2014'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf45da68",
   "metadata": {},
   "source": [
    "I want to join the zones and demographics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900592c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['zones_demographics'] = pd.merge(datasets['zones'], datasets['demographics'], how = 'left')\n",
    "datasets['zones_demographics'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112593c4",
   "metadata": {},
   "source": [
    "I want to merge the uber 2015 dataset with the zones to increase the amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f9df73",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_2015 = pd.merge(datasets['uber_trips_2015'], datasets['zones_demographics'], how = 'left', left_on = ['pickup_location_id'], right_on = ['location_id'])\n",
    "\n",
    "uber_2015.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f12a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_2015['pickup_datetime'] = pd.to_datetime(datasets['uber_trips_2015']['pickup_datetime'])\n",
    "\n",
    "uber_2015.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22aa0d4",
   "metadata": {},
   "source": [
    "I want to do the same but with the uber 2014 data, but because it's with longitud and latitud We need to understand how this works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1193e013",
   "metadata": {},
   "source": [
    "We need to map over the dataframe, and the column is going to be iterating over every column until it finds it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b27749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import MultiPoint\n",
    "\n",
    "geo_df = datasets['geographic']\n",
    "rows, cols = geo_df.shape\n",
    "\n",
    "# Build a hash with the location_id as key and Polygon as value\n",
    "geo_polygons = {}\n",
    "for location_id in geo_df.columns:\n",
    "    poly_points = geo_df[location_id]\n",
    "\n",
    "    half_range = range(int(rows / 2))\n",
    "    coordinates = [(poly_points[i * 2], poly_points[i * 2 + 1]) for i in half_range]\n",
    "    poly = MultiPoint(coordinates).convex_hull\n",
    "    geo_polygons[location_id] = poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0422d1f0",
   "metadata": {},
   "source": [
    "We can check the forms of the polygons just in case we want to see locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3833aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_id = 'BX59'\n",
    "geo_polygons[location_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaab63ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from descartes import PolygonPatch\n",
    "\n",
    "fig = plt.figure(figsize = (12, 16))\n",
    "\n",
    "for key, value in geo_polygons.items():\n",
    "    x, y = value.exterior.xy\n",
    "    \n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(x, y, color = 'blue', alpha = 0.7, linewidth = 0.5)\n",
    "\n",
    "#     ring = PolygonPatch(value)\n",
    "#     ax.add_patch(ring)\n",
    "\n",
    "for i, row in datasets['uber_trips_2014'].head(1000).iterrows():\n",
    "    x, y = row['pickup_longitude'], row['pickup_latitude']\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(x = x, y = y, linewidth = 0.1, alpha = 0.1, color = 'black')\n",
    "\n",
    "for i, row in datasets['green_trips'][datasets['green_trips']['pickup_latitude'] != 0].head(1000).iterrows():\n",
    "    x, y = row['pickup_longitude'], row['pickup_latitude']\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(x = x, y = y, linewidth = 0.1, alpha = 0.1, color = 'green')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7410ec",
   "metadata": {},
   "source": [
    "Now i want to map over the column of the 2014 uber data to find the location id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f14c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "def find_location_id(row):\n",
    "    latitude = row['pickup_latitude']\n",
    "    longitude = row['pickup_longitude']\n",
    "    \n",
    "    point = Point(longitude, latitude)\n",
    "    trip_location_id = None\n",
    "    for location_id in list(geo_polygons.keys()):\n",
    "        polygon = geo_polygons[location_id]\n",
    "        if point.within(polygon):\n",
    "            trip_location_id = location_id\n",
    "            break\n",
    "    return trip_location_id\n",
    "\n",
    "new_uber_2014 = uber_2014.copy()\n",
    "result = uber_2014.apply(find_location_id, axis = 1)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d49cd5",
   "metadata": {},
   "source": [
    "## Remeber to use the new geographical data instead of the original to be more precise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdd0083",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_uber_2014['location_id'] = result.map(lambda x: None if x == string.empty else x)\n",
    "new_uber_2014.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e435b2e",
   "metadata": {},
   "source": [
    "Now the idea is that the uber 2014 uses the same columns as the 2015 uber so we can append the 2 datasets together, thiw way we are going to have all the uber data integrated in one data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f0dc19",
   "metadata": {},
   "source": [
    "We could aggregate the data for the taxis also, if this take too long it's not practical so I will suggest to only left taxi trips that match the same dates as the uber rides so we can compare it. if not, the search in polygons would take too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b59070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "geo_json_path = '../Dataset/geo_json.json'\n",
    "\n",
    "alpha = 0.2\n",
    "radius = 20\n",
    "sample_size = 1000\n",
    "\n",
    "geo_json = {}\n",
    "with open(geo_json_path, 'r') as reader:\n",
    "    geo_json = json.loads(reader.read())\n",
    "\n",
    "fig = plt.figure(figsize = (16, 16))\n",
    "for geo_info in geo_json['features']:\n",
    "    coordinates = geo_info['geometry']['coordinates'][0]\n",
    "    if len(coordinates) == 1:\n",
    "        coordinates = coordinates[0]\n",
    "    x = [coord[0] for coord in coordinates]\n",
    "    y = [coord[1] for coord in coordinates]\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(x, y, color = 'blue', alpha = 0.7, linewidth = 0.5)\n",
    "\n",
    "for i, row in datasets['uber_trips_2014'].sample(sample_size).iterrows():\n",
    "    x, y = row['pickup_longitude'], row['pickup_latitude']\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(x = x, y = y, s = radius, alpha = alpha, color = 'black')\n",
    "\n",
    "for i, row in datasets['green_trips'][datasets['green_trips']['pickup_latitude'] != 0].sample(sample_size).iterrows():\n",
    "    x, y = row['pickup_longitude'], row['pickup_latitude']\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(x = x, y = y, s = radius, alpha = alpha, color = 'green')\n",
    "\n",
    "for i, row in datasets['yellow_trips'][datasets['yellow_trips']['pickup_latitude'] != 0].sample(sample_size).iterrows():\n",
    "    x, y = row['pickup_longitude'], row['pickup_latitude']\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(x = x, y = y, s = radius, alpha = alpha, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f8909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "folium_map = folium.Map(location = [40.738, -73.98],\n",
    "                        zoom_start = 10,\n",
    "                        tiles = 'OpenStreetMap')\n",
    "\n",
    "for i, row in datasets['uber_trips_2014'].sample(2000).iterrows():\n",
    "    longitude, latitude = row['pickup_longitude'], row['pickup_latitude']\n",
    "    marker = folium.CircleMarker(location = [latitude, longitude], radius = 3, color = 'black', fill = True, stroke = False)\n",
    "    marker.add_to(folium_map)\n",
    "\n",
    "folium_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b028844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folium_map = folium.Map(location = [40.738, -73.98],\n",
    "                        zoom_start = 10,\n",
    "                        tiles = 'Stamen Terrain')\n",
    "\n",
    "for geo_info in geo_json['features']:\n",
    "    coordinates = geo_info['geometry']['coordinates'][0]\n",
    "    if len(coordinates) == 1:\n",
    "        coordinates = coordinates[0]\n",
    "    coordinates = [[coord[1], coord[0]] for coord in coordinates]\n",
    "\n",
    "    polygon = folium.Polygon(locations = coordinates, weight = 1, fill = True, color = 'blue')\n",
    "    polygon.add_to(folium_map)\n",
    "\n",
    "def embed_map(m):\n",
    "    from IPython.display import IFrame\n",
    "\n",
    "    m.save('../Dataset/map.html')\n",
    "    return IFrame('../Dataset/map.html', width='100%', height='750px')\n",
    "\n",
    "embed_map(folium_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ff8ce1",
   "metadata": {},
   "source": [
    "One idea is to remove all points that are not inside a polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c05882",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['mta_trips'].groupby('station')[['latitude', 'longitude']].first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585b2d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['yellow_trips']"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
