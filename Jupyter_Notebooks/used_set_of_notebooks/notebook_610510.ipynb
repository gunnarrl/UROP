{"cells":[{"cell_type":"markdown","id":"725617bc","metadata":{"id":"725617bc"},"source":["# Image Colorization using Transfer Learning\n","\n","This notebook outlines steps to develop a image colorization network based on a 3 component network consisting of an Encoder, a Fusion Layer and a Decoder. The network utilizes VGG16 as a feature extractor to its benefit. The implementation is based on the works of Baldassarre et al.\n","\n","The network is a result of creatively utilizing alternate colorspaces like LAB and YUV instead of the regular RGB colorspace. The following image showcases how different colorspaces represent the same image across different channels.\n","\n","<img src=\"colorspaces.png\">\n","\n","\n","Utilizing a colorspace such as LAB, the advantage is due to the fact that one of the channels is the grayscale channel itself. Thus the coloring task is transformed into a task of mapping the grayscale pixel values to pixel values in channels __a__ and __b__ , as shown in the following image.\n","\n","<img src=\"colorization_task.png\" style=\"height:250px;\">\n","\n","\n","The network expects a grayscale input image transformed from RGB to LAB colorspace. We also perform a few more preprocessing steps to enable the network to learn the task. The overall network architecture is as shown in the following image.\n","<img src=\"colornet_architecture.png\" style=\"height:300px;\">"]},{"cell_type":"code","execution_count":null,"id":"6dd4a908","metadata":{"id":"6dd4a908"},"outputs":[],"source":["from google.colab import files"]},{"cell_type":"code","execution_count":null,"id":"ac02e53b","metadata":{"id":"ac02e53b"},"outputs":[],"source":["import os\n","import random\n","import numpy as np\n","\n","import scipy as sp\n","import scipy.ndimage as spi\n","\n","from skimage.io import imsave,imshow\n","from skimage.transform import resize\n","from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n","\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"id":"ab2675b1","metadata":{"id":"ab2675b1"},"outputs":[],"source":["import keras\n","from keras.preprocessing import image\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.preprocessing.image import array_to_img, img_to_array, load_img\n","\n","from keras.applications.vgg16 import VGG16\n","from keras.applications.vgg16 import preprocess_input\n","\n","from keras.engine import Layer\n","from keras.layers import Reshape, merge, concatenate\n","from keras.layers import Input,Activation, Dense, Dropout, Flatten\n","from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose\n","\n","from keras.layers.core import RepeatVector, Permute\n","from keras.layers.normalization import BatchNormalization\n","\n","from keras.callbacks import TensorBoard\n","\n","from keras.models import Sequential, Model\n","\n","import tensorflow as tf"]},{"cell_type":"markdown","id":"c9fe4715","metadata":{"id":"c9fe4715"},"source":["## Load the sample dataset\n","We utilize a small subset of images from the ImageNet Dataset"]},{"cell_type":"code","execution_count":null,"id":"624e6899","metadata":{"id":"624e6899"},"outputs":[],"source":["def upload_files():\n","  uploaded = files.upload()\n","  for k, v in uploaded.items():\n","    open(k, 'wb').write(v)\n","  return list(uploaded.keys())"]},{"cell_type":"code","execution_count":null,"id":"3ec43839","metadata":{"id":"3ec43839"},"outputs":[],"source":["file_list = upload_files()"]},{"cell_type":"markdown","id":"5c7855a6","metadata":{"id":"5c7855a6"},"source":["## Preprocess\n","\n","Input images are resized and pixel values are brought within the 24bit RGB range"]},{"cell_type":"code","execution_count":null,"id":"c6e5db8a","metadata":{"id":"c6e5db8a"},"outputs":[],"source":["def prep_data(file_list=[],\n","              dir_path=None,\n","              dim_x=256,\n","              dim_y=256):\n","    #Get images\n","    X = []\n","    for filename in file_list:\n","        X.append(img_to_array(\n","            sp.misc.imresize(\n","                load_img(\n","                    dir_path+filename),\n","                    (dim_x, dim_y))\n","            )\n","        )\n","    X = np.array(X, dtype=np.float64)\n","    X = 1.0/255*X\n","    return X"]},{"cell_type":"markdown","id":"550ff137","metadata":{"id":"550ff137"},"source":["## load pretrained VGG16"]},{"cell_type":"code","execution_count":null,"id":"f94e50d0","metadata":{"id":"f94e50d0"},"outputs":[],"source":["#Load weights\n","vgg16 = VGG16(weights=r'imagenet', include_top=True)\n","vgg16.graph = tf.get_default_graph()"]},{"cell_type":"code","execution_count":null,"id":"529baf52","metadata":{"id":"529baf52"},"outputs":[],"source":["vgg16.output_shape"]},{"cell_type":"markdown","id":"eb8bcc9e","metadata":{"id":"eb8bcc9e"},"source":["## Prepare Train and Test Datasets"]},{"cell_type":"code","execution_count":null,"id":"33cc5a13","metadata":{"id":"33cc5a13"},"outputs":[],"source":["train_files,test_files =train_test_split(file_list,\n","                                            test_size=0.3,\n","                                            random_state=42)\n","len(train_files),len(test_files)"]},{"cell_type":"code","execution_count":null,"id":"37c73621","metadata":{"id":"37c73621"},"outputs":[],"source":["DIM = 256"]},{"cell_type":"code","execution_count":null,"id":"22b13d92","metadata":{"id":"22b13d92"},"outputs":[],"source":["X_train = prep_data(file_list=train_files,\n","                    dir_path='',\n","                    dim_x=DIM,dim_y=DIM)"]},{"cell_type":"markdown","id":"9d9e7246","metadata":{"id":"9d9e7246"},"source":["## Construct the network\n","\n","As mentioned earlier, the network has a unique architecture consisting of the following components:\n","+ Encoder\n","+ Feature Extractor\n","+ Fusion Layer\n","+ Decoder"]},{"cell_type":"code","execution_count":null,"id":"c21a9fc5","metadata":{"id":"c21a9fc5"},"outputs":[],"source":["emd_input = Input(shape=(1000,))\n","\n","#Encoder\n","enc_input = Input(shape=(DIM, DIM, 1,))\n","enc_output = Conv2D(64, (3,3),\n","                        activation='relu',\n","                        padding='same', strides=2)(enc_input)\n","enc_output = Conv2D(128, (3,3),\n","                        activation='relu',\n","                        padding='same')(enc_output)\n","enc_output = Conv2D(128, (3,3),\n","                        activation='relu',\n","                        padding='same', strides=2)(enc_output)\n","enc_output = Conv2D(256, (3,3),\n","                        activation='relu',\n","                        padding='same')(enc_output)\n","enc_output = Conv2D(256, (3,3),\n","                        activation='relu',\n","                        padding='same', strides=2)(enc_output)\n","enc_output = Conv2D(512, (3,3),\n","                        activation='relu',\n","                        padding='same')(enc_output)\n","enc_output = Conv2D(512, (3,3),\n","                        activation='relu',\n","                        padding='same')(enc_output)\n","enc_output = Conv2D(256, (3,3),\n","                        activation='relu',\n","                        padding='same')(enc_output)\n","\n","#Fusion\n","fusion_layer_output = RepeatVector(32*32)(emd_input)\n","fusion_layer_output = Reshape(([32,32,\n","                          1000]))(fusion_layer_output)\n","fusion_layer_output = concatenate([enc_output,\n","                                   fusion_layer_output], axis=3)\n","fusion_layer_output = Conv2D(DIM, (1, 1),\n","                       activation='relu',\n","                       padding='same')(fusion_layer_output)\n","\n","#Decoder\n","dec_output = Conv2D(128, (3,3),\n","                        activation='relu',\n","                        padding='same')(fusion_layer_output)\n","dec_output = UpSampling2D((2, 2))(dec_output)\n","dec_output = Conv2D(64, (3,3),\n","                        activation='relu',\n","                        padding='same')(dec_output)\n","dec_output = UpSampling2D((2, 2))(dec_output)\n","dec_output = Conv2D(32, (3,3),\n","                        activation='relu',\n","                        padding='same')(dec_output)\n","dec_output = Conv2D(16, (3,3),\n","                        activation='relu',\n","                        padding='same')(dec_output)\n","dec_output = Conv2D(2, (3, 3),\n","                        activation='tanh',\n","                        padding='same')(dec_output)\n","dec_output = UpSampling2D((2, 2))(dec_output)\n","\n","model = Model(inputs=[enc_input, emd_input], outputs=dec_output)"]},{"cell_type":"markdown","id":"2872f5b2","metadata":{"id":"2872f5b2"},"source":["## Feature Extraction and Image Augmentation\n","\n","VGG16 is used as feature extractor. We also utilize data augmentation to help our model generalize better."]},{"cell_type":"code","execution_count":null,"id":"f4cfde67","metadata":{"id":"f4cfde67"},"outputs":[],"source":["#Create embedding\n","def create_vgg_embedding(grayscaled_rgb):\n","    gs_rgb_resized = []\n","    for i in grayscaled_rgb:\n","        i = resize(i, (224, 224, 3),\n","                   mode='constant')\n","        gs_rgb_resized.append(i)\n","    gs_rgb_resized = np.array(gs_rgb_resized)\n","    gs_rgb_resized = preprocess_input(gs_rgb_resized)\n","    with vgg16.graph.as_default():\n","      embedding = vgg16.predict(gs_rgb_resized)\n","    return embedding\n","\n","# Image transformer\n","datagen = ImageDataGenerator(\n","        shear_range=0.2,\n","        zoom_range=0.2,\n","        rotation_range=20,\n","        horizontal_flip=True)\n","\n","def colornet_img_generator(X,\n","                  batch_size=BATCH_SIZE):\n","    for batch in datagen.flow(X, batch_size=batch_size):\n","        gs_rgb = gray2rgb(rgb2gray(batch))\n","        batch_lab = rgb2lab(batch)\n","\n","        batch_l = batch_lab[:,:,:,0]\n","        batch_l = batch_l.reshape(batch_l.shape+(1,))\n","\n","        batch_ab = batch_lab[:,:,:,1:] / 128\n","        yield ([batch_l,\n","                create_vgg_embedding(gs_rgb)], batch_ab)"]},{"cell_type":"markdown","id":"28152637","metadata":{"id":"28152637"},"source":["## Train the Model"]},{"cell_type":"code","execution_count":null,"id":"a39c49d2","metadata":{"id":"a39c49d2"},"outputs":[],"source":["model.compile(optimizer='adam', loss='mse')"]},{"cell_type":"code","execution_count":null,"id":"81404929","metadata":{"id":"81404929"},"outputs":[],"source":["#Generate training data\n","BATCH_SIZE = 64\n","EPOCH=800\n","STEPS_PER_EPOCH = 2"]},{"cell_type":"code","execution_count":null,"id":"aadc398b","metadata":{"id":"aadc398b"},"outputs":[],"source":["history = model.fit_generator(colornet_img_generator(X_train,\n","                                                     BATCH_SIZE),\n","                              epochs=EPOCH,\n","                              steps_per_epoch=STEPS_PER_EPOCH)"]},{"cell_type":"code","execution_count":null,"id":"205ff272","metadata":{"id":"205ff272"},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","id":"60a71864","metadata":{"id":"60a71864"},"source":["## Analyze Model Performance"]},{"cell_type":"code","execution_count":null,"id":"ddc6a8b2","metadata":{"id":"ddc6a8b2"},"outputs":[],"source":["f, (ax2) = plt.subplots(1, 1, figsize=(25, 5))\n","t = f.suptitle('ColorNet Performance', fontsize=12)\n","f.subplots_adjust(top=0.85, wspace=0.3)\n","\n","epochs = list(range(1,EPOCH+1))\n","\n","ax2.plot(epochs, history.history['loss'], label='Train Loss')\n","#ax2.plot(epochs, history.history['val_loss'], label='Validation Loss')\n","ax2.set_xticks(epochs)\n","ax2.set_ylabel('Loss Value')\n","ax2.set_xlabel('Epoch')\n","ax2.set_title('Loss')\n","l2 = ax2.legend(loc=\"best\")\n","\n","plt.tight_layout()\n","f.autofmt_xdate()\n","plt.show()"]},{"cell_type":"markdown","id":"1139ff1d","metadata":{"id":"1139ff1d"},"source":["## Test Data\n","\n","Visualize the hallucinated output on the test dataset to understand how well the model has learnt the image colorization task"]},{"cell_type":"code","execution_count":null,"id":"48369166","metadata":{"id":"48369166"},"outputs":[],"source":["#Make predictions on validation images\n","\n","IMG_DIR = ''\n","sample_img = []\n","for filename in test_files:\n","    sample_img.append(sp.misc.imresize(load_img(IMG_DIR+filename),\n","                                     (DIM, DIM)))\n","sample_img = np.array(sample_img,\n","                    dtype=float)\n","sample_img = 1.0/255*sample_img\n","sample_img = gray2rgb(rgb2gray(sample_img))\n","\n","sample_img = rgb2lab(sample_img)[:,:,:,0]\n","sample_img = sample_img.reshape(sample_img.shape+(1,))\n","\n","#embedding input\n","sample_img_embed = create_vgg_embedding(sample_img)"]},{"cell_type":"code","execution_count":null,"id":"81f9da4c","metadata":{"id":"81f9da4c"},"outputs":[],"source":["# Test model\n","output_img = model.predict([sample_img, sample_img_embed])\n","output_img = output_img * 128"]},{"cell_type":"code","execution_count":null,"id":"7e7dda80","metadata":{"id":"7e7dda80"},"outputs":[],"source":["filenames = test_files\n","# Output colorizations\n","for i in range(len(output_img)):\n","    fig = plt.figure(figsize=(8,8))\n","    final_img = np.zeros((DIM,DIM, 3))\n","\n","    # add grayscale channel\n","    final_img[:,:,0] = sample_img[i][:,:,0]\n","\n","    # add predicted channel\n","    final_img[:,:,1:] = output_img[i]\n","\n","    img_obj = load_img(IMG_DIR+filenames[i])\n","\n","    fig.add_subplot(1, 3, 1)\n","    plt.axis('off')\n","\n","    grayed_img = gray2rgb(\n","                  rgb2gray(\n","                      img_to_array(\n","                          img_obj)/255)\n","                  )\n","    plt.imshow(grayed_img)\n","    plt.title(\"grayscale\")\n","\n","    fig.add_subplot(1, 3, 2)\n","    plt.axis('off')\n","    imshow(lab2rgb(final_img))\n","    plt.title(\"hallucination\")\n","\n","    fig.add_subplot(1, 3, 3)\n","    plt.imshow(img_obj)\n","    plt.title(\"original\")\n","    plt.axis('off')\n","    plt.show()"]}],"metadata":{"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}