{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2bc55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, fbeta_score\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Activation, BatchNormalization, GlobalAveragePooling2D, Input\n",
    "\n",
    "# Set seeds to make the experiment more reproducible.\n",
    "from tensorflow import set_random_seed\n",
    "from numpy.random import seed\n",
    "set_random_seed(0)\n",
    "seed(0)\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b4f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/imet-2019-fgvc6/train.csv')\n",
    "labels = pd.read_csv('../input/imet-2019-fgvc6/labels.csv')\n",
    "test = pd.read_csv('../input/imet-2019-fgvc6/sample_submission.csv')\n",
    "\n",
    "train[\"attribute_ids\"] = train[\"attribute_ids\"].apply(lambda x:x.split(\" \"))\n",
    "train[\"id\"] = train[\"id\"].apply(lambda x: x + \".png\")\n",
    "test[\"id\"] = test[\"id\"].apply(lambda x: x + \".png\")\n",
    "\n",
    "print('Number of train samples: ', train.shape[0])\n",
    "print('Number of test samples: ', test.shape[0])\n",
    "print('Number of labels: ', labels.shape[0])\n",
    "display(train.head())\n",
    "display(labels.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc03cc69",
   "metadata": {},
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e99567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.0001\n",
    "HEIGHT = 64\n",
    "WIDTH = 64\n",
    "CANAL = 3\n",
    "N_CLASSES = labels.shape[0]\n",
    "ES_PATIENCE = 10\n",
    "DECAY_DROP = 0.5\n",
    "DECAY_EPOCHS = 10\n",
    "classes = list(map(str, range(N_CLASSES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea499a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2_score_thr(threshold=0.5):\n",
    "    def f2_score(y_true, y_pred):\n",
    "        beta = 2\n",
    "        y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), threshold), K.floatx())\n",
    "\n",
    "        true_positives = K.sum(K.clip(y_true * y_pred, 0, 1), axis=1)\n",
    "        predicted_positives = K.sum(K.clip(y_pred, 0, 1), axis=1)\n",
    "        possible_positives = K.sum(K.clip(y_true, 0, 1), axis=1)\n",
    "\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "        return K.mean(((1+beta**2)*precision*recall) / ((beta**2)*precision+recall+K.epsilon()))\n",
    "    return f2_score\n",
    "\n",
    "\n",
    "def custom_f2(y_true, y_pred):\n",
    "    beta = 2\n",
    "\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f2 = (1+beta**2)*p*r / (p*beta**2 + r + 1e-15)\n",
    "\n",
    "    return f2\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = LEARNING_RATE\n",
    "    drop = DECAY_DROP\n",
    "    epochs_drop = DECAY_EPOCHS\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    \n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2c68e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen=ImageDataGenerator(rescale=1./255, validation_split=0.25)\n",
    "\n",
    "train_generator=train_datagen.flow_from_dataframe(\n",
    "    dataframe=train,\n",
    "    directory=\"../input/imet-2019-fgvc6/train\",\n",
    "    x_col=\"id\",\n",
    "    y_col=\"attribute_ids\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    class_mode=\"categorical\",\n",
    "    classes=classes,\n",
    "    target_size=(HEIGHT, WIDTH),\n",
    "    subset='training')\n",
    "\n",
    "valid_generator=train_datagen.flow_from_dataframe(\n",
    "    dataframe=train,\n",
    "    directory=\"../input/imet-2019-fgvc6/train\",\n",
    "    x_col=\"id\",\n",
    "    y_col=\"attribute_ids\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    class_mode=\"categorical\",    \n",
    "    classes=classes,\n",
    "    target_size=(HEIGHT, WIDTH),\n",
    "    subset='validation')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(  \n",
    "        dataframe=test,\n",
    "        directory = \"../input/imet-2019-fgvc6/test\",    \n",
    "        x_col=\"id\",\n",
    "        target_size=(HEIGHT, WIDTH),\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        class_mode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ff8a02",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5826a855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_out):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    base_model = applications.Xception(weights=None, include_top=False,\n",
    "                   input_tensor=input_tensor)\n",
    "    base_model.load_weights('../input/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    final_output = Dense(n_out, activation='sigmoid', name='final_output')(x)\n",
    "    model = Model(input_tensor, final_output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134002df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warm up model\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "model = create_model(input_shape=(HEIGHT, WIDTH, CANAL), n_out=N_CLASSES)\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "for i in range(-5,0):\n",
    "    model.layers[i].trainable = True\n",
    "    \n",
    "optimizer = optimizers.Adam(lr=LEARNING_RATE)\n",
    "metrics = [\"accuracy\", \"categorical_accuracy\"]\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=ES_PATIENCE)\n",
    "callbacks = [es]\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\",  metrics=metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ce819",
   "metadata": {},
   "source": [
    "#### Train top layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759dc4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID = valid_generator.n//valid_generator.batch_size\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                              validation_data=valid_generator,\n",
    "                              validation_steps=STEP_SIZE_VALID,\n",
    "                              epochs=EPOCHS,\n",
    "                              callbacks=callbacks,\n",
    "                              verbose=2,\n",
    "                              max_queue_size=16, workers=3, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33af7154",
   "metadata": {},
   "source": [
    "#### Fine-tune the complete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04dab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "metrics = [\"accuracy\", \"categorical_accuracy\"]\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=(ES_PATIENCE))\n",
    "callbacks = [es, lrate]\n",
    "optimizer = optimizers.Adam(lr=0.0001)\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\",  metrics=metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17df6f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID = valid_generator.n//valid_generator.batch_size\n",
    "\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                              validation_data=valid_generator,\n",
    "                              validation_steps=STEP_SIZE_VALID,\n",
    "                              epochs=EPOCHS,\n",
    "                              callbacks=callbacks,\n",
    "                              verbose=2,\n",
    "                              max_queue_size=16, workers=3, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0e25b4",
   "metadata": {},
   "source": [
    "### Complete model graph loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20adc231",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharex='col', figsize=(20,7))\n",
    "\n",
    "\n",
    "ax1.plot(history.history['loss'], label='Train loss')\n",
    "ax1.plot(history.history['val_loss'], label='Validation loss')\n",
    "ax1.legend(loc='best')\n",
    "ax1.set_title('Loss')\n",
    "\n",
    "ax2.plot(history.history['acc'], label='Train Accuracy')\n",
    "ax2.plot(history.history['val_acc'], label='Validation accuracy')\n",
    "ax2.legend(loc='best')\n",
    "ax2.set_title('Accuracy')\n",
    "\n",
    "ax3.plot(history.history['categorical_accuracy'], label='Train Cat Accuracy')\n",
    "ax3.plot(history.history['val_categorical_accuracy'], label='Validation Cat Accuracy')\n",
    "ax3.legend(loc='best')\n",
    "ax3.set_title('Cat Accuracy')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4cf628",
   "metadata": {},
   "source": [
    "### Find best threshold value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89ae5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lastFullValPred = np.empty((0, N_CLASSES))\n",
    "lastFullValLabels = np.empty((0, N_CLASSES))\n",
    "\n",
    "for i in range(STEP_SIZE_VALID+1):\n",
    "    im, lbl = next(valid_generator)\n",
    "    scores = model.predict(im, batch_size=valid_generator.batch_size)\n",
    "    lastFullValPred = np.append(lastFullValPred, scores, axis=0)\n",
    "    lastFullValLabels = np.append(lastFullValLabels, lbl, axis=0)\n",
    "    \n",
    "print(lastFullValPred.shape, lastFullValLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d78fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_fixed_threshold(preds, targs, do_plot=True):\n",
    "    score = []\n",
    "    thrs = np.arange(0, 0.5, 0.01)\n",
    "    for thr in thrs:\n",
    "        score.append(custom_f2(targs, (preds > thr).astype(int)))\n",
    "    score = np.array(score)\n",
    "    pm = score.argmax()\n",
    "    best_thr, best_score = thrs[pm], score[pm].item()\n",
    "    print(f'thr={best_thr:.3f}', f'F2={best_score:.3f}')\n",
    "    if do_plot:\n",
    "        plt.plot(thrs, score)\n",
    "        plt.vlines(x=best_thr, ymin=score.min(), ymax=score.max())\n",
    "        plt.text(best_thr+0.03, best_score-0.01, f'$F_{2}=${best_score:.3f}', fontsize=14);\n",
    "        plt.show()\n",
    "    return best_thr, best_score\n",
    "\n",
    "threshold, best_score = find_best_fixed_threshold(lastFullValPred, lastFullValLabels, do_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dce33f",
   "metadata": {},
   "source": [
    "### Apply model to test set and output predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f941f492",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.reset()\n",
    "STEP_SIZE_TEST = test_generator.n//test_generator.batch_size\n",
    "preds = model.predict_generator(test_generator, steps=STEP_SIZE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1484525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for pred_ar in preds:\n",
    "    valid = []\n",
    "    for idx, pred in enumerate(pred_ar):\n",
    "        if pred > threshold:\n",
    "            valid.append(idx)\n",
    "    if len(valid) == 0:\n",
    "        valid.append(np.argmax(pred_ar))\n",
    "    predictions.append(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fe7e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = test_generator.filenames\n",
    "label_map = {valid_generator.class_indices[k] : k for k in valid_generator.class_indices}\n",
    "\n",
    "results = pd.DataFrame({'id':filenames, 'attribute_ids':predictions})\n",
    "results['id'] = results['id'].map(lambda x: str(x)[:-4])\n",
    "results['attribute_ids'] = results['attribute_ids'].apply(lambda x: list(map(label_map.get, x)))\n",
    "results[\"attribute_ids\"] = results[\"attribute_ids\"].apply(lambda x: ' '.join(x))\n",
    "results.to_csv('submission.csv',index=False)\n",
    "results.head(10)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
