{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86db26f3",
   "metadata": {},
   "source": [
    "## Grouping Objects by similarity using k-means\n",
    "* __k-means__ clustering belongs to the category of __prototype-based clustering.__\n",
    "* __Prototype-based clustering__ means that each cluster is represented by a prototype, which can either be the __centroid (average)__ of similar points with continuous features, or the __medoid (the most representative or most frequently occurring point)__ in the case of categorical features.\n",
    "* Easy to implement as well as computationally very efficient as compared to other clustering algorithms.\n",
    "* We have to specify the number of clusters k a priori.\n",
    "* An inappropriate choice for k can result in poor clustering performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eaf04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''function to save image'''\n",
    "def saveimg(name='fig'):\n",
    "    if not os.path.exists(os.path.join(os.getcwd(), 'figures')):\n",
    "        os.mkdir('figures')\n",
    "    plt.savefig('./figures/%s.png'% name, dpi=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73695fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Learning by doing\n",
    "'''\n",
    "import os\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=150,\n",
    "                 n_features=2,\n",
    "                 centers=3,\n",
    "                 cluster_std=0.5,\n",
    "                 shuffle=True,\n",
    "                 random_state=0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[:, 0],\n",
    "           X[:, 1],\n",
    "           c='black',\n",
    "           marker='o',\n",
    "           s=50)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "# save('dataset')\n",
    "plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c972e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''apply k-means algorithm to our dataset'''\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=3,\n",
    "           init='random',\n",
    "           n_init=10,\n",
    "           max_iter=300,\n",
    "           tol=1e-04,\n",
    "           random_state=0)\n",
    "y_km = km.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3833cb0",
   "metadata": {},
   "source": [
    "## Shortcomings of K-means\n",
    "1. the classic k-means algorithm that uses a random seed to place the initial centroids, which can sometimes result in bad clusterings or slow convergence if the initial centroids are chosen poorly. Methods to solve this are: _run kmeans multiple times and select the best performing model in terms of SSE._ __OR__ _place the initial centroids far away from each other via the k-means++ algorithm, which leads to better and more consistent results._\n",
    "\n",
    "2. Another problem with k-means is that one or more clusters can be empty. Note that this problem does not exist for k-medoids or fuzzy C-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''implementing k-means++'''\n",
    "km_plus2 = KMeans(n_clusters=3,\n",
    "           init='k-means++',\n",
    "           n_init=10,\n",
    "           max_iter=300,\n",
    "           tol=1e-04,\n",
    "           random_state=0)\n",
    "y_km_plus2 = km.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af58ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''visualize the clusters that k-means identified'''\n",
    "plt.scatter(X[y_km==0, 0],\n",
    "           X[y_km==0, 1],\n",
    "           s=50,\n",
    "           c='lightgreen',\n",
    "           marker='s',\n",
    "           label='cluster 1')\n",
    "\n",
    "plt.scatter(X[y_km==1, 0],\n",
    "           X[y_km==1, 1],\n",
    "           s=50,\n",
    "           c='orange',\n",
    "           marker='o',\n",
    "           label='cluster 2')\n",
    "\n",
    "plt.scatter(X[y_km==2, 0],\n",
    "           X[y_km==2, 1],\n",
    "           s=50,\n",
    "           c='lightblue',\n",
    "           marker='v',\n",
    "           label='cluster 3')\n",
    "\n",
    "plt.scatter(km.cluster_centers_[:, 0],\n",
    "            km.cluster_centers_[:, 1],\n",
    "            c='red',\n",
    "            s=250,\n",
    "            marker='*',\n",
    "            label='centroids')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "# saveimg('k-means-plot')\n",
    "plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279ea3f6",
   "metadata": {},
   "source": [
    "## Hard versus Soft clustering\n",
    "* Hard clustering describes a family of algorithms where each sample in a dataset is assigned to __exactly one cluster__, as in the k-means algorithm.\n",
    "* In contrast, algorithms for soft clustering (sometimes also called fuzzy clustering) assign a sample to __one or more clusters.__\n",
    "* A popular example of soft clustering is the __fuzzy C-means (FCM) algorithm__ (also called soft k-means or fuzzy k-means)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436778bc",
   "metadata": {},
   "source": [
    "## Using the elbow method to find the optimal number of clusters\n",
    "In order to quantify the quality of clustering, we need to use intrinsic metrics—such as the within-cluster SSE (distortion), to compare the performance of different k-means clusterings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a029c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''the `inertia_` method return the within-cluster SSE'''\n",
    "print('Distortion: %.2f' % km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a724c7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Elbow Method\n",
    "------------\n",
    "If k increases, the distortion will decrease.\n",
    "\"\"\"\n",
    "distortions = []\n",
    "for i in range(1, 11):\n",
    "    km = KMeans(n_clusters=i,\n",
    "               init='k-means++',\n",
    "               n_init=10,\n",
    "               max_iter=300,\n",
    "               random_state=0)\n",
    "    km.fit(X)\n",
    "    distortions.append(km.inertia_)\n",
    "plt.plot(range(1,11), distortions, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.show()\n",
    "# saveimg('elbow-plot')\n",
    "plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37fb451",
   "metadata": {},
   "source": [
    "## Quantifying the quality of clustering via silhouette plots\n",
    "Silhouette analysis can be used as a graphical tool to plot a measure of how tightly grouped the samples in the clusters are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deadd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Silhouette analysis\n",
    "\"\"\"\n",
    "km = KMeans(n_clusters=3,\n",
    "            init='k-means++',\n",
    "            n_init=10,\n",
    "            max_iter=300,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "y_km = km.fit_predict(X)\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from sklearn.metrics import silhouette_samples\n",
    "cluster_labels = np.unique(y_km)\n",
    "n_clusters = cluster_labels.shape[0]\n",
    "silhouette_vals = silhouette_samples(X,y_km,\n",
    "                                     metric='euclidean')\n",
    "# plot the silhouette analysis\n",
    "y_ax_lower, y_ax_upper = 0,0\n",
    "yticks = []\n",
    "for i, c in enumerate(cluster_labels):\n",
    "    c_silhouette_vals = silhouette_vals[y_km == c]\n",
    "    c_silhouette_vals.sort()\n",
    "    y_ax_upper += len(c_silhouette_vals)\n",
    "    color = cm.jet(float(i) / n_clusters)\n",
    "    plt.barh(range(y_ax_lower, y_ax_upper),\n",
    "             c_silhouette_vals,\n",
    "             height=1.0,\n",
    "             edgecolor='none',\n",
    "             color=color)\n",
    "    yticks.append((y_ax_lower + y_ax_upper) / 2)\n",
    "    y_ax_lower += len(c_silhouette_vals)\n",
    "silhouette_avg = np.mean(silhouette_vals)\n",
    "plt.axvline(silhouette_avg,\n",
    "color=\"red\",\n",
    "linestyle=\"--\")\n",
    "plt.yticks(yticks, cluster_labels + 1)\n",
    "plt.ylabel('Cluster')\n",
    "plt.xlabel('Silhouette coefficient')\n",
    "plt.show()\n",
    "# saveimg('silhouette-analysis')\n",
    "plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8ae2dc",
   "metadata": {},
   "source": [
    "__Conclusion__: The silhouette coefficients are not even close to 0, which can be an indicator of a good clustering. Furthermore, to summarize the goodness of our clustering, we added the average silhouette coefficient to the plot (dotted line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d002a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's plot the silhouette analysis for bad clustering\n",
    "\"\"\"\n",
    "km = KMeans(n_clusters=2,\n",
    "            init='k-means++',\n",
    "            n_init=10,\n",
    "            max_iter=300,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "y_km = km.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[y_km==0,0],\n",
    "            X[y_km==0,1],\n",
    "            s=50, c='lightgreen',\n",
    "            marker='s',\n",
    "            label='cluster 1')\n",
    "\n",
    "plt.scatter(X[y_km==1,0],\n",
    "            X[y_km==1,1],\n",
    "            s=50,\n",
    "            c='orange',\n",
    "            marker='o',\n",
    "            label='cluster 2')\n",
    "\n",
    "plt.scatter(km.cluster_centers_[:,0],\n",
    "            km.cluster_centers_[:,1],\n",
    "            s=250,\n",
    "            marker='*',\n",
    "            c='red',\n",
    "            label='centroids')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "# saveimg('bad-clustering')\n",
    "plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf341ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the silhouette analysis\n",
    "y_ax_lower, y_ax_upper = 0,0\n",
    "yticks = []\n",
    "for i, c in enumerate(cluster_labels):\n",
    "    c_silhouette_vals = silhouette_vals[y_km == c]\n",
    "    c_silhouette_vals.sort()\n",
    "    y_ax_upper += len(c_silhouette_vals)\n",
    "    color = cm.jet(float(i) / n_clusters)\n",
    "    plt.barh(range(y_ax_lower, y_ax_upper),\n",
    "             c_silhouette_vals,\n",
    "             height=1.0,\n",
    "             edgecolor='none',\n",
    "             color=color)\n",
    "    yticks.append((y_ax_lower + y_ax_upper) / 2)\n",
    "    y_ax_lower += len(c_silhouette_vals)\n",
    "silhouette_avg = np.mean(silhouette_vals)\n",
    "plt.axvline(silhouette_avg,\n",
    "color=\"red\",\n",
    "linestyle=\"--\")\n",
    "plt.yticks(yticks, cluster_labels + 1)\n",
    "plt.ylabel('Cluster')\n",
    "plt.xlabel('Silhouette coefficient')\n",
    "plt.show()\n",
    "# saveimg('silhouette-bad-clustering')\n",
    "plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5ea38e",
   "metadata": {},
   "source": [
    "## Organizing clusters as a hierarchical tree\n",
    "* The two main approaches to hierarchical clustering are __agglomerative__ and __divisive__ hierarchical clustering.\n",
    "* In divisive hierarchical clustering, we start with one clusterthat encompasses all our samples, and we iteratively split the cluster into smaller clusters until each cluster only contains one sample.\n",
    "* The two standard algorithms for agglomerative hierarchical clustering are __single linkage__ and __complete linkage.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54923664",
   "metadata": {},
   "source": [
    "### Agglomerative clustering (using complete linkage approach)\n",
    "1. Compute the distance matrix of all the clusters.\n",
    "2. Represent each data point as a singleton cluster\n",
    "3. Merge the two closest clusters based on the distance of the most dissimilar members.\n",
    "4. Update the similarity matrix.\n",
    "5. Repeat steps 2 to 4 until one single cluster remains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cae9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Computing the distance matrix.\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "variables = ['X', 'Y', 'Z']\n",
    "labels = ['ID_0','ID_1','ID_2','ID_3','ID_4']\n",
    "X = np.random.random_sample([5,3])*10\n",
    "df = pd.DataFrame(X, columns=variables, index=labels)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01063b53",
   "metadata": {},
   "source": [
    "## Performing hierarchical clustering on a distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57739db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "row_dist = pd.DataFrame(squareform(\n",
    "                        pdist(df, metric='euclidean')),\n",
    "                       columns=labels, index=labels)\n",
    "row_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375a473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Apply complete linkage agglomeration to the clusters.\n",
    "\"\"\"\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "row_clusters = linkage(pdist(df, metric='euclidean'),\n",
    "                      method='complete')\n",
    "pd.DataFrame(row_clusters,\n",
    "             columns=['row label 1',\n",
    "                      'row label 2',\n",
    "                      'distance',\n",
    "                      'no. of items in clust.'],\n",
    "             index=['cluster %d' %(i+1) for i in\n",
    "                    range(row_clusters.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac2f2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "visualize the results in the form of a dendrogram\n",
    "\"\"\"\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "# make dendrogram black (part 1/2)\n",
    "# from scipy.cluster.hierarchy import set_link_color_palette\n",
    "# set_link_color_palette(['black'])\n",
    "row_dendr = dendrogram(row_clusters,\n",
    "                     labels=labels,\n",
    "                     # make dendogram black(part 2/2)\n",
    "                     # color_threshold=np.inf\n",
    "                     )\n",
    "plt.tight_layout()\n",
    "plt.ylabel('Euclidean distance')\n",
    "plt.show()\n",
    "# saveimg('dendrogram')\n",
    "plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71467646",
   "metadata": {},
   "source": [
    "## Attaching dendrograms to a heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f92272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. We create a new figure object and define the x axis position, y axis\n",
    "#    position, width, and height of the dendrogram via the add_axes attribute.\n",
    "#    Furthermore, we rotate the dendrogram 90 degrees counter-clockwise.\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "axd = fig.add_axes([0.09,0.1,0.2,0.6])\n",
    "row_dendr = dendrogram(row_clusters, orientation='left')\n",
    "\n",
    "# 2. Next we reorder the data in our initial DataFrame according to the clustering\n",
    "#    labels that can be accessed from the dendrogram object, which is essentially a\n",
    "#    Python dictionary, via the leaves key.\n",
    "df_rowclust = df.iloc[row_dendr['leaves'][::-1]]\n",
    "\n",
    "# 3. Now we construct the heat map from the reordered DataFrame and position\n",
    "#    it right next to the dendrogram\n",
    "axm = fig.add_axes([0.23,0.1,0.6,0.6])\n",
    "cax = axm.matshow(df_rowclust,\n",
    "                  interpolation='nearest', cmap='hot_r')\n",
    "\n",
    "# 4. Finally we will modify the aesthetics of the heat map by removing the axis\n",
    "#    ticks and hiding the axis spines. Also, we will add a color bar and assign\n",
    "#    the feature and sample names to the x and y axis tick labels, respectively.\n",
    "axd.set_xticks([])\n",
    "axd.set_yticks([])\n",
    "for i in axd.spines.values():\n",
    "    i.set_visible(False)\n",
    "fig.colorbar(cax)\n",
    "axm.set_xticklabels([''] + list(df_rowclust.columns))\n",
    "axm.set_yticklabels([''] + list(df_rowclust.index))\n",
    "plt.show()\n",
    "# saveimg('dendrogram-plus-heatmap')\n",
    "plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e806e8b1",
   "metadata": {},
   "source": [
    "## Applying agglomerative clustering via scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd13ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "ac = AgglomerativeClustering(n_clusters=2,\n",
    "                            affinity='euclidean',\n",
    "                            linkage='complete')\n",
    "labels = ac.fit_predict(X)\n",
    "print('Cluster labels: %s' % labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8707941c",
   "metadata": {},
   "source": [
    "## Locating regions of hight density using DBSCAN\n",
    "DBSCAN stands for _Density-based Spatial Clustering of Applications with Noise_\n",
    "\n",
    "In DBSCAN, a special label is assigned to each sample (point) using the\n",
    "following criteria:\n",
    "* A point is considered as core point if at least a specified number (MinPts) of neighboring points fall within the specified radius ε\n",
    "* A border point is a point that has fewer neighbors than MinPts within but lies within the ε radius of a core point ε ,\n",
    "* All other points that are neither core nor border points are considered as noise points\n",
    "\n",
    "After labeling the points as core, border, or noise points, the DBSCAN algorithm can\n",
    "be summarized in two simple steps:\n",
    "1.\t Form a separate cluster for each core point or a connected group of core\n",
    "points (core points are connected if they are no farther away than ε ).\n",
    "2.\t Assign each border point to the cluster of its corresponding core point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6588a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create a new dataset of half-moon-shaped structures to compare\n",
    "k-means clustering, hierarchical clustering, and DBSCAN\n",
    "\"\"\"\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=200,\n",
    "                 noise=0.05,\n",
    "                 random_state=0)\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.show()\n",
    "# saveimg('half-moon-dataset')\n",
    "plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4345f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "k-means algorithm and complete linkage clustering\n",
    "\"\"\"\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,3))\n",
    "km = KMeans(n_clusters=2,\n",
    "           random_state=0)\n",
    "y_km = km.fit_predict(X)\n",
    "ax1.scatter(X[y_km==0,0],\n",
    "            X[y_km==0,1],\n",
    "            c='lightblue',\n",
    "            marker='o',\n",
    "            s=40,\n",
    "            label='cluster 1')\n",
    "ax1.scatter(X[y_km==1,0],\n",
    "            X[y_km==1,1],\n",
    "            c='red',\n",
    "            marker='s',\n",
    "            s=40,\n",
    "            label='cluster 2')\n",
    "ax1.set_title('K-means clustering')\n",
    "ac = AgglomerativeClustering(n_clusters=2,\n",
    "                            affinity='euclidean',\n",
    "                            linkage='complete')\n",
    "y_ac = ac.fit_predict(X)\n",
    "ax2.scatter(X[y_ac==0,0],\n",
    "            X[y_ac==0,1],\n",
    "            c='lightblue',\n",
    "            marker='o',\n",
    "            s=40,\n",
    "            label='cluster 1')\n",
    "ax2.scatter(X[y_ac==1,0],\n",
    "            X[y_ac==1,1],\n",
    "            c='red',\n",
    "            marker='s',\n",
    "            s=40,\n",
    "            label='cluster 2')\n",
    "ax2.set_title('Agglomerative clustering')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# saveimg('kmeans-agglomerative-performance')\n",
    "plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c174124",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "using DBSCAN algorithm\n",
    "\"\"\"\n",
    "from sklearn.cluster import DBSCAN\n",
    "db = DBSCAN(eps=0.2,\n",
    "           min_samples=5,\n",
    "           metric='euclidean'\n",
    "           )\n",
    "y_db = db.fit_predict(X)\n",
    "\n",
    "# plot the graphs\n",
    "plt.scatter(X[y_db==0,0],\n",
    "            X[y_db==0,1],\n",
    "            c='lightblue',\n",
    "            marker='o',\n",
    "            s=40,\n",
    "            label='cluster 1')\n",
    "plt.scatter(X[y_db==1,0],\n",
    "            X[y_db==1,1],\n",
    "            c='red',\n",
    "            marker='s',\n",
    "            s=40,\n",
    "            label='cluster 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# saveimg('DBSCAN-performance')\n",
    "plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545b1341",
   "metadata": {},
   "source": [
    "## Disadvantages of DBSCAN\n",
    "1. With an increasing number of features in our dataset—given a fixed size training set—the negative effect of the curse of dimensionality increases.\n",
    "2. This is especially a problem if we are using the Euclidean distance metric.\n",
    "3. Finding a good combination of MinPts and ε can be problematic if the density differences in the dataset are relatively large."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
