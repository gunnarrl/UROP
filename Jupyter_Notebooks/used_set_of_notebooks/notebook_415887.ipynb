{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a3da951",
   "metadata": {},
   "source": [
    "# Working with NewLab data set using SQL data\n",
    "We have data from our Sept-Dec deployment in NL uploded to our SQL DB \n",
    "\n",
    "I'd like to try: \n",
    "* pulling this data\n",
    "* SQL query experimentation\n",
    "* plotting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347f6f6f",
   "metadata": {},
   "source": [
    "## FYI sqlconfig\n",
    "to ```import sqlconfig``` the file \"sqlconfig.py\" should be in this folder or directory adjusted acordingly  \n",
    "This file has the user/password for SQL connection and is in the gitignore so you will have to create this locally\n",
    "\n",
    "---\n",
    "Create sqlconfig.py as:\n",
    "```python\n",
    "# .gitignore should include reference to config.py\n",
    "passwd = \"[password]\"\n",
    "user = \"[username]\"\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a41ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as FF\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import os.path\n",
    "import pymysql\n",
    "import sqlconfig # From sqlconfig.py\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import psycopg2\n",
    "from tqdm import tqdm\n",
    "print(\"Import Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e9145f",
   "metadata": {},
   "source": [
    "### SQL setup\n",
    "create engine for CBAS db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239641f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "passwd = sqlconfig.passwd  # From sqlconfig.py\n",
    "user = sqlconfig.user  # From sqlconfig.py\n",
    "DB = 'NewLab'  #name of databases to activate \n",
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sqlalchemy.create_engine('postgresql+psycopg2://'+user+':'+passwd+'@35.221.58.17/'+DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da76e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query= ''' \n",
    "SELECT * from cbasnl\n",
    "-- where sensor = 'protoCBAS-G' AND\n",
    "-- timestamp BETWEEN '2019-09-21 00:00:00' and '2019-09-30 11:59:00'\n",
    "ORDER BY timestamp asc;\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3144b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#place query in CBAStest df\n",
    "\n",
    "CBAS =  pd.read_sql(query,engine,\n",
    "                        index_col=[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25dc748",
   "metadata": {},
   "outputs": [],
   "source": [
    "CBAS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dfc8b7",
   "metadata": {},
   "source": [
    "Check which sensors are in this data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2a0753",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CBAS.sensor.unique())\n",
    "# what unique values are in \"sensor\" column\n",
    "print(type(CBAS.index)) # check timestamp is recognized as DatetimeIndex\n",
    "print(CBAS.index.min())\n",
    "print(CBAS.index.max())\n",
    "# min/max index valeus (date range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606b034",
   "metadata": {},
   "source": [
    "### So \"CBAS\" is one dataframe \n",
    "* df date ranges from '2019-09-06 15:59:00' - '2020-01-08 19:05:00'\n",
    "* Sensors in this df ['protoCBAS-A', 'protoCBAS-B', 'protoCBAS-C', 'protoCBAS-D','protoCBAS-G']\n",
    "\n",
    "This is a bit different from how we usually managed dataframes where we had a list of dataframes for each sensor.  \n",
    "### options to manage (one df vs list of dfs):\n",
    "* grouping this df by \"sensor\" column into different dfs and place them in list\n",
    "* Use Pandas filtering/grouping as needed, only situation I see this needed is when ploting different sensors as seperate traces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31dfd24",
   "metadata": {},
   "source": [
    "### Further exploring the dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f220bfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CBAS.columns)\n",
    "# what columns do we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c399ec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CBAS.Position_HumanReadable.unique())\n",
    "# unique values in Position_HumanReadable column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4996c549",
   "metadata": {},
   "source": [
    "Thinking how this will plot, and how this data differes from .CSV files usually used\n",
    "looking at extradata_to_static_dash.py script and how it manages dfs\n",
    "\n",
    "Some in script modifications to this data for plotting are:\n",
    "* Timezone convesion\n",
    "    Convert to NYC tz, index will need to be timezone aware to work  \n",
    "    Code:\n",
    "    ```python\n",
    "    def tz_NYC(d): \n",
    "        d.index = d.index.tz_convert('America/New_York')\n",
    "        return d\n",
    "    dfs = list(map(tz_NYC, dfs))    \n",
    "    \n",
    "    ```\n",
    "    ---\n",
    "* Adjust for \"gremlins\" in CBAS-B CO2 sensor  \n",
    "```\n",
    "RCO2 data is offset by +782ppm from \"2019-09-05 \"-\"2019-11-10 \"\n",
    "after \"2019-11-10 \" CO2 seemed to report as it should.  \n",
    "Made this tweak in January, not sure if anything has changed as of writing this (Mar-09) \n",
    "```\n",
    "    ```python\n",
    "     dfs[1][\"2019-09-05 \":\"2019-11-10 \"][\"RCO2\"] = (dfs[1][\"2019-09-05 \":\"2019-11-10 \"][\"RCO2\"]-782) #adjust for gremlins in CBAS-B CO2 sensor\n",
    "\n",
    "    ```\n",
    "    ---\n",
    "* Remove \"Wind Tunnel\" testing  \n",
    "    pull data reffering to Wind tunnel as it is not related to NewLab \n",
    "```Python\n",
    "dfs = [d.loc[d[\"Position_HumanReadable\"] != '\"Wind Tunnel\"'] for d in dfs]\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cfd053",
   "metadata": {},
   "source": [
    "### Adjusting snippets for this data\n",
    "    My goal with this \"NewLab\" table is to have a dataset that is ready to pull and work with  \n",
    "    requiring less redundant modifications.  \n",
    "    So things like pulling out the wind tunnel testing and offsetting RCO@ for CBAS-B should be handled already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f40a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tz_NYC(d): \n",
    "        d.index = d.index.tz_convert('America/New_York')\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb25ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CBASNYC = tz_NYC(CBAS) # data in db shoudl stay as UTC, only convert just before displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101ea31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CBASNYC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72b8999",
   "metadata": {},
   "outputs": [],
   "source": [
    "CBASxwind = CBAS.loc[CBAS[\"Position_HumanReadable\"] != '\"Wind Tunnel\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f3cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CBAStestxwind.Position_HumanReadable.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532580c5",
   "metadata": {},
   "source": [
    "##  SELECT board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8270f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "Aquery= ''' \n",
    "SELECT * \n",
    "FROM cbasnl\n",
    "WHERE sensor = 'protoCBAS-A' \n",
    "-- AND timestamp BETWEEN '2019-09-06 00:00:00' and '2019-09-30 11:59:00'\n",
    "'''\n",
    "\n",
    "\n",
    "CBASA =  pd.read_sql(Aquery,engine,parse_dates=[\"timestamp\"], index_col=[\"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867788bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CBASA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c6408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CBASA.sensor"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
