{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73b3d996",
   "metadata": {},
   "source": [
    "# Chapter 2. Multi-armed Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1f49a9",
   "metadata": {},
   "source": [
    "## 2.1 A k-armed Bandit Problem\n",
    "k개의 레버(k-armed)을 가진 빠찡코 기계(bandit)을 통해 게임을 할 때에 어떤 레버를 당겼을 때에 가장 많은 돈을 벌 수 있는 인공지능 프로그램(agent)를 만들어내는 문제. 단 아래의 몇 가지 제약 사항은 존재한다.\n",
    "1. 각 레버의 당첨 확률을 고정(stationary)되어 있다\n",
    "1. 당첨되는 경우와 그렇지 않은 경우 두 가지의 결과(reward)만 존재한다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1f2d0d",
   "metadata": {},
   "source": [
    "### Approarch\n",
    "1. k개의 레버에 대한 k가지의 선택(action)을 할 수 있고 많은 시도 끝에 특정 레버에 대한 보상이 더 좋다는 사실을 알 수 있다.\n",
    "1. 이러한  반복적인 실험을 통해 특정한 레버의 가치(value)를 판단할 수 있고 함수로 표현할 수도 있다\n",
    "1. 아래의 수식은 임의의 시점(t)에 임의의 행동(a)를 했을 때에 얻을 수 있는 실제 보상(Rt)을 아래와 같이 q\\*로 표현한다.\n",
    "$$q_*(a) \\doteq E\\begin{bmatrix}Rt|At=a\\end{bmatrix}$$\n",
    "1. 위의 q\\*는 정확히 알기 어려우므로 반복적인 실험을 통해 예측된(estimated) Qt(a)를 구하고자 한다\n",
    "1. 현재 지식으로 최선의 가치(reward)를 추구하는 **exploting**, 긴 미래를 생각해 탐험하는 것을 **exploring**이라 부른다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f14bc21",
   "metadata": {},
   "source": [
    "## 2.2 Action-value Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f263c4a",
   "metadata": {},
   "source": [
    "### Method One. <font color=orange>Epsilon Greedy Bandit</font> Method\n",
    "<br>\n",
    "#### References\n",
    "1. __[datahubbs.com/multi_armed_bandits_part1](https://www.datahubbs.com/multi_armed_bandits_reinforcement_learning_1)__ \n",
    "2. __[Jupyter notebook cheatsheet](https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed)__\n",
    "<br>\n",
    "***\n",
    "<br>\n",
    "#### Lessons from mistakes\n",
    "1. 결과가 자연스럽게 수렴하지 않고 계속 흔들리는 현상 : **average_rewards 변수를 episode loop 내부에 넣는 실수**\n",
    "1. 결과가 실행마다 epsilon 결과의 순서가 바뀌는 현상 : **dict 킷값 없이 list 에 결과를 담아서 결과가 셔플되어 플롯되는 실수**\n",
    "1. reward 차트가 서서히 증가하지 않고 출력되는 현상 : **object 생성 및 episode 진행 시에 reset 하지않아 발생하는 오류**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a6a4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    " \n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "debug_flag = False\n",
    "\n",
    "def debug(*args):\n",
    "    if (debug_flag):\n",
    "        print(list(args))\n",
    "        \n",
    "def info(*args):\n",
    "    print(list(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2da98ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python self.n 과 같이 scalar 값의 increment 시에는 반드시 self.n 으로 써야 하고, reference 객체는 o = self.o 와 같이 받아 써도 된다.\n",
    "key = \"foo\"\n",
    "class Foo:\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "        self.dic = {}\n",
    "    def update(self):\n",
    "        print(\"update\")\n",
    "        n = self.n\n",
    "        dic = self.dic\n",
    "        n += 1\n",
    "        dic[key] = dic.get(key, 0) + n\n",
    "    def updateSelf(self):\n",
    "        print(\"udpateSelf\")\n",
    "        self.n += 1\n",
    "        self.dic[key] = self.dic.get(key, 0) + self.n\n",
    "    def print(self):\n",
    "        n = self.n\n",
    "        dic = self.dic\n",
    "        print('current n is {}'.format(n))\n",
    "        for k, v in dic.items():\n",
    "            print(k, v)\n",
    "        print()\n",
    "\n",
    "foo = Foo()\n",
    "foo.print()\n",
    "\n",
    "foo.update()\n",
    "foo.print()\n",
    "\n",
    "foo.updateSelf()\n",
    "foo.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e4cbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Figure 2.2. Average performance of \"-greedy action-value methods on the 10-armed testbed.\n",
    "class EgreedyBanditMethod:\n",
    "\n",
    "    def __init__(self, k = 10, eps=0):\n",
    "        self.k = k               # num of arms\n",
    "        self.eps = eps       # epsilon\n",
    "        self.n = 0             # step count\n",
    "        self.m_r = 0         # mean reward\n",
    "        self.k_n = np.zeros(k) # step count for every k\n",
    "        self.k_r = np.zeros(k) #  reward for every k\n",
    "        self.k_mu = np.random.normal(0, 1, k) # mu of every k\n",
    "        \n",
    "        info(\"{}({}, {}) called\".format(EgreedyBanditMethod.__name__, k, eps))\n",
    "\n",
    "    # eps 값에 따라서 0 ~ 10% 까지 not greedy action을 취한다.\n",
    "    def getAction(self):\n",
    "        prob = np.random.rand()\n",
    "        action = 0\n",
    "        if self.eps == 0 and self.n == 0:\n",
    "            action = np.random.choice(self.k)\n",
    "        elif prob < self.eps:\n",
    "            action = np.random.choice(self.k)\n",
    "        else:\n",
    "            action = np.argmax(self.k_r)\n",
    "        return action\n",
    "\n",
    "    # 현재 action의 mu의 normal distribution unit variance 의 값을 reward로 반환한다\n",
    "    def getReward(self, action):\n",
    "        reward = np.random.normal(self.k_mu[action], 1)\n",
    "        return reward\n",
    "\n",
    "    # reward 값을 모델에 반영 - update 시에는 항상 self 로 접근하는 것이 안전함\n",
    "    def updateModel(self, action, reward):\n",
    "        self.n += 1\n",
    "        self.k_n[action] += 1\n",
    "        self.m_r += (reward - self.m_r) / self.n\n",
    "        self.k_r[action] += (reward - self.k_r[action]) / self.k_n[action]\n",
    "        return self.m_r\n",
    "    \n",
    "    def reset(self):\n",
    "        self.n = 0             # step count\n",
    "        self.m_r = 0         # mean reward\n",
    "        self.k_n = np.zeros(self.k) # step count for every k\n",
    "        self.k_r = np.zeros(self.k) #  reward for every k\n",
    "        self.k_mu = np.random.normal(0, 1, self.k) # mu of every k\n",
    "\n",
    "def plotChart(title, rewards, episodes):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for key, reward in rewards.items():\n",
    "        plt.plot(reward, label=key)\n",
    "    plt.legend(bbox_to_anchor=(1.2, 0.5))\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Average Reward\")\n",
    "    plt.title(\"Average {} Rewards after {} Episodes\".format(title, str(episodes)))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173afb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runEgreedy(num_of_episodes = 1000, num_of_steps = 1000):\n",
    "    ebms = {\n",
    "        \"$\\\\epsilon=0.1$\":EgreedyBanditMethod(eps=0.1), \n",
    "        \"$\\\\epsilon=0.01$\":EgreedyBanditMethod(eps=0.01),\n",
    "        \"$\\\\epsilon=0 (greedy)$\":EgreedyBanditMethod(eps=0)\n",
    "    }\n",
    "    rwds = {}\n",
    "    for key, ebm in ebms.items():\n",
    "        average_rewards = np.zeros(num_of_steps)\n",
    "        for episode in range(num_of_episodes):\n",
    "            ebm.reset()\n",
    "            rewards = np.zeros(num_of_steps)\n",
    "            for step in range(num_of_steps):\n",
    "                action = ebm.getAction()\n",
    "                reward = ebm.getReward(action)\n",
    "                mean_reward = ebm.updateModel(action, reward)\n",
    "                debug(action, reward, mean_reward)\n",
    "                rewards[step] = mean_reward\n",
    "            average_rewards = average_rewards + (rewards - average_rewards) / (episode + 1)\n",
    "        rwds[key] = average_rewards.copy()\n",
    "    plotChart(\"$\\epsilon-greedy$\", rwds, num_of_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763de7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "runEgreedy(100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a2390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "runEgreedy(500, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93711b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "runEgreedy(1000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e0a57",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "### 2. Upper-Confidence-Bound Action Selection\n",
    "<br>\n",
    "#### References\n",
    "1. __[datahubbs.com/multi_armed_bandits_part2](https://www.datahubbs.com/multi-armed-bandits-reinforcement-learning-2)__\n",
    "2. __[Latex Mathmatics](https://en.wikibooks.org/wiki/LaTeX/Mathematics )__\n",
    "<br>\n",
    "$$A_t=\\underset{a}{\\operatorname{argmax}}\\begin{bmatrix}Q_t(a) + c\\sqrt{\\log{t}/N_t(a)}\\end{bmatrix},(2.10)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f53c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parent:\n",
    "    def __init__(self):\n",
    "        print(\"{} init called\".format(Parent.__name__))\n",
    "    def getAction(self):\n",
    "        print(\"parent get action\")\n",
    "    def getReward(self):\n",
    "        print(\"parent get reward\")\n",
    "    \n",
    "class Child(Parent):\n",
    "    bar = { \"foo\":\"bar\" }\n",
    "    def getAction(self):\n",
    "        print(\"child get action - {}\".format(self.bar[\"foo\"]))\n",
    "        \n",
    "class Foo:\n",
    "    bar = { \"foo\":\"bar\" }\n",
    "    \n",
    "child = Child()\n",
    "child.getAction()\n",
    "child.getReward()\n",
    "\n",
    "foo = Foo()\n",
    "print(foo.bar[\"foo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c267e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EgreedyBanditMethod 클래스를 상속하고 getAction 메소드만 override 한다.\n",
    "class UpperConfidenceBoundActionSelection(EgreedyBanditMethod):\n",
    "    def __init__(self, k=10, eps=0, steps=1000, c=2):\n",
    "        EgreedyBanditMethod.__init__(self, k, eps)\n",
    "        self.steps = steps\n",
    "        self.c = c\n",
    "        self.uncertainty = np.zeros(steps)\n",
    "        \n",
    "    def getAction(self):\n",
    "        action = 0\n",
    "        if self.eps == 0 and self.n == 0:\n",
    "            action = np.random.choice(self.k)\n",
    "        else:\n",
    "            action = np.argmax(self.k_r + self.c * np.sqrt(np.log(self.n) / self.k_n))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93bdbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runUCB(num_of_episodes = 1000, num_of_steps = 1000):\n",
    "    ucbs = {\n",
    "        \"$\\\\alpha=1$\" : UpperConfidenceBoundActionSelection(c=1),\n",
    "        \"$\\\\alpha=2$\" : UpperConfidenceBoundActionSelection(c=2),\n",
    "        \"$\\\\alpha=4$\" : UpperConfidenceBoundActionSelection(c=4) \n",
    "    }\n",
    "    rwds = {}\n",
    "    for key, ucb in ucbs.items():\n",
    "        average_rewards = np.zeros(num_of_steps)\n",
    "        for episode in range(num_of_episodes):\n",
    "            ucb.reset()\n",
    "            rewards = np.zeros(num_of_steps)\n",
    "            for step in range(num_of_steps):\n",
    "                action = ucb.getAction()\n",
    "                reward = ucb.getReward(action)\n",
    "                mean_reward = ucb.updateModel(action, reward)\n",
    "                debug(action, reward, mean_reward)\n",
    "                rewards[step] = mean_reward\n",
    "            average_rewards = average_rewards + (rewards - average_rewards) / (episode + 1)\n",
    "        rwds[key] = average_rewards.copy()\n",
    "    plotChart(\"UCB Action Selection\", rwds, num_of_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0218c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "runUCB(1000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a1f36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "runUCB(1000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ded913f",
   "metadata": {},
   "source": [
    "<br>\n",
    "### 3. Gradient Bandit Algorithms\n",
    "<br>\n",
    "#### References\n",
    "1. __[datahubbs.com/multi_armed_bandits_part2](https://www.datahubbs.com/multi-armed-bandits-reinforcement-learning-2)__\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6888d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBanditAlgorithms(EgreedyBanditMethod):\n",
    "    def __init__(self, k=10, eps=0, steps=1000, alpha=0.4):\n",
    "        EgreedyBanditMethod.__init__(self, k, eps)\n",
    "        self.actions = np.arange(k)\n",
    "        self.H = np.zeros(k) # preferences\n",
    "        self.alpha = alpha # learning rate\n",
    "        \n",
    "    def softmax(self):\n",
    "        self.prob_action = np.exp(self.H - np.max(self.H)) / np.sum(np.exp(self.H - np.max(self.H)), axis=0)\n",
    "        \n",
    "    def getAction(self):\n",
    "        self.softmax() # Update probabilities\n",
    "        action = np.random.choice(self.actions, p=self.prob_action) # Select highest preference action\n",
    "        return action\n",
    "    \n",
    "    def updateModel(self, action, reward):\n",
    "        m_r = EgreedyBanditMethod.updateModel(self, action, reward)\n",
    "        self.H[action] = self.H[action] + self.alpha * (reward - self.m_r) * (1 - self.prob_action[action]) # update preferences\n",
    "        actions_not_taken = self.actions!=action\n",
    "        self.H[actions_not_taken] = self.H[actions_not_taken] - self.alpha * (reward - self.m_r) * self.prob_action[actions_not_taken]\n",
    "        return m_r\n",
    "    \n",
    "    def reset(self):\n",
    "        EgreedyBanditMethod.reset(self)\n",
    "        self.H = np.zeros(self.k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runGBA(num_of_episodes = 1000, num_of_steps = 1000):\n",
    "    gbas = {\n",
    "        \"$\\\\alpha=0.1$\" : GradientBanditAlgorithms(alpha=0.1),\n",
    "        \"$\\\\alpha=0.4$\" : GradientBanditAlgorithms(alpha=0.4),\n",
    "        \"$\\\\alpha=0.8$\" : GradientBanditAlgorithms(alpha=0.8)\n",
    "    }\n",
    "    rwds = {}\n",
    "    for key, gba in gbas.items():\n",
    "        average_rewards = np.zeros(num_of_steps)\n",
    "        for episode in range(num_of_episodes):\n",
    "            gba.reset()\n",
    "            rewards = np.zeros(num_of_steps)\n",
    "            for step in range(num_of_steps):\n",
    "                action = gba.getAction()\n",
    "                reward = gba.getReward(action)\n",
    "                mean_reward = gba.updateModel(action, reward)\n",
    "                debug(action, reward, mean_reward)\n",
    "                rewards[step] = mean_reward\n",
    "            average_rewards = average_rewards + (rewards - average_rewards) / (episode + 1)\n",
    "        rwds[key] = average_rewards.copy()\n",
    "    plotChart(\"GBA Action Selection\", rwds, num_of_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7c3707",
   "metadata": {},
   "outputs": [],
   "source": [
    "runGBA(1000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3a10d7",
   "metadata": {},
   "source": [
    "<br>\n",
    "### 4. Comparison Various Bandit Algorithms\n",
    "1. e-greedy bandit method\n",
    "2. upper confidence bound method\n",
    "3. grdient bandit method\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08802320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAll(num_of_episodes = 1000, num_of_steps = 1000):\n",
    "    methods = {\n",
    "        \"$\\\\alpha=1 (Upper Confidence)$\" : UpperConfidenceBoundActionSelection(c=1),\n",
    "        \"$\\\\epsilon=0.1 (Epsilon Greedy)$\" : EgreedyBanditMethod(eps=0.1), \n",
    "        \"$\\\\alpha=0.4 (Gradient Bandit)$\" : GradientBanditAlgorithms(alpha=0.4)\n",
    "    }\n",
    "    rwds = {}\n",
    "    for key, method in methods.items():\n",
    "        average_rewards = np.zeros(num_of_steps)\n",
    "        for episode in range(num_of_episodes):\n",
    "            method.reset()\n",
    "            rewards = np.zeros(num_of_steps)\n",
    "            for step in range(num_of_steps):\n",
    "                action = method.getAction()\n",
    "                reward = method.getReward(action)\n",
    "                mean_reward = method.updateModel(action, reward)\n",
    "                debug(action, reward, mean_reward)\n",
    "                rewards[step] = mean_reward\n",
    "            average_rewards = average_rewards + (rewards - average_rewards) / (episode + 1)\n",
    "        rwds[key] = average_rewards.copy()\n",
    "    plotChart(\"$\\epsilon$-Greedy, UCB and GBA methods\", rwds, num_of_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eada45",
   "metadata": {},
   "outputs": [],
   "source": [
    "runAll(1000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77648cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "runAll(1000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa13d3e",
   "metadata": {},
   "source": [
    "##### 더 해보고 싶은 것들\n",
    "1. 알고리즘 혹은 메소드 별로 exploration vs exploitation 수행 빈도의 balance를 표로 나타내면 어떨까?\n",
    "2. runXXX 함수들도 공통되는 부분을 제거하고 main 함수에서 methods 들만 넘겨주는 방식으로 리팩토링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4122cef8",
   "metadata": {},
   "source": [
    "### 5. Testing Softmax function and etc\n",
    "#### References\n",
    "1. [draw_bandits@yoonforrh](http://localhost:8888/notebooks/reinforcement2e/chapter2/chapter2_yoonforh.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b50343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def draw_bandits(q_stars, r_dist) :\n",
    "    k = len(q_stars)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    # plot violin plot\n",
    "    ax.violinplot(r_dist,\n",
    "                showmeans=False,\n",
    "                showmedians=True,\n",
    "                showextrema=False,\n",
    "                widths=0.3)\n",
    "\n",
    "    # ax.set_title('Figure 2.1')\n",
    "    ax.set_xticks([t + 1 for t in range(k)])\n",
    "    ax.set_xticklabels([t + 1 for t in range(k)])\n",
    "    for t in range(k) :\n",
    "        ax.text(t + 1, q_stars[t], 'q*(' + str(t + 1) + ')')\n",
    "\n",
    "    ax.set_xlabel('Action')\n",
    "    ax.set_ylabel('Reward distribution')\n",
    "\n",
    "    ax.plot([0.5, 10.5], [0, 0], linestyle='--', linewidth=1)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbfbfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "np.random.seed(9999) # Fixing random state for reproducibility\n",
    "k = 10\n",
    "q_stars = [ np.random.normal() for a in range(k) ]\n",
    "r_dist = [np.random.normal(q_stars[t], 1.0, 2000) for t in range(k)]\n",
    "# save generated bandit\n",
    "pk.dump((q_stars, r_dist), open('10-armed.p', \"wb\" ))\n",
    "draw_bandits(H, prob_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5a0b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "H = np.random.normal(0, 1, k)\n",
    "prob_action = np.exp(H - np.max(H)) / np.sum(np.exp(H - np.max(H)), axis=0)\n",
    "print(np.random.normal())\n",
    "print(H)\n",
    "print(prob_action)\n",
    "print(sum(prob_action))\n",
    "\n",
    "action = 0\n",
    "reward = 1.0\n",
    "alpha = 1.0\n",
    "m_r = 0\n",
    "actions = np.arange(k)\n",
    "actions_not_taken = actions!=action\n",
    "H[actions_not_taken] = H[actions_not_taken] - alpha * (reward - m_r) * prob_action[actions_not_taken]\n",
    "print(actions_not_taken)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
