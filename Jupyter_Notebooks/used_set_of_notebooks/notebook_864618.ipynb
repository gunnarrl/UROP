{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca229dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from fastai.basics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf24c4",
   "metadata": {},
   "source": [
    "In this part of the lecture we explain Stochastic Gradient Descent (SGD) which is an **optimization** method commonly used in neural networks. We will illustrate the concepts with concrete examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c37e006",
   "metadata": {},
   "source": [
    "#  Linear Regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2ed766",
   "metadata": {},
   "source": [
    "The goal of linear regression is to fit a line to a set of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e634508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b266bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(n,2) \n",
    "x[:,0].uniform_(-1.,1)\n",
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345314a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tensor(3.,2); a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a09d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x@a + torch.rand(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8937a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[:,0], y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef1b0a1",
   "metadata": {},
   "source": [
    "You want to find **parameters** (weights) `a` such that you minimize the *error* between the points and the line `x@a`. Note that here `a` is unknown. For a regression problem the most common *error function* or *loss function* is the **mean squared error**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a43869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_hat, y): return ((y_hat-y)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1e3b9b",
   "metadata": {},
   "source": [
    "Suppose we believe `a = (-1.0,1.0)` then we can compute `y_hat` which is our *prediction* and then compute our error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0517396",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tensor(-1.,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6d834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = x@a\n",
    "mse(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc3da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[:,0],y)\n",
    "plt.scatter(x[:,0],y_hat);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5471c31",
   "metadata": {},
   "source": [
    "So far we have specified the *model* (linear regression) and the *evaluation criteria* (or *loss function*). Now we need to handle *optimization*; that is, how do we find the best values for `a`? How do we find the best *fitting* linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa652b11",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faad1a7",
   "metadata": {},
   "source": [
    "We would like to find the values of `a` that minimize `mse_loss`.\n",
    "\n",
    "**Gradient descent** is an algorithm that minimizes functions. Given a function defined by a set of parameters, gradient descent starts with an initial set of parameter values and iteratively moves toward a set of parameter values that minimize the function. This iterative minimization is achieved by taking steps in the negative direction of the function gradient.\n",
    "\n",
    "Here is gradient descent implemented in [PyTorch](http://pytorch.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6a67c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Parameter(a); a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba83058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update():\n",
    "    y_hat = x@a\n",
    "    loss = mse(y, y_hat)\n",
    "    if t % 10 == 0: print(loss)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        a.sub_(lr * a.grad)\n",
    "        a.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11654d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-1\n",
    "for t in range(100): update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdfd769",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[:,0],y)\n",
    "plt.scatter(x[:,0],x@a);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a4dbd0",
   "metadata": {},
   "source": [
    "## Animate it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02deefbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation, rc\n",
    "rc('animation', html='jshtml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6303a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Parameter(tensor(-1.,1))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(x[:,0], y, c='orange')\n",
    "line, = plt.plot(x[:,0], x@a)\n",
    "plt.close()\n",
    "\n",
    "def animate(i):\n",
    "    update()\n",
    "    line.set_ydata(x@a)\n",
    "    return line,\n",
    "\n",
    "animation.FuncAnimation(fig, animate, np.arange(0, 100), interval=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd2edc",
   "metadata": {},
   "source": [
    "In practice, we don't calculate on the whole file at once, but we use *mini-batches*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee59a0b",
   "metadata": {},
   "source": [
    "## Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066b61e7",
   "metadata": {},
   "source": [
    "- Learning rate\n",
    "- Epoch\n",
    "- Minibatch\n",
    "- SGD\n",
    "- Model / Architecture\n",
    "- Parameters\n",
    "- Loss function\n",
    "\n",
    "For classification problems, we use *cross entropy loss*, also known as *negative log likelihood loss*. This penalizes incorrect confident predictions, and correct unconfident predictions."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
