{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fbb42c7",
   "metadata": {},
   "source": [
    "# Independent Component Analysis (ICA) implementation from scratch\n",
    "\n",
    "This is the Python Jupyter Notebook for the Medium article about implementing the fast independent Component Analysis (ICA) algorithm.\n",
    "\n",
    "ICA is an efficient technique to decompose linear mixtures of signals into their underlying independent components. Classical examples of where this method is used are noise reduction in images, artifact removal from time series data or identification of driving components in financial data.\n",
    "\n",
    "Here we will start by first importing the necessary libraries and creating some toy signals which we will use to develop and test our ICA implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eacba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable plots inside the Jupyter NotebookLet the\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ee4b9e",
   "metadata": {},
   "source": [
    "### The generative model of ICA\n",
    "\n",
    "The ICA is based on a generative model. This means that it assumes an underlying process that generates the observed data. The ICA model is simple, it assumes that some independent source signals *s* are linear combined by a mixing matrix _A_.\n",
    "\n",
    "<h3 align=\"center\">$x=As$</h3>\n",
    "\n",
    "### Retrieving the components\n",
    "\n",
    "The above equations implies that if we invert *A* and multiply it with the observed signals _x_ we will retrieve our sources:\n",
    "\n",
    "<h3 align=\"center\">$W=A^{-1}$</h3>\n",
    "\n",
    "<h3 align=\"center\">$s=xW$</h3>\n",
    "\n",
    "This means that what our ICA algorithm needs to estimate is *W*. \n",
    "\n",
    "### Create toy signals\n",
    "\n",
    "We will start by creating some independent signals that will be mixed by matrix A. The independent sources signals are **(1)** a sine wave, **(2)** a saw tooth signal and **(3)** a random noise vector. After calculating their dot product with _A_ we get three linear combinations of these source signals.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ad3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for the random number generator for reproducibility\n",
    "np.random.seed(23)\n",
    "\n",
    "# Number of samples\n",
    "ns = np.linspace(0, 200, 1000)\n",
    "\n",
    "# Source matrix\n",
    "S = np.array([np.sin(ns * 1), \n",
    "              signal.sawtooth(ns * 1.9),\n",
    "              np.random.random(len(ns))]).T\n",
    "\n",
    "# Mixing matrix\n",
    "A = np.array([[0.5, 1, 0.2], \n",
    "              [1, 0.5, 0.4], \n",
    "              [0.5, 0.8, 1]])\n",
    "\n",
    "# Mixed signal matrix\n",
    "X = S.dot(A).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f45d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sources & signals\n",
    "fig, ax = plt.subplots(1, 1, figsize=[18, 5])\n",
    "ax.plot(ns, S, lw=5)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([-1, 1])\n",
    "ax.set_xlim(ns[0], ns[200])\n",
    "ax.tick_params(labelsize=12)\n",
    "ax.set_title('Independent sources', fontsize=25)\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize=[18, 5], sharex=True)\n",
    "ax[0].plot(ns, X[0], lw=5)\n",
    "ax[0].set_title('Mixed signals', fontsize=25)\n",
    "ax[0].tick_params(labelsize=12)\n",
    "\n",
    "ax[1].plot(ns, X[1], lw=5)\n",
    "ax[1].tick_params(labelsize=12)\n",
    "ax[1].set_xlim(ns[0], ns[-1])\n",
    "\n",
    "ax[2].plot(ns, X[2], lw=5)\n",
    "ax[2].tick_params(labelsize=12)\n",
    "ax[2].set_xlim(ns[0], ns[-1])\n",
    "ax[2].set_xlabel('Sample number', fontsize=20)\n",
    "ax[2].set_xlim(ns[0], ns[200])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04936244",
   "metadata": {},
   "source": [
    "### Interlude: Comparison of Gaussian vs. Non-Gaussian signals\n",
    "\n",
    "In the above it was already mentioned but for the ICA to work the observed signals x need to be a **linear** combination of **independent** components. The linearity follows from the generative model, which is linear. Independence means that signal x1 does not contain any information about signal x2. From this it follows that both signals are not correlated and have a covariance of 0. However just because two signals are not correlated it does not automatically mean that they are independent. \n",
    "\n",
    "A third precondition that needs to be met is **non-Gaussiananity** of the independent source signals. Why is that? The joint density distribution of two independent non-Gaussian signals will be uniform on a square. Mixing these two signals with an orthogonal matrix will result in two signals that are now not independent anymore and have a uniform distribution on a parallelogram. Which means that if we are at the minimum or maximum value of one of our mixed signals we know the value of the other signal. Therefore they are not independent anymore. \n",
    "Doing the same with two Gaussian signals will result in something else. The joint distribution of the source signals is completely symmetric and so is the joint distribution of the mixed signals. Therefore it does not contain any information about the mixing matrix, the inverse of which we want to calculate. It follows that in this case the ICA algorithm will fail.\n",
    "\n",
    "The code below illustrates these differences between Gaussian and non-Gaussian signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90573349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two non-gaussian uniform components\n",
    "s1 = np.random.rand(1000)\n",
    "s2 = np.random.rand(1000)\n",
    "s = np.array(([s1, s2]))\n",
    "\n",
    "# Define two gaussian components\n",
    "s1n = np.random.normal(size=1000)\n",
    "s2n = np.random.normal(size=1000)\n",
    "sn = np.array(([s1n, s2n]))\n",
    "\n",
    "# Define orthogonal mixing matrix\n",
    "A = np.array(([0.96, -0.28],[0.28, 0.96]))\n",
    "\n",
    "# Mix signals\n",
    "mixedSignals = s.T.dot(A)\n",
    "mixedSignalsN = sn.T.dot(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbf09bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig, ax = plt.subplots(2, 2, figsize=[18, 10])\n",
    "ax[0][0].scatter(s[0], s[1])\n",
    "ax[0][0].tick_params(labelsize=12)\n",
    "ax[0][0].set_title('Sources (non-Gaussian)', fontsize=25)\n",
    "ax[0][0].set_xlim([-0.25, 1.5])\n",
    "ax[0][0].set_xticks([])\n",
    "\n",
    "ax[0][1].scatter(sn[0], sn[1])\n",
    "ax[0][1].tick_params(labelsize=12)\n",
    "ax[0][1].set_title('Sources (Gaussian)', fontsize=25)\n",
    "ax[0][1].set_xlim([-4, 4])\n",
    "ax[0][1].set_xticks([])\n",
    "ax[0][1].set_yticks([])\n",
    "\n",
    "ax[1][0].scatter(mixedSignals.T[0], mixedSignals.T[1])\n",
    "ax[1][0].tick_params(labelsize=12)\n",
    "ax[1][0].set_title('Mixed signals (non-Gaussian sources)', fontsize=25)\n",
    "ax[1][0].set_xlim([-0.25, 1.5])\n",
    "\n",
    "ax[1][1].scatter(mixedSignalsN.T[0], mixedSignalsN.T[1])\n",
    "ax[1][1].tick_params(labelsize=12)\n",
    "ax[1][1].set_title('Mixed signals (Gaussian sources)', fontsize=25)\n",
    "ax[1][1].set_xlim([-4, 4])\n",
    "ax[1][1].set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e6d222",
   "metadata": {},
   "source": [
    "### Visualize properties of toy signals\n",
    "\n",
    "To check if the properties discussed above also apply for our toy signals we will plot them accordingly.\n",
    "As expected the source signals are distributed on a square for non-Gaussian random variables. Likewise the mixed signals form a parallelogram in the right plot of Figure 3 which shows that the mixed signals are not independent anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d687be1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=[18, 5])\n",
    "ax[0].scatter(S.T[0], S.T[1], c=S.T[2])\n",
    "ax[0].tick_params(labelsize=12)\n",
    "ax[0].set_yticks([-1, 0, 1])\n",
    "ax[0].set_xticks([-1, 0, 1])\n",
    "ax[0].set_xlabel('signal 1', fontsize=20)\n",
    "ax[0].set_ylabel('signal 2', fontsize=20)\n",
    "ax[0].set_title('Sources', fontsize=25)\n",
    "\n",
    "ax[1].scatter(X[0], X[1], c=X[2])\n",
    "ax[1].tick_params(labelsize=12)\n",
    "ax[1].set_yticks([-1.5, 0, 1.5])\n",
    "ax[1].set_xticks([-1.5, 0, 1.5])\n",
    "ax[1].set_xlabel('signal 1', fontsize=20)\n",
    "ax[1].set_title('Mixed signals', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c420f765",
   "metadata": {},
   "source": [
    "## Preprocessing functions\n",
    "\n",
    "To get an optimal estimate of the independent components it is advisable to do some pre-processing of the data. In the following the two most important pre-processing techniques are explained in more detail.\n",
    "\n",
    "The first pre-processing step we will discuss here is **centering**. This is a simple subtraction of the mean from our input X. As a result the centered mixed signals will have zero mean which implies that also our source signals s are of zero mean. This simplifies the ICA calculation and the mean can later be added back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be26051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center(x):\n",
    "    mean = np.mean(x, axis=1, keepdims=True)\n",
    "    centered =  x - mean \n",
    "    return centered, mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c36d86",
   "metadata": {},
   "source": [
    "For the second pre-processing technique we need to calculate the covariance. So lets quickly define it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0819b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance(x):\n",
    "    mean = np.mean(x, axis=1, keepdims=True)\n",
    "    n = np.shape(x)[1] - 1\n",
    "    m = x - mean\n",
    "\n",
    "    return (m.dot(m.T))/n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5d96b4",
   "metadata": {},
   "source": [
    "The second pre-processing method is called **whitening**. The goal here is to linearly transform the observed signals X in a way that potential correlations between the signals are removed and their variances equal unity. As a result the covariance matrix of the whitened signals will be equal to the identity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c5b484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def whiten(x):\n",
    "    # Calculate the covariance matrix\n",
    "    coVarM = covariance(X) \n",
    "    \n",
    "    # Single value decoposition\n",
    "    U, S, V = np.linalg.svd(coVarM)\n",
    "    \n",
    "    # Calculate diagonal matrix of eigenvalues\n",
    "    d = np.diag(1.0 / np.sqrt(S)) \n",
    "    \n",
    "    # Calculate whitening matrix\n",
    "    whiteM = np.dot(U, np.dot(d, U.T))\n",
    "    \n",
    "    # Project onto whitening matrix\n",
    "    Xw = np.dot(whiteM, X) \n",
    "    \n",
    "    return Xw, whiteM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e34f922",
   "metadata": {},
   "source": [
    "## Implement the fast ICA algorithm\n",
    "\n",
    "Now it is time to look at the actual ICA algorithm. As discussed above one precondition for the ICA algorithm to work is that the source signals are non-Gaussian. Therefore the result of the ICA should return sources that are as non-Gaussian as possible. To achieve this we need a measure of Gaussianity. One way is Kurtosis and it could be used here but another way has proven more efficient. Nevertheless we will have a look at kurtosis at the end of this notebook. \n",
    "For the actual algorithm however we will use the equations $g$ and $g'$ which are derivatives of $f(u)$ as defined below.\n",
    "\n",
    "<h3 align=\"center\">$f(u)=\\log \\cosh(u)$</h3> \n",
    "\n",
    "<h3 align=\"center\">$g(u)=\\tanh(u)$</h3> \n",
    "\n",
    "<h3 align=\"center\">$g'(u)=1-\\tanh^{2}(u)$</h3> \n",
    "\n",
    "These equations allow an approximation of negentropy and will be used in the below ICA algorithm which is [based on a fixed-point iteration scheme](https://homepage.math.uiowa.edu/~whan/072.d/S3-4.pdf):\n",
    "\n",
    "<h3>$for\\ 1\\ to\\ number\\ of\\ components\\ c:$</h3>\n",
    "\n",
    "<h3>$\\ \\ \\ \\ \\ \\ \\ \\ w_{p} \\equiv random\\ initialisation$</h3>\n",
    "\n",
    "<h3>$\\ \\ \\ \\ \\ \\ \\ \\ while\\ w_{p}\\ not\\ < threshold:$</h3>\n",
    "\n",
    "<h3>$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ w_{p} \\equiv \\frac{1}{n}(Xg(W^{T}X) - g'(W^{T}X)W)$</h3>\n",
    "\n",
    "<h3>$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ w_{p} \\equiv w_{p}-\\sum_{j=1}^{p-1}(w_{p}^{T}w_{j})w_{j}$</h3>\n",
    "\n",
    "<h3>$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ w_{p} \\equiv w_{p}/||w_{p}||$</h3>\n",
    "\n",
    "<h3>$\\ \\ \\ \\ \\ \\ \\ \\ W \\equiv [w_{1},...,w_{c}]$</h3>\n",
    "\n",
    "\n",
    "So according to the above what we have to do is to take a random guess for the weights of each component. The dot product of the random weights and the mixed signals is passed into the two functions g and g'. We then subtract the result of g' from g and calculate the mean. The result is our new weights vector. Next we could directly divide the new weights vector by its norm and repeat the above until the weights do not change anymore. There would be nothing wrong with that. However the problem we are facing here is that in the iteration for the second component we might identify the same component as in the first iteration. To solve this problem we have to decorrelate the new weights from the previously identified weights. This is what is happening in the step between updating the weights and dividing by their norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fc5a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastIca(signals,  alpha = 1, thresh=1e-8, iterations=5000):\n",
    "    m, n = signals.shape\n",
    "\n",
    "    # Initialize random weights\n",
    "    W = np.random.rand(m, m)\n",
    "\n",
    "    for c in range(m):\n",
    "            w = W[c, :].copy().reshape(m, 1)\n",
    "            w = w / np.sqrt((w ** 2).sum())\n",
    "\n",
    "            i = 0\n",
    "            lim = 100\n",
    "            while ((lim > thresh) & (i < iterations)):\n",
    "\n",
    "                # Dot product of weight and signal\n",
    "                ws = np.dot(w.T, signals)\n",
    "\n",
    "                # Pass w*s into contrast function g\n",
    "                wg = np.tanh(ws * alpha).T\n",
    "\n",
    "                # Pass w*s into g prime \n",
    "                wg_ = (1 - np.square(np.tanh(ws))) * alpha\n",
    "\n",
    "                # Update weights\n",
    "                wNew = (signals * wg.T).mean(axis=1) - wg_.mean() * w.squeeze()\n",
    "\n",
    "                # Decorrelate weights              \n",
    "                wNew = wNew - np.dot(np.dot(wNew, W[:c].T), W[:c])\n",
    "                wNew = wNew / np.sqrt((wNew ** 2).sum())\n",
    "\n",
    "                # Calculate limit condition\n",
    "                lim = np.abs(np.abs((wNew * w).sum()) - 1)\n",
    "                \n",
    "                # Update weights\n",
    "                w = wNew\n",
    "                \n",
    "                # Update counter\n",
    "                i += 1\n",
    "\n",
    "            W[c, :] = w.T\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1417b00",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "So... before we run the ICA we need to do the pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17648479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center signals\n",
    "Xc, meanX = center(X)\n",
    "\n",
    "# Whiten mixed signals\n",
    "Xw, whiteM = whiten(Xc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f816b1",
   "metadata": {},
   "source": [
    "### Check whitening\n",
    "\n",
    "Above we mentioned that the covariance matrix of the whitened signal should equal the identity matrix:\n",
    "\n",
    "<h3 align=\"center\">$covariance(\\check{X})=I$</h3>\n",
    "\n",
    "...and as we can see below this is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9cb6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if covariance of whitened matrix equals identity matrix\n",
    "print(np.round(covariance(Xw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1926c12c",
   "metadata": {},
   "source": [
    "### Running the ICA\n",
    "\n",
    "Finally... it is time to feed the whitened signals into the ICA algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f606161",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = fastIca(Xw,  alpha=1)\n",
    "\n",
    "#Un-mix signals using \n",
    "unMixed = Xw.T.dot(W.T)\n",
    "\n",
    "# Subtract mean\n",
    "unMixed = (unMixed.T - meanX).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebffc6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot input signals (not mixed)\n",
    "fig, ax = plt.subplots(1, 1, figsize=[18, 5])\n",
    "ax.plot(S, lw=5)\n",
    "ax.tick_params(labelsize=12)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([-1, 1])\n",
    "ax.set_title('Source signals', fontsize=25)\n",
    "ax.set_xlim(0, 100)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=[18, 5])\n",
    "ax.plot(unMixed, '--', label='Recovered signals', lw=5)\n",
    "ax.set_xlabel('Sample number', fontsize=20)\n",
    "ax.set_title('Recovered signals', fontsize=25)\n",
    "ax.set_xlim(0, 100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ec6c80",
   "metadata": {},
   "source": [
    "The result of the ICA are plotted above, and the result looks very good. We got all three sources back!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c471b8",
   "metadata": {},
   "source": [
    "## Kurtosis\n",
    "\n",
    "So finally lets check one last thing: The kurtosis of the signals. \n",
    "Kurtosis is the fourth moment of the data and measures the \"tailedness\" of a distribution. A normal distribution has a value of 3, a uniform distribution like the one we used in our toy data has a kurtosis < 3. The implementation in Python is straight forward as can be seen from the code below which also calculates the other moments of the data. The first moment is the mean, the second is the variance, the third is the skewness and the fourth is the kurtosis. Here 3 is subtracted from the fourth moment so that a normal distribution has a kurtosis of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a153ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Kurtosis\n",
    "\n",
    "def kurt(x):\n",
    "    n = np.shape(x)[0]\n",
    "    mean = np.sum((x**1)/n) # Calculate the mean\n",
    "    var = np.sum((x-mean)**2)/n # Calculate the variance\n",
    "    skew = np.sum((x-mean)**3)/n # Calculate the skewness\n",
    "    kurt = np.sum((x-mean)**4)/n # Calculate the kurtosis\n",
    "    kurt = kurt/(var**2)-3\n",
    "\n",
    "    return kurt, skew, var, mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d4ae2a",
   "metadata": {},
   "source": [
    "As we can see in the following all of our mixed signals have a kurtosis of ≤ 1 whereas all recovered independent components have a kurtosis of 1.5 which means they are less Gaussian than their sources. This has to be the case since the ICA tries to maximize non-Gaussianity. Also it nicely illustrates the fact mentioned above that the mixture of non-Gaussian signals will be more Gaussian than the sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e1b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10))\n",
    "for i in range(X.shape[0]):\n",
    "    \n",
    "    sns.kdeplot(X[i, :], lw=5, label='Mixed Kurtosis={}'.format(np.round(kurt(X[i, :])[0], decimals=1)))\n",
    "    \n",
    "for i in range(X.shape[0]):   \n",
    "    sns.kdeplot(unMixed[i, :], lw=5, ls='--', label='unMixed Kurtosis={}'.format(np.around(kurt(unMixed[i, :])[0], decimals=1)))\n",
    "\n",
    "ax.tick_params(labelsize=12)\n",
    "ax.set_xlabel('value', fontsize=20)\n",
    "ax.set_ylabel('count', fontsize=20)\n",
    "ax.set_title('KDE plot of ', fontsize=25)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
