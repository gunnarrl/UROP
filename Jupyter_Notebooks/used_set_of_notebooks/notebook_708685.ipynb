{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7da0e08c",
   "metadata": {},
   "source": [
    "# Simple data analysis with Apache Spark\n",
    "In this example we are going to use Apache Spark to perform distributed analysis on a CSV generated by our Python scrapping class. The goal of this program is mainly to clean data for further analysis down the line. At the end, however we will also show some MLlib alogrithms as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e92ef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do an initial test of Spark to make sure it works.\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "# do something to prove it works\n",
    "rdd = sc.parallelize(range(1000))\n",
    "rdd.takeSample(False, 5)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e08de93",
   "metadata": {},
   "source": [
    "Now that we have checked that PySpark is up and running let's start to do some processing using a csv file generated previously using fb_post.py (for instructions on using fb_scrapper please see the readme file). First we are going to read the CSV file into a data frame and filter out stop words and punctation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce65e29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import regexp_replace, trim, col, lower \n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "#Comment and uncomment the following line as necessary\n",
    "#sc = pyspark.SparkContext('local[*]')\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('160558090672531_facebook_statuses.csv')\n",
    "df.show()\n",
    "# Drop the null crap \n",
    "df = df.na.drop(subset=[\"status_message\"])\n",
    "# Remove punctation from status messages\n",
    "df2 = df.select(regexp_replace(\"status_message\", \"\\p{Punct}\", \"\").alias(\"status_message\"), \"status_id\")\n",
    "df2.show()\n",
    "messages = df2\n",
    "# Tokenize and remove stop words\n",
    "tokenizer = Tokenizer(inputCol=\"status_message\", outputCol=\"filtered\")\n",
    "filterw = tokenizer.transform(messages)\n",
    "filterw.show()\n",
    "remover = StopWordsRemover(inputCol=\"filtered\", outputCol=\"filtered1\")\n",
    "filtered_final = remover.transform(filterw)\n",
    "filtered_final.show()\n",
    "messages = filtered_final.select(\"filtered1\")\n",
    "messages2 = filtered_final.select(\"status_id\",\"status_message\").rdd\n",
    "# We will use this to create our inverted index later in the file\n",
    "doc_index = messages2.collectAsMap()\n",
    "#Convert to RDD\n",
    "message_rdd=messages.rdd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8fa142",
   "metadata": {},
   "source": [
    "Now we are going to do a simple word count with the rdd that we just created and generate a graph of the top words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f3eb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCount with RDD. Normally this would be used in conjunction with NLP to extract trending topics.\n",
    "from operator import add\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def plot_histogram(indexes, values):\n",
    "    indexes2 = np.arange(len(indexes))\n",
    "    values = np.asarray(values)\n",
    "    plt.bar(indexes2, values)\n",
    "    bar_width = 2\n",
    "    plt.xticks(indexes2 + bar_width*.2, indexes)\n",
    "    plt.show()\n",
    "# Prints the WordCount of words that appear more than 25 times and appends them to indexes and values.\n",
    "def print_word_count(output, indexes, values):\n",
    "    for (word, count) in output:\n",
    "        if count > 25 and word is not \"\":\n",
    "            # append to indexes\n",
    "            indexes.append(word)\n",
    "            values.append(count)\n",
    "            print(\"%s: %i\" % (word, count))\n",
    "    \n",
    "    \n",
    "\n",
    "statuses = message_rdd.flatMap(lambda x: x)\n",
    "words = statuses.flatMap(lambda x: x)\n",
    "#se = statuses.flatMap(String)\n",
    "counts = words.flatMap(lambda x: x.split(' ')) \\\n",
    "                  .map(lambda x: (x, 1)) \\\n",
    "                  .reduceByKey(add)               \n",
    "\n",
    "\n",
    "output1 = counts.sortByKey().collectAsMap()\n",
    "wordsMap = counts.collectAsMap()\n",
    "#ouput1.saveAsTextFile(\"keys.txt\")\n",
    "# Get just the most popular words \n",
    "output = counts.takeOrdered(9, key = lambda x: -x[1])\n",
    "indexes = []\n",
    "values = []\n",
    "print(\"The top words are:\")\n",
    "print_word_count(output, indexes, values)\n",
    "plot_histogram(indexes,values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb2b2b",
   "metadata": {},
   "source": [
    "### Create a document index and an inverted index\n",
    "Now that we have done some simple procesing lets get into the meat of this notebook. We are going to create an inverted index using MR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877cf985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_document_index(raw_status):\n",
    "    status = raw_status.flatMap(lambda x : x)\n",
    "    doc_index = status.zipWithIndex()\n",
    "    doc_index = doc_index.map(lambda y: (y[1], y[0])).collectAsMap()\n",
    "    return doc_index\n",
    "def inverted_index(documents):\n",
    "    #Invert to (status_id, terms[]) and then flatMapValues in order to get (status_id, term)\n",
    "    documents = documents.map(lambda x: (x[1],x[0])).flatMapValues(lambda x: x)\n",
    "    #Undo to (term, status_id) and reduce to get (term, status_ids[])\n",
    "    documents = documents.map(lambda x: (x[1],[x[0]])).reduceByKey(lambda a,b: a+b)\n",
    "    print(\"inverted index sucessfully created\")\n",
    "    return documents \n",
    "messages = filtered_final.select(\"filtered1\",\"status_id\").rdd\n",
    "status_inverted_index = inverted_index(messages)\n",
    "inverted_map = status_inverted_index.collectAsMap() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0145a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file\n",
    "def get_word_count(some_dict, aRDD):\n",
    "    return aRDD.filter(lambda x: x in list(some_dict.keys())).map(lambda key: (key, some_dict[key]))\n",
    "names = sc.textFile(\"rivers.txt\")\n",
    "riverNames = names.flatMap(lambda x: x.split(\"\\n\"))\n",
    "r = get_word_count(wordsMap, riverNames)\n",
    "#r = riverNames.filter(lambda x : x in list(wordsMap.keys())).map(lambda key: (key, wordsMap[key]))\n",
    "r.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b292be",
   "metadata": {},
   "source": [
    "We will now be doing some interactive analysis with iPython widgets. (Note this will not necessarily render in GitHub, but if you download the notebook you should be able to see it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d023dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to make things a little more interactive\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "value_search = widgets.Text() \n",
    "display(value_search)\n",
    "value_search2 = widgets.Text() \n",
    "display(value_search2)\n",
    "\n",
    "def find_key(sender):\n",
    "    text = value_search.value\n",
    "    if text in output1: \n",
    "        print(text + \" occurs \" + str(output1[text]) + \" times in the file\")\n",
    "# Todo clean up sloppy method\n",
    "def mini_search(sender):\n",
    "    text = value_search2.value.strip(\" \")\n",
    "    if text in inverted_map:\n",
    "        l = inverted_map[text]\n",
    "        for i in l:\n",
    "            if i in doc_index:\n",
    "                l = i \n",
    "                print(doc_index[i])\n",
    "value_search.on_submit(find_key)\n",
    "value_search2.on_submit(mini_search)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14882376",
   "metadata": {},
   "source": [
    "Now we are going to generate a WordCloud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd0daae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wordcloud example \n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "def makeWordCloud(words):\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.add(\"thank\")\n",
    "    wordcloud = WordCloud(stopwords=stopwords, background_color='black').generate(\" \".join(wordlist))\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "wordlist = output1.keys()\n",
    "makeWordCloud(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a004d4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to extract known names of rivers and get useful URLS \n",
    "df2 = df.na.drop(subset=[\"status_link\"])\n",
    "df3 = df2.select(\"status_link\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76136f4",
   "metadata": {},
   "source": [
    "Okay so now we want to get the images and or videos. Uses for the images might include image search for our database (with the river extracted using NLP) or computer vision applications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5cdd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's try to use these urls to get their the respective images or videos \n",
    "from pyspark.sql import SparkSession\n",
    "from lxml import html\n",
    "import requests\n",
    "from IPython.display import Image\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "df2.createOrReplaceTempView(\"posts\")\n",
    "sqlDF = spark.sql(\"SELECT * FROM posts WHERE status_link LIKE '%photo%'\")\n",
    "sqlDF.show()\n",
    "statusRDD = sqlDF.select('status_link').rdd\n",
    "urls = statusRDD.flatMap(lambda x: x)\n",
    "print(urls.take(3))\n",
    "#Now let's save the images possibly for a Computer Vision application\n",
    "urls2 = urls.collect()\n",
    "\n",
    "#for url in urls2:\n",
    "    #page = requests.get(url)\n",
    "    #tree = html.fromstring(page.content)\n",
    "    #imageUrl = tree.xpath('//img[@class=\"spotlight\"]/@src')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df90900b",
   "metadata": {},
   "source": [
    "## Latent Dirichlet allocation\n",
    "We are now going to construct a topic model using an algorithm called LDA. You can find out more information on LDA <a href=\"https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\">here.</a> Our real goal is to get a set of topics for each document. These topic can them used to classify documents into sub-categories or themes. This will then be used in our search engine later on or for other analytics purposes. Unfortunately, at this point the PySpark LDA model is not all that useful for data analysis purposes but we will keep it around in the code anyways. (Note if anyone knows an easy way of visualizing a PySpark LDA model with pyLDAvis let me know!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0396ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section remains buggy and prone to jams run at your own risk!\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.feature import IDF\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "#vectorize tags array for each user\n",
    "vectorizer = CountVectorizer(inputCol=\"filtered1\", outputCol=\"features\").fit(filtered_final)\n",
    "countVectors = vectorizer.transform(filtered_final).select(\"status_id\", \"features\")\n",
    "countVectors.show()\n",
    "#find TF-IDF coefficients for each tag\n",
    "print(\"begin\")\n",
    "frequencyVectors = countVectors.rdd.map(lambda vector: vector[1])\n",
    "frequencyDenseVectors = frequencyVectors.map(lambda vector: Vectors.dense(vector))\n",
    "idf = IDF().fit(frequencyDenseVectors)\n",
    "print('fitting complete')\n",
    "tfidf = idf.transform(frequencyDenseVectors)\n",
    "print(\"tf idf complete\")\n",
    "#prepare corpus for LDA\n",
    "corpus = tfidf.map(lambda x: [1, x]).cache()\n",
    "print(\"entering lda phase\")\n",
    "#train LDA\n",
    "ldaModel = LDA.train(corpus, k = 15, maxIterations=100, optimizer=\"online\", docConcentration=2.0, topicConcentration=3.0)\n",
    "print(\"lda model complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a45ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ldaModel.save(sc, \"ldaModel\")\n",
    "from operator import itemgetter\n",
    "topics = ldaModel.topicsMatrix()\n",
    "#for topic in range(3):\n",
    "    #print(\"Topic \" + str(topic) + \":\")\n",
    "    #for word in range(0, ldaModel.vocabSize()):\n",
    "        #print(\" \" + str(topics[word][topic]))\n",
    "# Now lets get the words back\n",
    "topicIndices = ldaModel.describeTopics(maxTermsPerTopic=5)\n",
    "vocablist = vectorizer.vocabulary\n",
    "topicsRDD = sc.parallelize(topicIndices)\n",
    "termsRDD = topicsRDD.map(lambda topic: (zip(itemgetter(*topic[0])(vocablist), topic[1])))\n",
    "\n",
    "indexedTermsRDD = termsRDD.zipWithIndex()\n",
    "termsRDD = indexedTermsRDD.flatMap(lambda term: [(t[0], t[1], term[1]) for t in term[0]])\n",
    "termDF = termsRDD.toDF(['term', 'probability', 'topicId'])\n",
    "termDF.orderBy('topicID').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa449ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "t2 = termDF\n",
    "\n",
    "t2.createOrReplaceTempView(\"topics\")\n",
    "\n",
    "terms=spark.sql(\"SELECT * FROM topics WHERE probability>.0001\")\n",
    "terms.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cdd450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.graphlab\n",
    "import graphlab as gl\n",
    "pyLDAvis.prepare(ldaModel.topicsMatrix(),vocabList,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdace002",
   "metadata": {},
   "source": [
    "# Word2Vec Example\n",
    "Here is a basic example of using PySpark's Word2Vec library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4091a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"filtered1\", outputCol=\"result\")\n",
    "model = word2Vec.fit(filtered_final)\n",
    "result = model.transform(filtered_final)\n",
    "for feature in result.select(\"result\").take(3):\n",
    "    print(feature)\n",
    "\n",
    "topN = 13\n",
    "synonymsDF = model.findSynonyms('boat', topN).toPandas()\n",
    "synonymsDF\n",
    "synonymsDF = model.findSynonyms('whitewater', topN).toPandas()\n",
    "synonymsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fcbccd",
   "metadata": {},
   "source": [
    "# Further Processing\n",
    "Another useful thing to do before building a Word2Vec model is stemming and (depending on the model) tagging. We can accomplish this with NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8907f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More Natural Language Processing using NLP. Lemmatization, Chunking, and Tagging. \n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "messages = filtered_final.select(\"filtered\")\n",
    "message_rdd=messages.rdd\n",
    "message_rdd = message_rdd.flatMap(lambda x:x)\n",
    "print(message_rdd.first())\n",
    "pos_statuses = message_rdd.map(nltk.pos_tag)\n",
    "print(pos_statuses.take(5))\n",
    "# Todo get lemmatization working\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
