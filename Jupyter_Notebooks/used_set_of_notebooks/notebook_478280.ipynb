{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2250c2f8",
   "metadata": {},
   "source": [
    "# Download and split raw data into training and test data    \n",
    "Christoph Windheuser, ThoughtWorks, June 19, 2020    \n",
    "     \n",
    "This notebook needs to be run in SageMaker Studio. It reads the data as csv-file from a public S3 bucket. Then it splits the data into a training and a test set and saves all in a personal S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d1171c",
   "metadata": {},
   "source": [
    "## Import the necessary libraries    \n",
    "pandas is a python data science library to handle dataframes    \n",
    "boto3 is the Amazon Web Services SDK for Python. It enables Python developers to create, configure, and manage AWS services, such as EC2 and S3.    \n",
    "S3Uploader and S3Downloader are routines to upload or download data into S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdafcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from   sklearn.preprocessing import LabelEncoder\n",
    "import boto3\n",
    "from   sagemaker.s3 import S3Uploader, S3Downloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287b3fdb",
   "metadata": {},
   "source": [
    "---\n",
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1190df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename   = 'store47-2016-train.csv'\n",
    "test_filename    = 'store47-2016-test.csv'\n",
    "final_train_file = 'final_train.csv'\n",
    "final_test_file  = 'final_validate.csv'\n",
    "local_data_dir   = 'CD4ML-AWS-Serverless/data'\n",
    "local_tmp_dir    = 'CD4ML-AWS-Serverless/data/tmp'\n",
    "s3_prefix        = 'demandforecast'\n",
    "raw_filename     = 'store47-2016.csv'\n",
    "s3_raw_data_path = 'https://christoph-windheuser-public.s3.amazonaws.com'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fa40bf",
   "metadata": {},
   "source": [
    "## Do some preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d847ac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)     # Make sure we can see all of the columns\n",
    "pd.set_option('display.max_rows', 10)         # Keep the output on one page\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b734308",
   "metadata": {},
   "source": [
    "## Define write_df_to_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8653feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_df_to_s3(df, filename, s3_path):\n",
    "    if not os.path.exists(local_tmp_dir):\n",
    "        os.makedirs(local_tmp_dir)\n",
    "    df.to_csv('{}/{}'.format(local_tmp_dir, filename), index=False)\n",
    "    s3url = S3Uploader.upload('{}/{}'.format(local_tmp_dir, filename), s3_path)\n",
    "    os.remove('{}/{}'.format(local_tmp_dir, filename))\n",
    "    print(s3url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d67f0aa",
   "metadata": {},
   "source": [
    "## Define write_dic_to_json_to_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82096cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dic_to_json_to_s3(dic, filename, s3_path):\n",
    "    if not os.path.exists(local_tmp_dir):\n",
    "        os.makedirs(local_tmp_dir)\n",
    "    \n",
    "    with open('{}/{}'.format(local_tmp_dir, filename), 'w') as fp:\n",
    "        json.dump(dic, fp)\n",
    "        \n",
    "    s3url = S3Uploader.upload('{}/{}'.format(local_tmp_dir, filename), s3_path)\n",
    "    os.remove('{}/{}'.format(local_tmp_dir, filename))\n",
    "    print(s3url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b9e834",
   "metadata": {},
   "source": [
    "---\n",
    "## Open S3 Session and define Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606703a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess       = boto3.Session()\n",
    "account_id = sess.client('sts', region_name=sess.region_name).get_caller_identity()[\"Account\"]\n",
    "bucket     = 'sagemaker-studio-{}-{}'.format(sess.region_name, account_id)\n",
    "\n",
    "try:\n",
    "    if sess.region_name == \"us-east-1\":\n",
    "        sess.client('s3').create_bucket(Bucket=bucket)\n",
    "    else:\n",
    "        sess.client('s3').create_bucket(Bucket=bucket, \n",
    "                                        CreateBucketConfiguration={'LocationConstraint': sess.region_name})\n",
    "except Exception as e:\n",
    "    print(\"Looks like you already have a bucket of this name. That's good. Uploading the data files...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd60e783",
   "metadata": {},
   "source": [
    "## Read raw data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a2d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('{}/{}'.format(s3_raw_data_path, raw_filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e089e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9115309",
   "metadata": {},
   "source": [
    "## Split into train and test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3745bac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data at the date 2017-08-02 (last 14 days of data set)\n",
    "data_train = data[data['date'] < '2017-08-02']\n",
    "data_test  = data[data['date'] >= '2017-08-02']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c7d95f",
   "metadata": {},
   "source": [
    "## Save train and test data as csv-file to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6cd9c4",
   "metadata": {},
   "source": [
    "First, save files locally on the SageMaker instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cccebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_df_to_s3(data_train, train_filename, 's3://{}/{}/{}'.format(bucket, s3_prefix,'train'))\n",
    "write_df_to_s3(data_test,  test_filename,  's3://{}/{}/{}'.format(bucket, s3_prefix,'test'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdd155d",
   "metadata": {},
   "source": [
    "---\n",
    "## Encode non-numerical values and drop date column\n",
    "The encoding schema of the product families is written to the json-file \"family_encoder.json\", as this encoding is later necessary in the ML model for inference. This file is written to S3 into the directory \"train/final/\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5015c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tables(train, validate):\n",
    "    print(\"Joining tables for consistent encoding\")\n",
    "    return train.append(validate).drop('date', axis=1)\n",
    "\n",
    "\n",
    "def encode_categorical_columns(df):\n",
    "    obj_df = df.select_dtypes(include=['object', 'bool']).copy().fillna('-1')\n",
    "    lb = LabelEncoder()\n",
    "    classes_dic = {}\n",
    "    \n",
    "    for col in obj_df.columns:\n",
    "        print (col)\n",
    "        df[col] = lb.fit_transform(obj_df[col])\n",
    "\n",
    "        if col == \"family\":    \n",
    "            classes = list(lb.classes_)\n",
    "            for index, c in enumerate(classes):\n",
    "                classes_dic[c] = index\n",
    "            write_dic_to_json_to_s3(classes_dic, 'family_encoder.json', 's3://{}/{}/{}'.format(bucket, s3_prefix,'train/final'))\n",
    "\n",
    "            # print (classes_dic)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def encode(train, validate):\n",
    "    print(\"Encoding categorical variables\")\n",
    "    train_ids = train.id\n",
    "    validate_ids = validate.id\n",
    "\n",
    "    joined  = join_tables(train, validate)\n",
    "\n",
    "    encoded = encode_categorical_columns(joined.fillna(-1))\n",
    "\n",
    "    print(\"Not predicting returns (changing negative unit sales to 0)\")\n",
    "    encoded.loc[encoded.unit_sales < 0, 'unit_sales'] = 0\n",
    "\n",
    "    validate = encoded[encoded['id'].isin(validate_ids)]\n",
    "    train = encoded[encoded['id'].isin(train_ids)]\n",
    "    return train, validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac06a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate = encode(data_train, data_test)\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f3189",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3427e4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74425b8",
   "metadata": {},
   "source": [
    "---\n",
    "## Save train and test data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87de5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_df_to_s3(train,    final_train_file, 's3://{}/{}/{}'.format(bucket, s3_prefix,'train/final'))\n",
    "write_df_to_s3(validate, final_test_file,  's3://{}/{}/{}'.format(bucket, s3_prefix,'test/final'))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
