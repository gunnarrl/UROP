{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7762a91",
   "metadata": {},
   "source": [
    "# Desafio 6\n",
    "\n",
    "Neste desafio, vamos praticar _feature engineering_, um dos processos mais importantes e trabalhosos de ML. Utilizaremos o _data set_ [Countries of the world](https://www.kaggle.com/fernandol/countries-of-the-world), que contém dados sobre os 227 países do mundo com informações sobre tamanho da população, área, imigração e setores de produção.\n",
    "\n",
    "> Obs.: Por favor, não modifique o nome das funções de resposta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9addce68",
   "metadata": {},
   "source": [
    "## _Setup_ geral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79e2608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import KBinsDiscretizer, OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c5b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Algumas configurações para o matplotlib.\n",
    "# %matplotlib inline\n",
    "\n",
    "# from IPython.core.pylabtools import figsize\n",
    "\n",
    "\n",
    "# figsize(12, 8)\n",
    "\n",
    "# sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec44526",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = pd.read_csv(\"countries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216e9399",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_names = [\n",
    "    \"Country\", \"Region\", \"Population\", \"Area\", \"Pop_density\", \"Coastline_ratio\",\n",
    "    \"Net_migration\", \"Infant_mortality\", \"GDP\", \"Literacy\", \"Phones_per_1000\",\n",
    "    \"Arable\", \"Crops\", \"Other\", \"Climate\", \"Birthrate\", \"Deathrate\", \"Agriculture\",\n",
    "    \"Industry\", \"Service\"\n",
    "]\n",
    "\n",
    "countries.columns = new_column_names\n",
    "\n",
    "countries.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f59ea5b",
   "metadata": {},
   "source": [
    "## Observações\n",
    "\n",
    "Esse _data set_ ainda precisa de alguns ajustes iniciais. Primeiro, note que as variáveis numéricas estão usando vírgula como separador decimal e estão codificadas como strings. Corrija isso antes de continuar: transforme essas variáveis em numéricas adequadamente.\n",
    "\n",
    "Além disso, as variáveis `Country` e `Region` possuem espaços a mais no começo e no final da string. Você pode utilizar o método `str.strip()` para remover esses espaços."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883a8709",
   "metadata": {},
   "source": [
    "## Inicia sua análise a partir daqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c476b4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdc0580",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205191e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries['Country'] = countries['Country'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c377539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries['Region'] = countries['Region'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016ed762",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in countries.columns:\n",
    "    if countries[column].dtype == np.dtype('O'):\n",
    "        countries[column] = countries[column].str.replace(',', '.')\n",
    "        try:\n",
    "            countries[column] = pd.to_numeric(countries[column])\n",
    "        except:\n",
    "            pass\n",
    "countries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bc6b41",
   "metadata": {},
   "source": [
    "## Questão 1\n",
    "\n",
    "Quais são as regiões (variável `Region`) presentes no _data set_? Retorne uma lista com as regiões únicas do _data set_ com os espaços à frente e atrás da string removidos (mas mantenha pontuação: ponto, hífen etc) e ordenadas em ordem alfabética."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360757fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1():\n",
    "    return sorted(countries['Region'].unique())\n",
    "q1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86a8592",
   "metadata": {},
   "source": [
    "## Questão 2\n",
    "\n",
    "Discretizando a variável `Pop_density` em 10 intervalos com `KBinsDiscretizer`, seguindo o encode `ordinal` e estratégia `quantile`, quantos países se encontram acima do 90º percentil? Responda como um único escalar inteiro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6438602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2():\n",
    "    discretizer = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile')\n",
    "    pop_density = discretizer.fit_transform(countries[['Pop_density']])\n",
    "    return int(sum(pop_density[:,0] == 9))\n",
    "q2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e4f139",
   "metadata": {},
   "source": [
    "# Questão 3\n",
    "\n",
    "Se codificarmos as variáveis `Region` e `Climate` usando _one-hot encoding_, quantos novos atributos seriam criados? Responda como um único escalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133561c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3():\n",
    "    one_hot_encoder = OneHotEncoder(sparse=False, dtype=np.int)\n",
    "    region_climate_one_hot = one_hot_encoder.fit_transform(countries[[\"Region\", \"Climate\"]].fillna(0))\n",
    "    return region_climate_one_hot.shape[1]\n",
    "q3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0498a231",
   "metadata": {},
   "source": [
    "## Questão 4\n",
    "\n",
    "Aplique o seguinte _pipeline_:\n",
    "\n",
    "1. Preencha as variáveis do tipo `int64` e `float64` com suas respectivas medianas.\n",
    "2. Padronize essas variáveis.\n",
    "\n",
    "Após aplicado o _pipeline_ descrito acima aos dados (somente nas variáveis dos tipos especificados), aplique o mesmo _pipeline_ (ou `ColumnTransformer`) ao dado abaixo. Qual o valor da variável `Arable` após o _pipeline_? Responda como um único float arredondado para três casas decimais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77abe752",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_country = [\n",
    "    'Test Country', 'NEAR EAST', -0.19032480757326514,\n",
    "    -0.3232636124824411, -0.04421734470810142, -0.27528113360605316,\n",
    "    0.13255850810281325, -0.8054845935643491, 1.0119784924248225,\n",
    "    0.6189182532646624, 1.0074863283776458, 0.20239896852403538,\n",
    "    -0.043678728558593366, -0.13929748680369286, 1.3163604645710438,\n",
    "    -0.3699637766938669, -0.6149300604558857, -0.854369594993175,\n",
    "    0.263445277972641, 0.5712416961268142\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255e8c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q4():\n",
    "    test_country_df = pd.DataFrame([test_country], columns=countries.columns)\n",
    "    nums_pipeline = Pipeline(steps=[\n",
    "        ('fill_median', SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "        ('standardize', StandardScaler())])\n",
    "    \n",
    "    nums = nums_pipeline.fit_transform(countries.iloc[:,2:])\n",
    "    nums = nums_pipeline.transform(test_country_df.iloc[:,2:])\n",
    "    return float(pd.DataFrame([nums[0]], columns=countries.columns[2:])['Arable'].round(3))\n",
    "q4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4e725b",
   "metadata": {},
   "source": [
    "## Questão 5\n",
    "\n",
    "Descubra o número de _outliers_ da variável `Net_migration` segundo o método do _boxplot_, ou seja, usando a lógica:\n",
    "\n",
    "$$x \\notin [Q1 - 1.5 \\times \\text{IQR}, Q3 + 1.5 \\times \\text{IQR}] \\Rightarrow x \\text{ é outlier}$$\n",
    "\n",
    "que se encontram no grupo inferior e no grupo superior.\n",
    "\n",
    "Você deveria remover da análise as observações consideradas _outliers_ segundo esse método? Responda como uma tupla de três elementos `(outliers_abaixo, outliers_acima, removeria?)` ((int, int, bool))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca520e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q5():\n",
    "    q1 = countries['Net_migration'].quantile(0.25)\n",
    "    q3 = countries['Net_migration'].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    outlier_interval_iqr = [q1 - 1.5 * iqr, q3 + 1.5 * iqr]\n",
    "    \n",
    "    outliers_inferior = countries[countries['Net_migration'] < outlier_interval_iqr[0]]\n",
    "    outliers_superior = countries[countries['Net_migration'] > outlier_interval_iqr[1]]\n",
    "    \n",
    "    return (outliers_inferior.shape[0], outliers_superior.shape[0], False)\n",
    "q5()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b69415",
   "metadata": {},
   "source": [
    "## Questão 6\n",
    "Para as questões 6 e 7 utilize a biblioteca `fetch_20newsgroups` de datasets de test do `sklearn`\n",
    "\n",
    "Considere carregar as seguintes categorias e o dataset `newsgroups`:\n",
    "\n",
    "```\n",
    "categories = ['sci.electronics', 'comp.graphics', 'rec.motorcycles']\n",
    "newsgroup = fetch_20newsgroups(subset=\"train\", categories=categories, shuffle=True, random_state=42)\n",
    "```\n",
    "\n",
    "\n",
    "Aplique `CountVectorizer` ao _data set_ `newsgroups` e descubra o número de vezes que a palavra _phone_ aparece no corpus. Responda como um único escalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcca53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q6():\n",
    "    categories = ['sci.electronics', 'comp.graphics', 'rec.motorcycles']\n",
    "    newsgroups = fetch_20newsgroups(subset=\"train\", categories=categories, shuffle=True, random_state=42)\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    newsgroups_count = count_vectorizer.fit_transform(newsgroups.data)\n",
    "    count_df = pd.DataFrame(newsgroups_count.toarray(), columns=count_vectorizer.get_feature_names())\n",
    "    \n",
    "    return int(count_df['phone'].sum())\n",
    "q6()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e576560e",
   "metadata": {},
   "source": [
    "## Questão 7\n",
    "\n",
    "Aplique `TfidfVectorizer` ao _data set_ `newsgroups` e descubra o TF-IDF da palavra _phone_. Responda como um único escalar arredondado para três casas decimais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6983fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q7():\n",
    "    categories = ['sci.electronics', 'comp.graphics', 'rec.motorcycles']\n",
    "    newsgroups = fetch_20newsgroups(subset=\"train\", categories=categories, shuffle=True, random_state=42)\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    newsgroups_count = count_vectorizer.fit_transform(newsgroups.data)\n",
    "    words_idx = count_vectorizer.vocabulary_.get('phone')\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_vectorizer.fit(newsgroups.data)\n",
    "\n",
    "    newsgroups_tfidf_vectorized = tfidf_vectorizer.transform(newsgroups.data)\n",
    "    \n",
    "    return float(newsgroups_tfidf_vectorized[:, words_idx].toarray().sum().round(3))\n",
    "q7()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
