{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6afde240",
   "metadata": {},
   "source": [
    "# Mars Target Encyclopedia  - NER\n",
    "Thamme Gowda (Thamme.Gowda@jpl.nasa.gov)\n",
    "\n",
    "Named Entity Recognition / Sequence Tagging\n",
    "This notebook contains NER tagging using CRF suite\n",
    "\n",
    "\n",
    "### Notes:\n",
    " + Use python3, Reason: we need unicode strings, which is default in python3\n",
    " + install Python-crfsuite\n",
    " + Start CoreNLP Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38327dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn\n",
    "import pycrfsuite\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from codecs import open as copen\n",
    "from collections import defaultdict as ddict\n",
    "from csv import DictWriter\n",
    "import sys\n",
    "from copy import copy\n",
    "import time\n",
    "from pprint import pprint\n",
    "import re\n",
    "\n",
    "import os, glob\n",
    "import pickle\n",
    "\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b0d59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accept_labels = set(['Element', 'Mineral', 'Target', 'Material', 'Locality', 'Site'])\n",
    "accept_labels = set(['Target', 'Mineral', 'Element'])\n",
    "\n",
    "class BratToCRFSuitFeaturizer(object):\n",
    "    def __init__(self, corenlp_url='http://localhost:9000', iob=False):\n",
    "        '''\n",
    "        Create Converter for converting brat annotations to Core NLP NER CRF\n",
    "        classifier training data.\n",
    "        @param corenlp_url: URL to corenlp server.\n",
    "                To start the server checkout: http://stanfordnlp.github.io/CoreNLP/corenlp-server.html#getting-started\n",
    "        @param iob: set 'True' for IOB encoding\n",
    "        '''\n",
    "        self.corenlp = StanfordCoreNLP(corenlp_url)\n",
    "        self.iob = iob\n",
    "\n",
    "    def convert(self, text_file, ann_file):\n",
    "        text, tree = self.parse(text_file, ann_file)\n",
    "        props = { 'annotators': 'tokenize,ssplit,lemma,pos,ner', 'outputFormat': 'json'}\n",
    "        if text[0].isspace():\n",
    "            text = '.' + text[1:]\n",
    "            # Reason: some tools trim/strip off the white spaces\n",
    "            # which will mismatch the character offsets\n",
    "        output = self.corenlp.annotate(text, properties=props)\n",
    "        records = []\n",
    "        for sentence in output['sentences']:\n",
    "            sent_features = []\n",
    "            continue_ann, continue_ann_en = None, None\n",
    "            for tok in sentence['tokens']:\n",
    "                begin, tok_end = tok['characterOffsetBegin'], tok['characterOffsetEnd']\n",
    "                label = 'O'\n",
    "                if begin in tree:\n",
    "                    node = tree[begin]\n",
    "                    if len(node) > 1:\n",
    "                        print(\"WARN: multiple starts at \", begin, node)\n",
    "                        if tok_end in node:\n",
    "                            node = {tok_end: node[tok_end]} # picking one\n",
    "                            print(\"Chose:\", node)\n",
    "\n",
    "                    ann_end, labels = list(node.items())[0]\n",
    "                    if not len(labels) == 1:\n",
    "                        print(\"WARN: Duplicate labels for token: %s, label:%s.\\\n",
    "                              Using the first one!\" % (tok['word'], str(labels)))\n",
    "                    if accept_labels is not None and labels[0] in accept_labels:\n",
    "                        label = labels[0]\n",
    "\n",
    "                    if tok_end == ann_end: # annotation ends where token ends\n",
    "                        continue_ann = None\n",
    "                    elif tok_end < ann_end and label != 'O':\n",
    "                        #print(\"Continue for the next %d chars\" % (ann_end - tok_end))\n",
    "                        continue_ann = label\n",
    "                        continue_ann_end = ann_end \n",
    "                    if label != 'O' and self.iob:\n",
    "                        label = \"B-\" + label\n",
    "                elif continue_ann is not None and tok_end <= continue_ann_end:\n",
    "                    #print(\"Continuing the annotation %s, %d:%d %d]\" % \n",
    "                    #(continue_ann, begin, tok_end, continue_ann_end))\n",
    "                    label = continue_ann            # previous label is this label\n",
    "                    if continue_ann_end == tok_end: # continuation ends here\n",
    "                        #print(\"End\")\n",
    "                        continue_ann = None\n",
    "                    if self.iob:\n",
    "                        label = \"I-\" + label\n",
    "                sent_features.append([tok['word'], tok['lemma'], tok['pos'], tok['ner'], label])\n",
    "            yield sent_features\n",
    "\n",
    "    def parse(self, txt_file, ann_file):\n",
    "        with copen(ann_file, 'r', encoding='utf-8') as ann_file:\n",
    "            with copen(txt_file, 'r', encoding='utf-8') as text_file:\n",
    "                texts = text_file.read()\n",
    "            anns = map(lambda x: x.strip().split('\\t'), ann_file)\n",
    "            anns = filter(lambda x: len(x) > 2, anns)\n",
    "            # FIXME: ignoring the annotatiosn which are complex\n",
    "\n",
    "            anns = filter(lambda x: ';' not in x[1], anns)\n",
    "            # FIXME: some annotations' spread have been split into many, separated by ; ignoring them\n",
    "\n",
    "            def __parse_ann(ann):\n",
    "                spec = ann[1].split()\n",
    "                name = spec[0]\n",
    "                markers = list(map(lambda x: int(x), spec[1:]))\n",
    "                #t = ' '.join([texts[begin:end] for begin,end in zip(markers[::2], markers[1::2])])\n",
    "                t = texts[markers[0]:markers[1]]\n",
    "                if not t == ann[2]:\n",
    "                    print(\"Error: Annotation mis-match, file=%s, ann=%s\" % (txt_file, str(ann)))\n",
    "                    return None\n",
    "                return (name, markers, t)\n",
    "            anns = map(__parse_ann, anns) # format\n",
    "            anns = filter(lambda x: x, anns) # skip None\n",
    "\n",
    "            # building a tree index for easy accessing\n",
    "            tree = {}\n",
    "            for entity_type, pos, name in anns:\n",
    "                if entity_type not in accept_labels:\n",
    "                    continue\n",
    "                begin, end = pos[0], pos[1]\n",
    "                if begin not in tree:\n",
    "                    tree[begin] = {}\n",
    "                node = tree[begin]\n",
    "                if end not in node:\n",
    "                    node[end] = []\n",
    "                node[end].append(entity_type)\n",
    "\n",
    "            # Re-read file in without decoding it\n",
    "            text_file = copen(txt_file, 'r', encoding='utf-8')\n",
    "            texts = text_file.read()\n",
    "            text_file.close()\n",
    "            return texts, tree\n",
    "\n",
    "def scan_dir(dir_name):\n",
    "    items = glob.glob(dir_name + \"/*.ann\")\n",
    "    items = map(lambda f: (f, f.replace(\".ann\", \".txt\")), items)\n",
    "    return items\n",
    "\n",
    "def preprocess_all(list_file, out_file):\n",
    "    featzr = BratToCRFSuitFeaturizer(iob=True)\n",
    "    tokenized = []\n",
    "    with open(list_file) as f:\n",
    "        examples = map(lambda l:l.strip().split(','), f.readlines())\n",
    "    for txt_file, ann_file in examples:\n",
    "        sents = featzr.convert(txt_file, ann_file)\n",
    "        tokenized.append(list(sents))\n",
    "\n",
    "    pickle.dump(tokenized, open(out_file, 'wb'))\n",
    "    print(\"Dumped %d docs to %s\" % (len(tokenized), out_file))\n",
    "\n",
    "#######################\n",
    "# Evaluates the model\n",
    "def evaluate(tagger, corpus_file):\n",
    "    \n",
    "    corpus = pickle.load(open(corpus_file, 'rb'))\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for doc in corpus:\n",
    "        seq = merge_sequences(doc)\n",
    "        truth = seq2labels(seq)\n",
    "        preds = tagger.tag(seq2features(seq))\n",
    "        assert len(truth) == len(preds)\n",
    "        y_true.extend(truth)\n",
    "        y_pred.extend(preds)    \n",
    "    assert len(y_true) == len(y_pred)\n",
    "    table = ddict(lambda: ddict(int)) \n",
    "    for truth, pred in zip(y_true, y_pred):\n",
    "        table[truth][pred] += 1\n",
    "        table[truth]['total'] += 1\n",
    "        table['total'][pred] += 1\n",
    "        table['total']['total'] += 1\n",
    "    keys = []\n",
    "    for label in accept_labels:\n",
    "        keys.append('B-%s' % label)\n",
    "        keys.append('I-%s' % label)\n",
    "    col_keys = copy(keys)\n",
    "    precision, recall = {}, {}\n",
    "    for k in set(keys):\n",
    "        tot_preds = table['total'][k]\n",
    "        tot_truth = table[k]['total']\n",
    "        table['Precision'][k] = \"%.4f\" % (float(table[k][k]) / tot_preds) if tot_preds else 0 \n",
    "        table['Recall'][k] = \"%.4f\" % (float(table[k][k]) / tot_truth) if tot_truth else 0 \n",
    "    col_keys.extend(['O', 'total'])\n",
    "    keys.extend(['', 'Precision', 'Recall', '', 'O', 'total'])\n",
    "    return table, keys, col_keys\n",
    "\n",
    "\n",
    "def printtable(table, row_keys, col_keys, delim=','):\n",
    "    \"\"\"\n",
    "    print table in CSV format which is meant to be copy pasted to Excel sheet \n",
    "    \"\"\"\n",
    "    f = sys.stdout\n",
    "    out = DictWriter(f, delimiter=delim, restval=0, fieldnames=col_keys)\n",
    "    f.write(\"%s%s\" % (\"***\", delim))\n",
    "    out.writeheader()\n",
    "    for k in row_keys:\n",
    "        if not k.strip():\n",
    "            f.write(\"\\n\")\n",
    "            continue\n",
    "        f.write(\"%s%s\" % (k, delim))\n",
    "        out.writerow(table[k])\n",
    "    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341bb157",
   "metadata": {},
   "source": [
    "## Parse and store the corpus\n",
    "\n",
    "In this step, we pass the text through CoreNLP pipeline, tokenize and POS tag them. \n",
    "In addition, we lookup the annotations file and match the target annotations with the token. \n",
    "\n",
    "Since this step is expensive, we store the results in pickle file, so that we can later load and resume our analysis for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f16913",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dir = \"/Users/thammegr/work/mte/data/newcorpus/workspace\"\n",
    "train_list = p_dir + \"/train_62r15_685k14_384k15.list\"\n",
    "dev_list= p_dir + \"/development.list\"\n",
    "test_list = p_dir + \"/test.list\"\n",
    "\n",
    "train_corpus_file = 'mte-corpus-train.pickle'\n",
    "preprocess_all(train_list, train_corpus_file)\n",
    "\n",
    "# Test and Development set\n",
    "dev_corpus_file = 'mte-corpus-dev.pickle'\n",
    "preprocess_all(dev_list, dev_corpus_file)\n",
    "test_corpus_file = 'mte-corpus-test.pickle'\n",
    "preprocess_all(test_list, test_corpus_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501f4293",
   "metadata": {},
   "source": [
    "## Load the corpus\n",
    "Here we load the corpus from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = 'mte-corpus-train.pickle'\n",
    "corpus = pickle.load(open(corpus_file, 'rb'))\n",
    "corpus[0][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f52fb7",
   "metadata": {},
   "source": [
    "Next, we start playing with the features of CRF Suite to build a sequence tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de15ec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "config = {\n",
    "    'POS': False,\n",
    "    'gen_POS': True, # generalize POS\n",
    "    'bias': True,\n",
    "    'max_suffix_chars': 3,\n",
    "    'is_lower': True,\n",
    "    'is_upper': True,\n",
    "    'is_title': True,\n",
    "    'text': True,\n",
    "    'wordshape': 'sound',\n",
    "    'NER': False, # default NER\n",
    "    'context': list(range(-1, 2))\n",
    "}\n",
    "\n",
    "def get_wordshape_general(word):\n",
    "    \"\"\"\n",
    "    Makes shape of the word based on upper case, lowercase or digit\n",
    "    \"\"\"\n",
    "    # Note : the order of replacement matters, digits should be at the last\n",
    "    return re.sub(\"[0-9]\", 'd', \n",
    "                  re.sub(\"[A-Z]\", 'X',\n",
    "                         re.sub(\"[a-z]\", 'x', word)))\n",
    "\n",
    "def get_wordshape_sound(word):\n",
    "    \"\"\"\n",
    "    Makes shape of word based on the vowel or consonenet sound\n",
    "    \"\"\"\n",
    "    # Note : the order of replacement matters, c, v, d in order\n",
    "    word = re.sub(\"[bcdfghjklmnpqrstvwxyzBCDFGHJKLMNPQRSTVWXYZ]\", 'c', word) # consonents\n",
    "    word = re.sub(\"[AEIOUaeiou]\", 'v', word) # vowels\n",
    "    word = re.sub(\"[0-9]\", 'd', word) # digits\n",
    "    return word\n",
    "\n",
    "def get_wordshape_sound_case(word):\n",
    "    \"\"\"\n",
    "    Makes shape of word based on the vowel or consonenet sound considering case\n",
    "    \"\"\"\n",
    "    word = re.sub(\"[bcdfghjklmnpqrstvwxyz]\", 'c', word) # consonents\n",
    "    word = re.sub(\"[BCDFGHJKLMNPQRSTVWXYZ]\", 'C', word) # upper consonents\n",
    "    word = re.sub(\"[aeiou]\", 'v', word) # vowels\n",
    "    word = re.sub(\"[AEIOU]\", 'V', word) # upper vowels\n",
    "    word = re.sub(\"[+-]?[0-9]+(\\.[0-9]+)?\", 'N', word) # digits\n",
    "    return word\n",
    "\n",
    "def word2features(sent, idx):\n",
    "    word = sent[idx]\n",
    "    words = []\n",
    "    feats = []\n",
    "\n",
    "    # Context\n",
    "    context = set(config.get('context', []))\n",
    "    context.add(0)  # current word\n",
    "    for ctx in sorted(context):\n",
    "        pos = ctx + idx\n",
    "        if pos >= 0 and pos < len(sent):\n",
    "            words.append((str(ctx), sent[pos]))\n",
    "    \n",
    "    if idx == 0:\n",
    "        feats.append('BOS') # begin of sequence\n",
    "    if idx == len(sent) - 1:\n",
    "        feats.append('EOS')\n",
    "    for prefix, word in words:\n",
    "        assert len(word) == 5\n",
    "        txt, lemma, POS, ner, label = word \n",
    "        if config.get('bias'):\n",
    "            feats.append('%sword.bias'% (prefix))\n",
    "        if config.get('POS'):\n",
    "            feats.append('%sword.pos=%s' %(prefix, POS))\n",
    "        if config.get('gen_POS'):\n",
    "            feats.append('%sword.genpos=%s' %(prefix, POS[:2]))\n",
    "        if config.get('max_suffix_chars'):\n",
    "            for i in range(1, config.get('max_suffix_chars', -1) + 1):\n",
    "                if len(txt) < i:\n",
    "                    break\n",
    "                feats.append('%sword[-%d:]=%s' % (prefix, i, txt[-i:]))\n",
    "        if config.get('is_lower'):\n",
    "            feats.append('%sword.islower=%s' % (prefix, txt.islower()))\n",
    "        if config.get('is_upper'):\n",
    "            feats.append('%sword.isupper=%s' % (prefix, txt.isupper()))\n",
    "        if config.get('is_title'):\n",
    "            feats.append('%sword.istitle=%s' % (prefix, txt.istitle()))\n",
    "        if config.get('wordshape'):\n",
    "            shape = config['wordshape'] \n",
    "            if shape == 'general':\n",
    "                shape_val = get_wordshape_general(txt)\n",
    "            elif shape == 'sound':\n",
    "                shape_val = get_wordshape_sound(txt)\n",
    "            elif shape == 'sound_case':\n",
    "                shape_val = get_wordshape_sound_case(txt)\n",
    "            else:\n",
    "                raise Error(\"Word Shape spec unknown '%s'\" % config['wordshape'])\n",
    "            feats.append('%sword.shape=%s' % (prefix, shape_val))\n",
    "        if config.get('NER'):\n",
    "            feats.append('%sword.ner=%s' % (prefix, ner))\n",
    "        if config.get('text'):\n",
    "            feats.append('%sword.text=%s' % (prefix, txt))\n",
    "    return feats\n",
    "\n",
    "def seq2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def seq2labels(sent):\n",
    "    # Assumption the last one in array is always a label\n",
    "    return [tok[-1] for tok in sent] \n",
    "\n",
    "def merge_sequences(doc):\n",
    "    '''\n",
    "    document contains multiple sentences. here all sentences in document are merged to form one large sequence.\n",
    "    '''\n",
    "    res = []\n",
    "    for seq in doc:\n",
    "        res.extend(seq)\n",
    "        res.append(['|', '|', '|', 'O', 'O']) # sentence end marker\n",
    "    return res\n",
    "  \n",
    "\n",
    "def train(corpus, model_file):\n",
    "    trainer = pycrfsuite.Trainer(verbose=False)\n",
    "    # Load training examples\n",
    "    flag = True\n",
    "    for doc in corpus:\n",
    "        seq = merge_sequences(doc)\n",
    "        x_seq = seq2features(seq)\n",
    "        if flag:\n",
    "            p = 403\n",
    "            print(\"Sample features:\")\n",
    "            print(\"\\n\".join(map(str, seq[p-6:p+6])))\n",
    "            print(\"\\n\".join(x_seq[p]))\n",
    "            flag = False\n",
    "        y_seq = seq2labels(seq)\n",
    "        trainer.append(x_seq, y_seq)\n",
    "\n",
    "    trainer.set_params({\n",
    "        'c1': 0.5,   # coefficient for L1 penalty\n",
    "        'c2': 1e-3,  # coefficient for L2 penalty\n",
    "        'max_iterations': 50,  # stop earlier\n",
    "        # include transitions that are possible, but not observed\n",
    "        'feature.possible_transitions': True\n",
    "    })\n",
    "\n",
    "\n",
    "    st = time.time()\n",
    "    pprint(trainer.params())\n",
    "    pprint(config)\n",
    "    trainer.train(model_file)\n",
    "    print(\"Training Time: %.3fs\" % (time.time() - st))\n",
    "\n",
    "model_file = 'jpl-mars-target-ner-model.crfsuite'\n",
    "train(corpus, model_file)\n",
    "\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open(model_file)\n",
    "print(\"\\nEvaluating on Development Set\\n\")\n",
    "dev_corpus_file = 'mte-corpus-dev.pickle'\n",
    "printtable(*evaluate(tagger, dev_corpus_file))\n",
    "\n",
    "print(\"\\nEvaluating on Test Set\\n\")\n",
    "test_corpus_file = 'mte-corpus-test.pickle'\n",
    "printtable(*evaluate(tagger, test_corpus_file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6094664d",
   "metadata": {},
   "source": [
    "# Using the model to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc01df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open(model_file)\n",
    "\n",
    "with open(dev_corpus_file, 'rb') as f:\n",
    "    dev_corpus = pickle.load(f)\n",
    "\n",
    "ctx = (-3, 4)\n",
    "c = 0\n",
    "print(\"idx, Truth, Predicted, Word, Comment \")\n",
    "for doc in dev_corpus:\n",
    "    seq = merge_sequences(doc)\n",
    "    y = seq2labels(seq)\n",
    "    y_ = tagger.tag(seq2features(seq))\n",
    "    \n",
    "    for idx in range(len(seq)):\n",
    "        a, p, tok = y[idx], y_[idx], seq[idx]\n",
    "        if a == 'O' and p == 'B-Element':\n",
    "            for pos in filter(lambda p: 0 <= p < len(seq), range(idx+ctx[0], idx+ctx[1])):\n",
    "                if idx == pos:\n",
    "                    label = \"<CORR>\" if a == p else \"<ERR>\"\n",
    "                else:\n",
    "                    label = \"%d\" % (pos - idx)\n",
    "                print(\"%4d %9s %9s %8s %s\" % (pos, y[pos], y_[pos], label, str(seq[pos])))\n",
    "            print(\"\")\n",
    "            if a != p:\n",
    "                c += 1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b483b72a",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "### Interpretation of matrix\n",
    "Row sum is total true labels\n",
    "Column sum is predictions total labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3509b60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"\\nTest Set\")\n",
    "test_corpus_file = 'mte-corpus-test.pickle'\n",
    "printtable(*evaluate(tagger, test_corpus_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625315db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_classification_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Classification report for a list of BIO-encoded sequences.\n",
    "    It computes token-level metrics and discards \"O\" labels.\n",
    "    \n",
    "    Note that it requires scikit-learn 0.15+ (or a version from github master)\n",
    "    to calculate averages properly!\n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    #tagset.append('O')\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )\n",
    "\n",
    "def evaluate(tagger, corpus_file):    \n",
    "    corpus = pickle.load(open(corpus_file, 'rb'))\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for doc in corpus:\n",
    "        seq = merge_sequences(doc)\n",
    "        y_true.append(seq2labels(seq))\n",
    "        y_pred.append(tagger.tag(seq2features(seq)))\n",
    "    return bio_classification_report(y_true, y_pred)\n",
    "\n",
    "\n",
    "dev_corpus_file = 'mte-corpus-dev.pickle'\n",
    "test_corpus_file = 'mte-corpus-test.pickle'\n",
    "print(\"Development\")\n",
    "print(evaluate(tagger, dev_corpus_file))\n",
    "\n",
    "print(\"Testing\")\n",
    "print(evaluate(tagger, test_corpus_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a11fee",
   "metadata": {},
   "source": [
    "# Learning: State Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70828a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "info = tagger.info()\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common(15))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common()[-15:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb64f60",
   "metadata": {},
   "source": [
    "# Learning: State Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-6s %s\" % (weight, label, attr))    \n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(info.state_features).most_common(20))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(info.state_features).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580c902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = ['a', 'b', 'c']\n",
    "a, b, c =  *arr\n",
    "print(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036dc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s = \"hellow 124.45 -65.7623\"\n",
    "get_wordshape_sound_case(\"hellow 124.45 -65.7623\")\n",
    "#get_wordshape_sound(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dd2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"abcd\"[:2]"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
