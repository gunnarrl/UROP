{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b130ae",
   "metadata": {},
   "source": [
    "# Image pixel-level saliency using VGG-16 Conv-net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c29be",
   "metadata": {},
   "source": [
    "Lets have plots appear inline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293c8b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a71829",
   "metadata": {},
   "source": [
    "We're going to need `numpy`, `matplotlib`, `skimage`, `torch`, `torch.nn`, `torch.nn.functional` and `torchvision`.\n",
    "\n",
    "Our `utils` module provides an image preparation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7de1d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "import torchvision\n",
    "import skimage.transform, skimage.util\n",
    "from skimage.util import montage\n",
    "\n",
    "import utils\n",
    "import imagenet_classes\n",
    "\n",
    "torch_device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64b7da6",
   "metadata": {},
   "source": [
    "## Load an image to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f486b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_block(path, x_pos=None, y_pos=None, width=None, height=None):\n",
    "    return plt.imread(path)[y_pos:y_pos+height, x_pos:x_pos+width]\n",
    "\n",
    "def load_image(path):\n",
    "    return plt.imread(path)\n",
    "\n",
    "# Extract a 896 x 896 block surrounding the peacock\n",
    "peacock = load_image_block(os.path.join('images', 'P1013781.JPG'), 652, 1800, 896, 896)\n",
    "plt.imshow(peacock)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7aaed3",
   "metadata": {},
   "source": [
    "## Load the model\n",
    "\n",
    "To save some time, we are going to use the `VGG16Model` class from `pretrained_vgg_models` to build the network and load its parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776baa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build it, requesting that the pre-trained model weights are loaded\n",
    "# The call to the `to` method moves it onto the GPU\n",
    "vgg16_net = torchvision.models.vgg.vgg16(pretrained=True).to(torch_device)\n",
    "\n",
    "# Call the eval() method; we are not training\n",
    "vgg16_net.eval()\n",
    "\n",
    "# Also, set a variable \n",
    "MODEL_MEAN = np.array([0.485, 0.456, 0.406])\n",
    "MODEL_STD = np.array([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af326f77",
   "metadata": {},
   "source": [
    "## Discover the pixel-level image saliency\n",
    "We will be using guided backpropagation as described in [1], referring to [2], implemented as a [custom PyTorch autograd function](https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html):\n",
    "\n",
    "[1]: Springenberg et al. (2015): \"Striving for Simplicity - The All Convolutional Net\", http://arxiv.org/abs/1412.6806\n",
    "[2]: Utku Ozbulak, \"Pytorch implementation of convolutional neural network visualization techniques\", http://github.com/utkuozbulak/pytorch-cnn-visualizations\n",
    "\n",
    "Please see the above references for more information on the technique and its implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2255baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuidedBackpropReLUFunction (torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input = grad_input.clamp(min=0) * (input >= 0).float()\n",
    "        return grad_input\n",
    "    \n",
    "class GuidedBackpropReLU (nn.Module):\n",
    "    def forward(self, x):\n",
    "        return GuidedBackpropReLUFunction.apply(x)\n",
    "\n",
    "    \n",
    "def apply_guided_backprop(model):\n",
    "    if isinstance(model, (nn.ModuleList, nn.Sequential)):\n",
    "        for i, sub in enumerate(model):\n",
    "            if isinstance(sub, nn.ReLU):\n",
    "                model[i] = GuidedBackpropReLU()\n",
    "            else:\n",
    "                apply_guided_backprop(sub)\n",
    "    else:\n",
    "        for name, sub in model.named_children():\n",
    "            if isinstance(sub, nn.ReLU):\n",
    "                setattr(model, name, GuidedBackpropReLU())\n",
    "            else:\n",
    "                apply_guided_backprop(sub)\n",
    "            \n",
    "apply_guided_backprop(vgg16_net)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1910da7b",
   "metadata": {},
   "source": [
    "## Define a function to visualise saliency\n",
    "\n",
    "Define a function to predict the class of the image, compute a saliency map and visualise it (for convenience):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa38cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_classify_images(image_batch):\n",
    "    \"\"\"\n",
    "    image_batch - images as a NumPy 4D array - (sample, channel, height, width)\n",
    "    return: probabilities - (sample, class)\n",
    "    \"\"\"\n",
    "    # We don't need gradients here as we are only performing inference/prediction\n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor(image_batch, dtype=torch.float, device=torch_device)\n",
    "        logits = vgg16_net(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        return probs.detach().cpu().numpy()\n",
    "    \n",
    "def f_class_saliency_map(image, model, class_index=None):\n",
    "    x = torch.tensor(image, dtype=torch.float, device=torch_device, requires_grad=True)\n",
    "    logits = vgg16_net(x)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    if class_index is not None:\n",
    "        cls_prob = probs[:,class_index]\n",
    "    else:\n",
    "        cls_prob, _ = torch.max(probs, dim=1)\n",
    "    cls_prob.sum().backward()\n",
    "    return x.grad.detach().cpu().numpy()\n",
    "\n",
    "def visualise_cls_saliency(img, saliency_class=None):\n",
    "    # Prepare the image for use with VGG\n",
    "    raw_img, img_for_vgg = utils.prepare_image(img, MODEL_MEAN, MODEL_STD, image_size=224)\n",
    "    \n",
    "    # Predict the image class\n",
    "    pred_prob = f_classify_images(img_for_vgg)\n",
    "    pred_cls = np.argmax(pred_prob, axis=1)[0]\n",
    "    pred_confidence = pred_prob[0,pred_cls]\n",
    "    \n",
    "    # Print what we found:\n",
    "    print('Predicted class = {}, name = {}, confidence = {:.2%}'.format(\n",
    "            pred_cls, imagenet_classes.IMAGENET_CLASSES[pred_cls], pred_confidence))\n",
    "    \n",
    "    # If no saliency class was specified, use the predicted one\n",
    "    if saliency_class is None:\n",
    "        saliency_class = pred_cls\n",
    "    \n",
    "    # Compute a saliency image as a tensor\n",
    "    # Choose the function depending on whether the class is specified or not\n",
    "    saliency = f_class_saliency_map(img_for_vgg, saliency_class)\n",
    "    \n",
    "    # Extract sample 0, move channel axis to the back\n",
    "    saliency_image = saliency[0].transpose(1, 2, 0)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.imshow(np.sqrt(np.maximum(saliency_image, 0.0) / saliency_image.max()))\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.imshow(np.sqrt(np.maximum(-saliency_image, 0.0) / -saliency_image.min()))\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.imshow(raw_img)\n",
    "    plt.imshow(np.sqrt(np.maximum(saliency_image, 0.0) / saliency_image.max()), alpha=0.6)\n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.imshow(raw_img)\n",
    "    plt.imshow(np.sqrt(np.maximum(-saliency_image, 0.0) / -saliency_image.min()), alpha=0.6)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b936918c",
   "metadata": {},
   "source": [
    "## Lets try it out:\n",
    "\n",
    "Try it on the peacock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcc2ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_cls_saliency(peacock, 84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fb320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers = load_image_block(os.path.join('images', 'P8131065.JPG'), 600, 1300, 224*3, 224*3)\n",
    "plt.imshow(flowers)\n",
    "\n",
    "visualise_cls_saliency(flowers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f055e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = load_image_block(os.path.join('images', 'P8131065.JPG'), 1000, 900, 224*4, 224*4)\n",
    "plt.imshow(dog)\n",
    "\n",
    "visualise_cls_saliency(dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eba77fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "wall_and_flowers = load_image_block(os.path.join('images', 'P8131065.JPG'), 0, 500, 224*3, 224*3)\n",
    "plt.imshow(wall_and_flowers)\n",
    "\n",
    "visualise_cls_saliency(wall_and_flowers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe1ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "wall_and_flowers = load_image_block(os.path.join('images', 'P8131065.JPG'), 900, 0, 224*3, 224*3)\n",
    "plt.imshow(wall_and_flowers)\n",
    "\n",
    "visualise_cls_saliency(wall_and_flowers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbf1371",
   "metadata": {},
   "source": [
    "### Visualising saliency at lower levels of the network\n",
    "\n",
    "It is often informative to visualise pixel level saliency with respect to activations of the convolutional layers of the network.\n",
    "\n",
    "In the following examples, we visualise saliency with respect to the output at the pooling layers that follow the convolutional layers. It allows us to see how the lower levels of the network detect small simple features - such as colour and gradient - while higher levels detect more complex features.\n",
    "\n",
    "First, we define functions similar to those before for generating and plotting the saliency images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef0def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache for storing compiled functions; compiling Theano functions can be slow\n",
    "_SALIENCY_FUNCTIONS_BY_LAYER = {}\n",
    "\n",
    "def f_saliency_map(image, model, channel_index):\n",
    "    x = torch.tensor(image, dtype=torch.float, device=torch_device, requires_grad=True)\n",
    "    y = model(x)\n",
    "    chn_out = y[:, channel_index]\n",
    "    chn_out.max().backward()\n",
    "    return x.grad.detach().cpu().numpy()\n",
    "\n",
    "        \n",
    "def f_model(image, model):\n",
    "    x = torch.tensor(image, dtype=torch.float, device=torch_device)\n",
    "    y = model(x)\n",
    "    return y.detach().cpu().numpy()\n",
    "\n",
    "        \n",
    "def visualise_low_level_saliency(img, model, n_channels, patch_shape, max_cols=8, out_path=None):\n",
    "    half_patch_shape = patch_shape[0] // 2, patch_shape[1] // 2\n",
    "    # Prepare the image for use with VGG\n",
    "    raw_img, img_for_vgg = utils.prepare_image(img, MODEL_MEAN, MODEL_STD, image_size=224)\n",
    "    \n",
    "    y = f_model(img_for_vgg, model)\n",
    "    \n",
    "    # Compute the downsampling factor at the specified layer of the network\n",
    "    scale_factor = raw_img.shape[0] // y.shape[2]\n",
    "    \n",
    "    # Get the maximum activation value in each channel\n",
    "    chn_max = y.max(axis=(0,2,3))\n",
    "    # Get the channel indices in order of decreasing maximum activation value\n",
    "    channel_indices = np.argsort(chn_max[::-1])\n",
    "    channel_indices = list(channel_indices[:n_channels])\n",
    "    \n",
    "    saliency_patches = []\n",
    "    for chn in channel_indices:\n",
    "        # Get the activation values in the selected channel\n",
    "        y_chn = y[0, chn, :, :]\n",
    "        # Get the position of the maximum activation\n",
    "        y_chn_max_pos = np.unravel_index(np.argmax(y_chn.flatten()), y_chn.shape)\n",
    "        # Apply scale factor\n",
    "        y_chn_max_pos = y_chn_max_pos[0] * scale_factor, y_chn_max_pos[1] * scale_factor\n",
    "        # Clamp position so that the patch lies completely in the bounds of the image\n",
    "        y_chn_max_pos = (min(max(y_chn_max_pos[0], half_patch_shape[0]), raw_img.shape[0]-half_patch_shape[0]-1),\n",
    "                         min(max(y_chn_max_pos[1], half_patch_shape[1]), raw_img.shape[1]-half_patch_shape[1]-1))\n",
    "        y_sal = f_saliency_map(img_for_vgg, model, chn)\n",
    "\n",
    "        # Extract sample 0, move channel axis to the back\n",
    "        y_sal = y_sal[0].transpose(1, 2, 0)\n",
    "\n",
    "        # Compute patch bounds\n",
    "        p_lower = y_chn_max_pos[0] - half_patch_shape[0], y_chn_max_pos[1] - half_patch_shape[1]\n",
    "        p_upper = y_chn_max_pos[0] + half_patch_shape[0], y_chn_max_pos[1] + half_patch_shape[1]\n",
    "        # Extract patches\n",
    "        img_patch = raw_img[p_lower[0]:p_upper[0], p_lower[1]:p_upper[1], :]\n",
    "        sal_patch = y_sal[p_lower[0]:p_upper[0], p_lower[1]:p_upper[1], :]\n",
    "        # Scale saliency to [0,1] range, then apply sqrt to make the result more visible when displayed\n",
    "        sal_patch = np.sqrt(np.maximum(sal_patch, 0.0) / sal_patch.max())\n",
    "        # Store\n",
    "        saliency_patches.append((img_patch, sal_patch))\n",
    "        \n",
    "    N = len(saliency_patches) * 2\n",
    "    num_cols = max(1024 // patch_shape[1], 2)\n",
    "    if num_cols % 2 == 1:\n",
    "        num_cols -= 1\n",
    "    num_cols = min(num_cols, max_cols)\n",
    "    \n",
    "    num_rows = n_channels * 2 // num_cols\n",
    "    if num_rows * num_cols < N:\n",
    "        num_rows += 1\n",
    "    \n",
    "    patch_width = 16.0 / num_cols\n",
    "    height = float(num_rows * patch_shape[0] * patch_width) / patch_shape[1]\n",
    "\n",
    "    # Plot\n",
    "    patches = []\n",
    "    for ip, sp in saliency_patches:\n",
    "        patches.append(ip[None, :, :, :])\n",
    "        patches.append(sp[None, :, :, :])\n",
    "    patches = np.concatenate(patches, axis=0)\n",
    "    m = montage(patches, grid_shape=(num_rows, num_cols), multichannel=True)\n",
    "    \n",
    "    plt.figure(figsize=(16,int(height+0.5)))\n",
    "    plt.imshow(m)\n",
    "    plt.show()\n",
    "    \n",
    "    if out_path is not None:\n",
    "        plt.imsave(out_path, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af8717e",
   "metadata": {},
   "source": [
    "Lets get the layout of the `features` component of the model so we can select the parts of the model up to just after the pooling layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ebd88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vgg16_net.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f368382",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_to_pool1 = vgg16_net.features[:5]\n",
    "net_to_pool2 = vgg16_net.features[:10]\n",
    "net_to_pool3 = vgg16_net.features[:17]\n",
    "net_to_pool4 = vgg16_net.features[:24]\n",
    "net_to_pool5 = vgg16_net.features[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ae1a14",
   "metadata": {},
   "source": [
    "#### Try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175cd6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, layer_name, patch_size, n_chns in zip([net_to_pool1, net_to_pool2, net_to_pool3, net_to_pool4, net_to_pool5],\n",
    "                                                 ['pool1', 'pool2', 'pool3', 'pool4', 'pool5'],\n",
    "                                                 [32, 32, 48, 96, 192], [16, 16, 12, 8, 8]):\n",
    "    channels = range(11)\n",
    "    print('----------------------------------------------------')\n",
    "    print('PIXEL LEVEL SALIENCY OF PEACOCK IMAGE AT LAYER {}'.format(layer_name))\n",
    "    print('----------------------------------------------------')\n",
    "    visualise_low_level_saliency(peacock, model, n_chns, (patch_size,patch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4427e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, layer_name, patch_size, n_chns in zip([net_to_pool1, net_to_pool2, net_to_pool3, net_to_pool4, net_to_pool5],\n",
    "                                                 ['pool1', 'pool2', 'pool3', 'pool4', 'pool5'],\n",
    "                                                 [32, 32, 48, 96, 192], [16, 16, 12, 8, 8]):\n",
    "    channels = range(11)\n",
    "    print('----------------------------------------------------')\n",
    "    print('PIXEL LEVEL SALIENCY OF FLOWERS IMAGE AT LAYER {}'.format(layer_name))\n",
    "    print('----------------------------------------------------')\n",
    "    visualise_low_level_saliency(flowers, model, n_chns, (patch_size,patch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89fcb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, layer_name, patch_size, n_chns in zip([net_to_pool1, net_to_pool2, net_to_pool3, net_to_pool4, net_to_pool5],\n",
    "                                                 ['pool1', 'pool2', 'pool3', 'pool4', 'pool5'],\n",
    "                                                 [32, 32, 48, 96, 192], [16, 16, 12, 8, 8]):\n",
    "    channels = range(11)\n",
    "    print('----------------------------------------------------')\n",
    "    print('PIXEL LEVEL SALIENCY OF DOG IMAGE AT LAYER {}'.format(layer_name))\n",
    "    print('----------------------------------------------------')\n",
    "    visualise_low_level_saliency(dog, model, n_chns, (patch_size,patch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cc2aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, layer_name, patch_size, n_chns in zip([net_to_pool1, net_to_pool2, net_to_pool3, net_to_pool4, net_to_pool5],\n",
    "                                                 ['pool1', 'pool2', 'pool3', 'pool4', 'pool5'],\n",
    "                                                 [32, 32, 48, 96, 192], [16, 16, 12, 8, 8]):\n",
    "    channels = range(11)\n",
    "    print('----------------------------------------------------')\n",
    "    print('PIXEL LEVEL SALIENCY OF WALL AND FLOWERS IMAGE AT LAYER {}'.format(layer_name))\n",
    "    print('----------------------------------------------------')\n",
    "    visualise_low_level_saliency(wall_and_flowers, model, n_chns, (patch_size,patch_size))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
