{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e8abc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf09280",
   "metadata": {},
   "source": [
    "## The CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e83d578",
   "metadata": {},
   "source": [
    "A CNN architecture influenced by [NaimishNet](https://arxiv.org/pdf/1710.00977.pdf) and [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf). It was mostly built by trial and\n",
    "error because the main purpose was to get insights about the CNN as a whole. The analogy here is: I would like to know the individual parts of the car, but I am mainly interested in driving it.\n",
    "\n",
    "NaimishNet was my first when training the facekeypoints dataset. The results with NaimishNet weren't very good, perhaps because NaimishNet worked with a larger database and their image processing pipeline was better suited for 96x96 images, as opposed to the images used in this project, which are better used when keeping width and height close to 224. AlexNet, however, was a better solution for this case scenario. The only problem was the time it took to train the model.\n",
    "\n",
    "As a result of experimenting with two different architectures, I tried to keep the the best of both worlds and have a CNN that has a high have of predicting the x and y positions of the facekeypoints, and can also be trained at a speed that does not compromise my time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dba0806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from models import Net\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb59855e",
   "metadata": {},
   "source": [
    "## Image Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f34220e",
   "metadata": {},
   "source": [
    "I experimented a few variations of the pre-processing. The majority of the experiments consisted in rescaling the image, performing a random crop and including RGB channels in the normalization step.\n",
    "\n",
    "- Rescaling the image to 224x224 worked best because:\n",
    "    - It is the recommend size for this project\n",
    "    - It is the same size proposed in the AlexNet paper\n",
    "    - Ground truth values were consistently close to the left eye, the same as observed in the NaimishNet paper\n",
    "    - All target keypoints were included\n",
    "- Random cropping was not used as it brought no significant benefits\n",
    "- Image normalization used default implementation of Normalize\n",
    "\n",
    "**Note**: Image augmentation could be use to increase the size and add variation to the image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43213398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from data_load import Rescale, RandomCrop, Normalize, ToTensor\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    Rescale((224, 224)),\n",
    "    Normalize(),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef74af8",
   "metadata": {},
   "source": [
    "## Load and transform datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9dd6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_load import FacialKeypointsDataset\n",
    "\n",
    "transformed_dataset = FacialKeypointsDataset(csv_file='data/training_frames_keypoints.csv',\n",
    "                                             root_dir='data/training/',\n",
    "                                             transform=data_transform)\n",
    "\n",
    "print('Number of images: ', len(transformed_dataset))\n",
    "for i in range(4):\n",
    "    sample = transformed_dataset[i]\n",
    "    print(i, sample['image'].size(), sample['keypoints'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c8ebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_load import FacialKeypointsDataset\n",
    "\n",
    "test_dataset = FacialKeypointsDataset(csv_file='data/test_frames_keypoints.csv',\n",
    "                                             root_dir='data/test/',\n",
    "                                             transform=data_transform)\n",
    "\n",
    "print('Number of images: ', len(test_dataset))\n",
    "for i in range(4):\n",
    "    sample = test_dataset[i]\n",
    "    print(i, sample['image'].size(), sample['keypoints'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a992466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "batch_size = 30\n",
    "train_loader = DataLoader(transformed_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2babcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Net\n",
    "import torch\n",
    "\n",
    "net = Net()\n",
    "\n",
    "\"\"\"\n",
    "Passes loader images to the trained model and outputs each \n",
    "image, its target keypoints and predicted keypoints\n",
    "\n",
    ":param name: A DataLoader object\n",
    "\"\"\"\n",
    "def net_sample_output(loader):    \n",
    "    for i, sample in enumerate(loader):\n",
    "        images = sample['image']\n",
    "        key_pts = sample['keypoints']\n",
    "        images = images.type(torch.FloatTensor)\n",
    "        output_pts = net(images)\n",
    "        output_pts = output_pts.view(output_pts.size()[0], 68, -1)\n",
    "        if i == 0:\n",
    "            return images, output_pts, key_pts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1e5b5c",
   "metadata": {},
   "source": [
    "## Visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc077d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Shows image with predicted keypoints\n",
    "\n",
    ":param image: A numpy image that needs prediction\n",
    ":param predicted_key_pts: The keypoints predicted by the model\n",
    ":param gt_pts: The target keypoints (ground truth)\n",
    "\"\"\"\n",
    "def show_all_keypoints(image, predicted_key_pts, gt_pts=None):\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.scatter(predicted_key_pts[:, 0], predicted_key_pts[:, 1], s=20, marker='.', c='m')\n",
    "    if gt_pts is not None:\n",
    "        plt.scatter(gt_pts[:, 0], gt_pts[:, 1], s=20, marker='.', c='g')\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "Visualizes the model output for a set of images\n",
    "\n",
    ":param test_images: The images that need prediction\n",
    ":param test_outputs: The predicted keypoints\n",
    ":param gt_pts: The target keypoints (ground truth)\n",
    ":param batch_size: The batch size to use in the model\n",
    "\"\"\"\n",
    "def visualize_output(test_images, test_outputs, gt_pts=None, batch_size=30):\n",
    "    f = plt.figure(figsize=(18, 5))\n",
    "\n",
    "    for i in range(1, columns*rows +1):\n",
    "        f.add_subplot(2, 6, i)\n",
    "        \n",
    "        image = test_images[i].data\n",
    "        image = image.numpy()\n",
    "        image = np.transpose(image, (1, 2, 0))\n",
    "\n",
    "        predicted_key_pts = test_outputs[i].data\n",
    "        predicted_key_pts = predicted_key_pts.numpy()\n",
    "        \n",
    "        predicted_key_pts = predicted_key_pts*50.0+100\n",
    "        \n",
    "        ground_truth_pts = None\n",
    "        if gt_pts is not None:\n",
    "            ground_truth_pts = gt_pts[i]         \n",
    "            ground_truth_pts = ground_truth_pts*50.0+100\n",
    "        \n",
    "        show_all_keypoints(np.squeeze(image), predicted_key_pts, ground_truth_pts)\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "test_images, test_outputs, gt_pts = net_sample_output(test_loader)\n",
    "print(\"Test images size: \", test_images.data.size())\n",
    "print(\"Test output size: \",test_outputs.data.size())\n",
    "print(\"Ground truth size: \", gt_pts.size())\n",
    "\n",
    "visualize_output(test_images, test_outputs, gt_pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a8591f",
   "metadata": {},
   "source": [
    "## Loss & Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32c6b2b",
   "metadata": {},
   "source": [
    "* A regression function - MSE - was used to minimize the distance between the target and predicted points. This loss function provided good results. L1 and L1 Smooth losses were used but no advantage was observed.\n",
    "* The NaimishNet paper uses Adam optimizer with default parameters, I incorporated this strategy after experimenting with different optmizers, such as SGD, and different learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ebe25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trains the model with a set of training images\n",
    "\n",
    ":param n_epochs: The number of epochs to train the model\n",
    ":param train_loader: A DataLoader object with a set of training images\n",
    "\"\"\"\n",
    "def train_net(n_epochs, train_loader):\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch_i, data in enumerate(train_loader):\n",
    "            \n",
    "            # get the input images and their corresponding labels\n",
    "            images = data['image']\n",
    "            key_pts = data['keypoints']\n",
    "\n",
    "            # flatten pts\n",
    "            key_pts = key_pts.view(key_pts.size(0), -1)\n",
    "\n",
    "            # convert variables to floats for regression loss\n",
    "            key_pts = key_pts.type(torch.FloatTensor)\n",
    "            images = images.type(torch.FloatTensor)\n",
    "\n",
    "            # forward pass to get outputs\n",
    "            output_pts = net(images)\n",
    "\n",
    "            # calculate the loss between predicted and target keypoints\n",
    "            loss = criterion(output_pts, key_pts)\n",
    "\n",
    "            # zero the parameter (weight) gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # backward pass to calculate the weight gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # print loss statistics\n",
    "            running_loss += loss.item()\n",
    "            if batch_i % 10 == 9:    # print every 10 batches\n",
    "                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, batch_i+1, running_loss/10))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8cc252",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07876c5",
   "metadata": {},
   "source": [
    "I took into consideration the training speed and error margin when training the model. Number of epochs was experimented with 1, 5, 30, 50, 100 and 150 iterations. In the end 150 epochs and batch size of 30 provided good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04571b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 150\n",
    "train_net(n_epochs, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254384c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_outputs, gt_pts = net_sample_output(test_loader)\n",
    "print(test_images.data.size())\n",
    "print(test_outputs.data.size())\n",
    "print(gt_pts.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6c762c",
   "metadata": {},
   "source": [
    "## Visualize predicted keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd777b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_output(test_images, test_outputs, gt_pts=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b891cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_output(test_images, test_outputs, gt_pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa2c4dd",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f09ef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Net\n",
    "\n",
    "model_dir = 'saved_models/'\n",
    "model_name = 'model_net.pt'\n",
    "torch.save(net.state_dict(), model_dir+model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061dcde7",
   "metadata": {},
   "source": [
    "## Visualize and apply a filter from the trained CNN to an image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c470b56",
   "metadata": {},
   "source": [
    "Below you will visualize one of CNN filters applied to an image in the dataset. This particular filter seems to preserve the edges of an object that is surrounded by dark pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dbdc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "\n",
    "test_images, test_outputs, gt_pts = net_sample_output(train_loader)\n",
    "image = test_images[10].data\n",
    "image = image.numpy()   # convert to numpy array from a Tensor\n",
    "image = np.transpose(image, (1, 2, 0))   # transpose to go from torch to numpy image\n",
    "\n",
    "cnn_weights = net.state_dict()['features.9.weight'].cpu()\n",
    "w = cnn_weights.numpy()\n",
    "\n",
    "f = plt.figure(figsize=(35, 20))\n",
    "columns = 6\n",
    "rows = 2\n",
    "\n",
    "kernel = w[0][0]\n",
    "\n",
    "filtered_image = cv2.filter2D(image, -1, kernel)\n",
    "\n",
    "f.add_subplot(rows, columns, 1)\n",
    "plt.imshow(kernel, cmap='gray')\n",
    "\n",
    "f.add_subplot(rows, columns, 2)\n",
    "plt.imshow(np.squeeze(image), cmap='gray')\n",
    "\n",
    "f.add_subplot(rows, columns, 3)\n",
    "plt.imshow(np.squeeze(filtered_image), cmap='gray')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
