{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094d9825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt') # one time execution\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import unicodedata\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f491e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z %#+_]')\n",
    "STOPWORDS = set(stopwords.words('portuguese'))\n",
    "\n",
    "def strip_accents(text):\n",
    "\n",
    "    text = unicodedata.normalize('NFD', str(text))\\\n",
    "           .encode('ascii', 'ignore')\\\n",
    "           .decode(\"utf-8\")\n",
    "\n",
    "    return str(text)\n",
    "\n",
    "STOPWORDS = [strip_accents(w) for w in STOPWORDS]\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = strip_accents(text) # remove accents\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    text = text.replace('\\W', '')\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a181425",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f429aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('../data/data_raw.csv')\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef8309f",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b02733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['TEXT'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e28eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(df_raw['TEXT'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757b8f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc70bdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['TEXT_CLEAN'] = df_clean['TEXT'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e2b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b17ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_clean.drop('TEXT', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79e17bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv('../data/data_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de810cc4",
   "metadata": {},
   "source": [
    "# New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8af01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = df_clean.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6cc28d",
   "metadata": {},
   "source": [
    "## New Feature - Number of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752dc6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat['NUM_STOPWORDS'] = df_feat['TEXT'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "print('maximum of num_stopwords in data',df_feat[\"NUM_STOPWORDS\"].max())\n",
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97495b95",
   "metadata": {},
   "source": [
    "## New Feature - Number of punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef25709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "df_feat['NUM_PUNCTUATIONS'] = df_feat['TEXT'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "print('maximum of num_punctuations in data',df_feat[\"NUM_PUNCTUATIONS\"].max())\n",
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4439f8f1",
   "metadata": {},
   "source": [
    "## New Feature - Number of title case words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e086ac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat['NUM_WORDS_UPPER'] = df_feat['TEXT'].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "print('maximum of num_words_upper in data',df_feat[\"NUM_WORDS_UPPER\"].max())\n",
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3bd274",
   "metadata": {},
   "source": [
    "## New Feature - Number of chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafc25e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat['NUM_CHARS'] = df_feat['TEXT'].str.len()\n",
    "print('maximum of num_chars in data',df_feat[\"NUM_CHARS\"].max())\n",
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d04cf88",
   "metadata": {},
   "source": [
    "## New Feature - Number of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d1cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat['NUM_WORDS'] = df_feat['TEXT'].str.split(' ').str.len()\n",
    "print('maximum of num_words in data',df_feat[\"NUM_WORDS\"].max())\n",
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28bc186",
   "metadata": {},
   "source": [
    "## New Feature - Number of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ec51ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_feat['NUM_NUMS'] = df_feat['TEXT'].str.split(' ').str.len()\n",
    "#print('maximum of num_nums in data',df_feat[\"NUM_NUMS\"].max())\n",
    "#df_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d5fb64",
   "metadata": {},
   "source": [
    "## New Feature - Average word length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1102c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "    if type(sentence) != str:\n",
    "        return 0\n",
    "    words = sentence.split()\n",
    "    return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "df_feat['AVG_WORD'] = df_feat['TEXT'].apply(avg_word)\n",
    "print('maximum of avg_word in data',df_feat[\"AVG_WORD\"].max())\n",
    "df_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cfb089",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat.to_csv('../data/data_feat.csv', index=False)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
