{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d240815",
   "metadata": {},
   "source": [
    "## Data Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84335136",
   "metadata": {},
   "source": [
    "This notebook contains the necessary data wrangling processes to format data acquired from environment sensors. \n",
    "* Video codes\n",
    "* IMU data streams\n",
    "* Empatica biosignals\n",
    "```\n",
    "compiled_data = {\n",
    "     user1: {\n",
    "         video: \n",
    "             web: \"adkada/akdlakds/asdkad.webm\"\n",
    "             mp4: \"video.mp4\"\n",
    "             codes: \"codes.json\"\n",
    "         sensors: \n",
    "             iron: \n",
    "                 imu: \"imu.json\"\n",
    "             bio: \n",
    "                 acc: acc.json\n",
    "                 ...\n",
    "     }\n",
    " }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca2749",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_data = {}\n",
    "import glob, os, json, platform\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def save_jsonfile(fn, data):\n",
    "    file = fn\n",
    "    with open(file, 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "    print(\"File saved!\", file)\n",
    "    \n",
    "def create_base(name, actor, sensor, channel):\n",
    "    \n",
    "    if not name in compiled_data: \n",
    "        compiled_data[name]= {}\n",
    "    if not actor in compiled_data[name]:\n",
    "        compiled_data[name][actor] = {}\n",
    "    if not sensor in compiled_data[name][actor]:\n",
    "        compiled_data[name][actor][sensor] = {}\n",
    "    if channel:\n",
    "        if not channel in compiled_data[name][actor][sensor]:\n",
    "            compiled_data[name][actor][sensor][channel] = {}\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030112ba",
   "metadata": {},
   "source": [
    "## Video Codes\n",
    "Root directory `VIDEO_ROOT = data/video_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844cc09",
   "metadata": {},
   "source": [
    "### Routine\n",
    "Processing video obtained from the session should be done as follows:\n",
    "1. [TODO] Run a ffmpeg batch script to convert `VIDEO_ROOT/raw` to optimized `MP4` and store in `VIDEO_ROOT/processed`.\n",
    "2. [TODO] Run a ffmpeg batch script to convert `VIDEO_ROOT/processed` to `WEBM` and store in `VIDEO_ROOT/web`. Should be a lower resolution [TODO] ... 720?\n",
    "3. In MaxQDA, code the video with the following code system `TODO`\n",
    "4. Activate all codes and export. Remove redundant columns in Excel and save as `VIDEO_ROOT/video_data.csv`\n",
    "\n",
    "\n",
    "```\n",
    "brew install ffmpeg --with-libvpx --with-vorbis --with-libvorbis --with-vpx --with-vorbis --with-theora --with-libogg --with-libvorbis --with-gpl --with-version3 --with-nonfree --with-postproc --with-libaacplus --with-libass --with-libcelt --with-libfaac --with-libfdk-aac --with-libfreetype --with-libmp3lame --with-libopencore-amrnb --with-libopencore-amrwb --with-libopenjpeg --with-openssl --with-libopus --with-libschroedinger --with-libspeex --with-libtheora --with-libvo-aacenc --with-libvorbis --with-libvpx --with-libx264 --with-libxvid\n",
    "```\n",
    "\n",
    "Use [ffmpeg](https://gist.github.com/clayton/6196167) to convert videos\n",
    "```\n",
    "ffmpeg -i cesar.mp4 -c:v libvpx-vp9 -pass 2 -b:v 0 -crf 33 -threads 8 -speed 2 -tile-columns 6 -frame-parallel 1 -auto-alt-ref 1 -lag-in-frames 25  -f webm cesar.webm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc3bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_ROOT = \"data/video_data/\"\n",
    "WEB = \"web/*.webm\"\n",
    "RAW = \"raw/*.MOV\"\n",
    "MP4 = \"mp4/*.mp4\"\n",
    "\n",
    "import datetime\n",
    "\n",
    "compiled_data = {}\n",
    "def process_video(RAW, MP4):\n",
    "    #SKIP PROCESSED FILES\n",
    "    #RUN FFMPEG SCRIPT\n",
    "    pass\n",
    "\n",
    "\n",
    "def append_data(root, directory, actor, sensor, channel, type):\n",
    "    videos = glob.glob(root + directory)\n",
    "    for i in videos:\n",
    "        if platform.system() == \"Darwin\":\n",
    "            posix_time = os.stat(i).st_birthtime\n",
    "            t = datetime.datetime.fromtimestamp(posix_time).strftime(\n",
    "                '%Y-%m-%dT%H:%M:%SZ')\n",
    "            session = os.path.basename(i).split('.')[0]\n",
    "            url = \"/\" + i\n",
    "            \n",
    "            create_base(session, actor, sensor, channel)\n",
    "            \n",
    "            sensor_data = {'url': url, 'timestamp': posix_time, 'c_time': t, 'type': type}\n",
    "            compiled_data[session][actor][sensor][channel] = sensor_data\n",
    "\n",
    "\n",
    "append_data(VIDEO_ROOT, RAW, \"env\", \"video\", \"raw\", \"link\")\n",
    "append_data(VIDEO_ROOT, MP4, \"env\", \"video\", \"mp4\", \"video\")\n",
    "append_data(VIDEO_ROOT, WEB, \"env\", \"video\", \"web\", \"link\")\n",
    "\n",
    "print(json.dumps(compiled_data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d8edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXQDA_OUTPUT=\"codes.csv\"\n",
    "CODES_ROOT=\"codes/\"\n",
    "file = VIDEO_ROOT + CODES_ROOT + MAXQDA_OUTPUT\n",
    "print(file)\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "start = pd.to_datetime(df['Begin'])\n",
    "end = pd.to_datetime(df['End'])\n",
    "t0 = start[0]\n",
    "print(t0)\n",
    "df['t_i'] = (start - t0).dt.total_seconds().astype(int) # Don't need millisecond precision for hand-coded codes\n",
    "df['t_f'] = (end - t0).dt.total_seconds().astype(int)\n",
    "df.drop(['Begin', 'End'], 1)\n",
    "\n",
    "data = {}\n",
    "for index, row in df.iterrows():\n",
    "    user = row[0]\n",
    "    codes = row[1].split(\"\\\\\")\n",
    "    if not user in data: \n",
    "        data[user] = []\n",
    "    codes2 = list(map(lambda x: x.lower(), codes[2:]))\n",
    "    data[user].append({\n",
    "        'codes': codes2,\n",
    "        'start': row[4],\n",
    "        'end': row[5],\n",
    "        'actor': codes[1].lower()\n",
    "    })\n",
    "    \n",
    "for user in data:\n",
    "    file = VIDEO_ROOT + CODES_ROOT + user + \".json\"\n",
    "    save_jsonfile(file, data[user])\n",
    "append_data(VIDEO_ROOT, CODES_ROOT + \"*.json\", \"env\", \"video\",\"codes\", \"codes_chart\")\n",
    "print(json.dumps(compiled_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e035a23",
   "metadata": {},
   "source": [
    "### MaxQDA codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e435252e",
   "metadata": {},
   "source": [
    "## IMU_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658e84d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather sensor files\n",
    "SENSOR_ROOT = \"data/sensor_data/\"\n",
    "IMU = \"*.json\"\n",
    "append_data(SENSOR_ROOT, IMU, \"iron\", \"imu\", \"various\", \"zip\")\n",
    "# print(json.dumps(compiled_data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc8099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DO NOT RUN AGAIN\n",
    "# # #Separate channels within IMU\n",
    "# imu_files = SENSOR_ROOT + IMU\n",
    "\n",
    "# videos = glob.glob(imu_files)\n",
    "# for i in videos:\n",
    "#     if platform.system() == \"Darwin\":\n",
    "#         posix_time = os.stat(i).st_birthtime\n",
    "#         t = datetime.datetime.fromtimestamp(posix_time).strftime(\n",
    "#             '%Y-%m-%dT%H:%M:%SZ')\n",
    "#         name = os.path.basename(i).split('.')[0]\n",
    "#         file = {\n",
    "#             'name': name,\n",
    "#             'timestamp': posix_time,\n",
    "#             'c_time': t,\n",
    "#             'type': \"motion\"\n",
    "#         }\n",
    "        \n",
    "        \n",
    "#         f = open(i, 'r')\n",
    "#         data = json.load(f)\n",
    "#         for k in data: \n",
    "#             data[k]['timestamp'] = posix_time\n",
    "#             data[k]['c_time'] = t\n",
    "#         f.close()\n",
    "        \n",
    "#         # Re-open file here\n",
    "#         f2 = open(i, 'w')\n",
    "#         json.dump(data, f2)\n",
    "#         f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2252c8bc",
   "metadata": {},
   "source": [
    "## Bio_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e41b0",
   "metadata": {},
   "source": [
    ".csv files in this archive are in the following format:\n",
    "The first row is the initial time of the session expressed as unix timestamp in UTC.\n",
    "The second row is the sample rate expressed in Hz.\n",
    "\n",
    "### temp.csv\n",
    "Data from temperature sensor expressed degrees on the Celsius (°C) scale.\n",
    "\n",
    "### EDA.csv\n",
    "Data from the electrodermal activity sensor expressed as microsiemens (μS).\n",
    "\n",
    "### BVP.csv\n",
    "Data from photoplethysmograph.\n",
    "\n",
    "### ACC.csv\n",
    "Data from 3-axis accelerometer sensor. The accelerometer is configured to measure acceleration in the range [-2g, 2g]. Therefore the unit in this file is 1/64g.\n",
    "Data from x, y, and z axis are respectively in first, second, and third column.\n",
    "\n",
    "### IBI.csv\n",
    "Time between individuals heart beats extracted from the BVP signal.\n",
    "No sample rate is needed for this file.\n",
    "The first column is the time (respect to the initial time) of the detected inter-beat interval expressed in seconds (s).\n",
    "The second column is the duration in seconds (s) of the detected inter-beat interval (i.e., the distance in seconds from the previous beat).\n",
    "\n",
    "### HR.csv\n",
    "Average heart rate extracted from the BVP signal.The first row is the initial time of the session expressed as unix timestamp in UTC.\n",
    "The second row is the sample rate expressed in Hz.\n",
    "\n",
    "### tags.csv\n",
    "Event mark times.\n",
    "Each row corresponds to a physical button press on the device; the same time as the status LED is first illuminated.\n",
    "The time is expressed as a unix timestamp in UTC and it is synchronized with initial time of the session indicated in the related data files from the corresponding session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2032d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIO_ROOT = \"data/bio_data/\"\n",
    "\n",
    "videos = glob.glob(BIO_ROOT + \"*\")\n",
    "\n",
    "def grab_and_save_data(user, sensor, name, file, df, columns):\n",
    "    timestamp = df.columns[0]\n",
    "    sampling_rate = df.iloc[[0]][timestamp][0].astype(int)\n",
    "    df = df.iloc[1:]\n",
    "    data = {\n",
    "        'name': name,\n",
    "        'timestamp': int(float(timestamp)),\n",
    "        'sampling_rate': int(sampling_rate)\n",
    "    }\n",
    "\n",
    "    for i in range(len(columns)):\n",
    "        name = columns[i]\n",
    "        data[name] = df.iloc[:,i].values.tolist()\n",
    "\n",
    "    save_jsonfile(file, data)\n",
    "    if user not in compiled_data:\n",
    "        compiled_data[user] = {'bio': {}}\n",
    "    if 'bio' not in compiled_data[user]:\n",
    "        compiled_data[user]['bio'] = {}\n",
    "    compiled_data[user]['bio'][sensor] = \"/\"+file\n",
    "        \n",
    "for session in videos:\n",
    "    bio_data = glob.glob(session + \"/*.csv\")\n",
    "    user = os.path.basename(session)\n",
    "    for sensor in bio_data: \n",
    "        sensor_name = name = os.path.basename(sensor).split('.')[0].lower()\n",
    "        print(sensor)\n",
    "        try: \n",
    "            df = pd.read_csv(sensor)\n",
    "        except pd.io.common.EmptyDataError:\n",
    "            print(\"Empty file:\", sensor)\n",
    "            continue\n",
    "        \n",
    "        # CONVERT CSV FILES INTO JSON         \n",
    "        file = session + \"/\" + sensor_name + \".json\"\n",
    "        \n",
    "        # SENSOR SPECIFIC PARSING\n",
    "        if sensor_name == \"temp\":\n",
    "            grab_and_save_data(user, sensor_name, \"Temperature (C)\", file, df, ['celsius'])\n",
    "        if sensor_name == \"tags\":\n",
    "            pass\n",
    "        if sensor_name == \"acc\":\n",
    "            grab_and_save_data(user, sensor_name, \"3-Axis Accelerometer (1/64g)\", file, df, ['x', 'y', 'z'])\n",
    "        if sensor_name == \"eda\":\n",
    "            grab_and_save_data(user, sensor_name, \"Electrodermal Activity (μS)\", file, df, ['mag'])\n",
    "        if sensor_name == \"bvp\":\n",
    "            grab_and_save_data(user, sensor_name, \"Blood Volume Pulse (BVP) from PPG\", file, df, ['mag'])\n",
    "        if sensor_name == \"hr\":\n",
    "            grab_and_save_data(user, sensor_name, \"Heart rate\", file, df, ['mag'])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70b3fe3",
   "metadata": {},
   "source": [
    "## Save compiled data as a JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ab4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "COMPILED_DATA = \"data/compiled.json\"\n",
    "save_jsonfile(COMPILED_DATA, compiled_data)\n",
    "print(json.dumps(compiled_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853c1249",
   "metadata": {},
   "source": [
    "### Copy to Rails App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7037259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN BASH SCRIPT\n",
    "#DO NOT COPY LARGE VIDEO FILES\n",
    "import subprocess\n",
    "print(\"start\")\n",
    "output = subprocess.call(\"bash transfer.sh\", shell=True)\n",
    "a = subprocess.Popen(\"ls\",shell=True)\n",
    "print(output)\n",
    "print(\"end\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
