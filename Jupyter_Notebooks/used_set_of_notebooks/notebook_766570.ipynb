{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd4164b3",
   "metadata": {},
   "source": [
    "GP Regression with GPflow\n",
    "--\n",
    "\n",
    "*James Hensman, 2015, 2016*\n",
    "\n",
    "GP regression (with Gaussian noise) is the most straightforward GP model in GPflow. Because of the conjugacy of the latent process and the noise, the marginal likelihood $p(\\mathbf y\\,|\\,\\theta)$ can be computed exactly.\n",
    "\n",
    "This notebook shows how to build a GPR model, estimate the parameters $\\theta$ by both maximum likelihood and MCMC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53788383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPflow\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaf0b2d",
   "metadata": {},
   "source": [
    "First build a simple data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fd89f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 12\n",
    "X = np.random.rand(N,1)\n",
    "Y = np.sin(12*X) + 0.66*np.cos(25*X) + np.random.randn(N,1)*0.1 + 3\n",
    "plt.plot(X, Y, 'kx', mew=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149dc85d",
   "metadata": {},
   "source": [
    "### Model construction\n",
    "\n",
    "A GPflow model is created by instantiating one of the GPflow model classes, in this case GPR. we'll make a kernel `k` and instantiate a GPR object using the generated data and the kernel. We'll set the variance of the likelihood to a sensible initial guess, too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9836d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = GPflow.kernels.Matern52(1, lengthscales=0.3)\n",
    "m = GPflow.gpr.GPR(X, Y, kern=k)\n",
    "m.likelihood.variance = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64151979",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "GPflow models have several prediction methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c429a38e",
   "metadata": {},
   "source": [
    " - `m.predict_f` returns the mean and variance of the latent function (f) at the points Xnew. \n",
    "\n",
    " - `m.predict_f_full_cov` additionally returns the full covariance matrix of the prediction.\n",
    "\n",
    " - `m.predict_y` returns the mean and variance of a new data point (i.e. includes the noise varaince). In the case of non-Gaussian likelihoods, the variance is computed by (numerically) integrating the non-Gaussian likelihood. \n",
    "\n",
    " - `m.predict_f_samples` returns samples of the latent function\n",
    "\n",
    " - `m.predict_density` returns the log-density of the points Ynew at Xnew. \n",
    " \n",
    "We'll use `predict_y` to make a simple plotting function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54023903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(m):\n",
    "    xx = np.linspace(-0.1, 1.1, 100)[:,None]\n",
    "    mean, var = m.predict_y(xx)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(X, Y, 'kx', mew=2)\n",
    "    plt.plot(xx, mean, 'b', lw=2)\n",
    "    plt.fill_between(xx[:,0], mean[:,0] - 2*np.sqrt(var[:,0]), mean[:,0] + 2*np.sqrt(var[:,0]), color='blue', alpha=0.2)\n",
    "    plt.xlim(-0.1, 1.1)\n",
    "plot(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fb6bb9",
   "metadata": {},
   "source": [
    "### Using mean functions\n",
    "\n",
    "All GPflow models can have parameterized mean functions, some simple ones are provided in `GPflow.mean_functions`. Here's a model with a Linear mean function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = GPflow.kernels.Matern52(1, lengthscales=0.3)\n",
    "meanf = GPflow.mean_functions.Linear(1,0)\n",
    "m = GPflow.gpr.GPR(X, Y, k, meanf)\n",
    "m.likelihood.variance = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea614d1",
   "metadata": {},
   "source": [
    "### Maximum Likelihood estimation of $\\theta$\n",
    "\n",
    "Getting the ML estimate of the parameters is a simple call to `optimize()`. By default, GPflow plugs into the L-BFGS-B algorithm via scipy. Here are the parameters before optimization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb587b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff55ea59",
   "metadata": {},
   "source": [
    "Here are the parameters after optimization, and a new plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.optimize()\n",
    "plot(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efc2c23",
   "metadata": {},
   "source": [
    "### MCMC for $\\theta$\n",
    "\n",
    "Here's a quick demonstratino of how to obtain posteriors over the hyper-parameters in GPregression. First, we'll set come priors on the kernel parameters, then we'll run MCMC and see how much posterior uncertainty there is in the parameters.\n",
    "\n",
    "First we'll choose rather arbitrary priors. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98164b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.kern.lengthscales.prior = GPflow.priors.Gamma(1., 1.)\n",
    "m.kern.variance.prior = GPflow.priors.Gamma(1., 1.)\n",
    "m.likelihood.variance.prior = GPflow.priors.Gamma(1., 1.)\n",
    "m.mean_function.A.prior = GPflow.priors.Gaussian(0., 10.)\n",
    "m.mean_function.b.prior = GPflow.priors.Gaussian(0., 10.)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81feaa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = m.sample(500, epsilon=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da52cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "plt.plot(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54decbf9",
   "metadata": {},
   "source": [
    "The samples are in the same form as the sampler runs (i.e. in the unconstrained space, with and fixed parameters removed). Each row is one sample, and each column is one free variable. GPflow can convert the samples into a nice DataFrame, which is much more interpretable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5682dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = m.get_samples_df(samples)\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521cef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a more informative plot\n",
    "plt.figure(figsize=(16, 4))\n",
    "for lab, s in sample_df.iteritems():\n",
    "    plt.plot(s, label=lab)\n",
    "_ = plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50f3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,3, figsize=(12,4))\n",
    "\n",
    "axs[0].plot(sample_df['model.likelihood.variance'],\n",
    "            sample_df['model.kern.variance'], 'k.', alpha = 0.15)\n",
    "axs[0].set_xlabel('noise_variance')\n",
    "axs[0].set_ylabel('signal_variance')\n",
    "\n",
    "axs[1].plot(sample_df['model.likelihood.variance'],\n",
    "            sample_df['model.kern.lengthscales'], 'k.', alpha = 0.15)\n",
    "axs[1].set_xlabel('noise_variance')\n",
    "axs[1].set_ylabel('lengthscale')\n",
    "\n",
    "axs[2].plot(sample_df['model.kern.lengthscales'],\n",
    "            sample_df['model.kern.variance'], 'k.', alpha = 0.1)\n",
    "axs[2].set_xlabel('lengthscale')\n",
    "axs[2].set_ylabel('signal_variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4b6a30",
   "metadata": {},
   "source": [
    "To plot the posterior of predictions, we'll iterate through the samples and set the model state with each samnple. then, for tht state (set of hyper-parameters) we'll draw some samples from the prediction function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66af697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the function posterior\n",
    "xx = np.linspace(-0.1, 1.1, 100)[:,None]\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, s in sample_df.iterrows():\n",
    "    m.set_parameter_dict(s)\n",
    "    f = m.predict_f_samples(xx, 1)\n",
    "    plt.plot(xx, f[0,:,:], 'b', lw=2, alpha = 0.05)\n",
    "    \n",
    "plt.plot(X, Y, 'kx', mew=2)\n",
    "_ = plt.xlim(xx.min(), xx.max())\n",
    "_ = plt.ylim(0, 6)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
