{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6de3f279",
   "metadata": {},
   "source": [
    "# Model Comparisons in Category Learning\n",
    "## Background\n",
    "* People learn cateories by recognizing category labels and relevant features (e.g., physical features and functions)\n",
    "* Many cognitive developmentalists are interested in the role of label in learning category membership, specifically, whether labels function as regular features or specially as a category marker\n",
    "* This project aims to capture the role of category label in binary category learning. Two models, Bayesian Model and Similarity Model, were employed based on the mainstream theories in cognitive sciences. Under each model, features are probabilistically associated with category membership. Two conditions were considered in each model, where the category labels are either deterministically (p=1) or probabilistically (p<1) associated with category membership.\n",
    "\n",
    "## Goals\n",
    "1. Compare the performances of the Bayesian Model and the Similarity Model in category learning\n",
    "2. Compare the effects of deterministic vs. probabilistic labels on category learning for both the Bayesian Model and the Similarity Model\n",
    "\n",
    "## Roadmap of Method\n",
    "\n",
    "<img src=\"Roadmap_Model_Comparison.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e16d6c3",
   "metadata": {},
   "source": [
    "## Set Up and Initialize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c01aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up\n",
    "%pylab inline\n",
    "from scipy import stats\n",
    "rcParams['font.size'] = 12;\n",
    "rcParams['lines.linewidth'] = 1.5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf0b634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize possible probabilities of features for generating observations\n",
    "nr_ps = 100 \n",
    "possible_ps = linspace(0,1,nr_ps); \n",
    "\n",
    "# Initialize priors for Bayesian updating\n",
    "prior_init = stats.beta.pdf(possible_ps,0.5,0.5)/nr_ps; \n",
    "prior_init = prior_init.clip(1e-10,1-1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd85b47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-defined numbers of training trials, features, and testing trials, and probabilities for labels and other features\n",
    "# For binary categorization, we consider Category A and Category B\n",
    "nr_trials = 20;\n",
    "nr_features = 6; \n",
    "p_reg_feature_a = 0.9; \n",
    "p_reg_feature_b = 0.1;\n",
    "\n",
    "p_label_a_deterministic = 1.0;\n",
    "p_label_b_deterministic = 0.0;\n",
    "p_label_a_probabilistic = 0.7;\n",
    "p_label_b_probabilistic = 0.3;\n",
    "\n",
    "test_trials = 100;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322b90d",
   "metadata": {},
   "source": [
    "## Create the Observation Generator and Generate Learning Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b723e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This generator produces stimuli with features probabilistically associated with category membership\n",
    "# It is used for generating both learning and testing datasets\n",
    "\n",
    "def obs_generator(nr_trials,nr_features,p_reg_feature,p_label):\n",
    "    observations = zeros((nr_trials,nr_features));\n",
    "    for i in range(nr_trials):\n",
    "        for j in range(nr_features):\n",
    "            \n",
    "            # Label is considered the first feature\n",
    "            if j==0:\n",
    "                observations[i][j]=stats.binom.rvs(1,p_label); \n",
    "            else:\n",
    "                observations[i][j]=stats.binom.rvs(1,p_reg_feature);\n",
    "                \n",
    "    return observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841bdf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training stimuli for Category A and Category B \n",
    "# Both conditions, deterministic label and probabilistic label, were considered\n",
    "\n",
    "obs_sample_a_determinstic = obs_generator(nr_trials,nr_features,p_reg_feature_a,p_label_a_deterministic);\n",
    "obs_sample_b_determinstic = obs_generator(nr_trials,nr_features,p_reg_feature_b,p_label_b_deterministic);\n",
    "\n",
    "obs_sample_a_probabilistic = obs_generator(nr_trials,nr_features,p_reg_feature_a,p_label_a_probabilistic);\n",
    "obs_sample_b_probabilistic = obs_generator(nr_trials,nr_features,p_reg_feature_b,p_label_b_probabilistic);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e978b",
   "metadata": {},
   "source": [
    "## The Bayesian Model\n",
    "1. Based on learning observations, update the posteriors for each features <font color = 'blue'>*(function feature_prob)* </font>\n",
    "2. For the whole learning sample, compute the posteriors <font color = 'blue'>*(function category_learning)*</font>\n",
    "3. Feed in the learning samples generated above, compute the posteriors of ps under deterministic label and probabilistic label conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db22f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to update the estimated probability that generates each feature\n",
    "\n",
    "def feature_prob(feature_obs):\n",
    "    nr_trials = len(feature_obs);\n",
    "    likelihoods = zeros((nr_trials,nr_ps));\n",
    "    posteriors = zeros((nr_trials,nr_ps));\n",
    "    prior = prior_init;\n",
    "    for i in range(nr_trials):\n",
    "        # Compute the likelihood as a function of possible p values\n",
    "        likelihoods[i] = possible_ps if feature_obs[i] else 1-possible_ps; \n",
    "        # Compute the (unnormalized) posterior\n",
    "        posteriors[i] = prior*likelihoods[i]; \n",
    "        # Normalize the posterior\n",
    "        posteriors[i]/= posteriors[i].sum();   \n",
    "        # Update: set the next prior equal to the current posterior\n",
    "        prior = posteriors[i]; \n",
    "    # Compute the Maximum A Posteriori Probability (MAP) estimated after each trial\n",
    "    p_hat = possible_ps[posteriors.argmax(1)]; \n",
    "    \n",
    "    # Plot the process of Bayesian Updating\n",
    "    colors = ['%.3f'%val for val in 1-linspace(0.1,1,nr_trials)]; \n",
    "    fig = figure(figsize=(8,3)); \n",
    "    subplot(1,2,1);\n",
    "    [plot(possible_ps,posterior,color=col) for posterior,col in zip(posteriors,colors)];\n",
    "    axis([0,1,0,0.1]);\n",
    "    xlabel(r'Values of $p$'); ylabel(r'$Pr(p)$'); title(r'Posterior over $p$');\n",
    "\n",
    "    subplot(1,2,2);\n",
    "    plot(arange(nr_trials)+1,feature_obs,'+',ms=30); \n",
    "    plot(arange(nr_trials)+1,p_hat,'o');\n",
    "    axis([0,nr_trials,0,1]);\n",
    "    xlabel(r'Trial nr'); ylabel(r'$\\hat{p}$'); title(r'MAP estimate of $p$');\n",
    "    return p_hat[nr_trials-1]\n",
    "\n",
    "# Defining the probabilistic learning function that utilizes the posterior of p for each feature after seeing all the training stimuli\n",
    "\n",
    "def category_learning(obs_sample,nr_features):\n",
    "    p_hats = zeros(nr_features);\n",
    "    for j in range(nr_features):\n",
    "        feature_obs = obs_sample[:,j];\n",
    "        p_hats[j] = feature_prob(feature_obs);\n",
    "    return p_hats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9decddf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the posterior of ps for each category under both conditions\n",
    "\n",
    "training_a_p_hats = category_learning(obs_sample_a_determinstic,nr_features);\n",
    "print ('Learned probability of features in category A with deterministic label:')\n",
    "print (training_a_p_hats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f4340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_b_p_hats = category_learning(obs_sample_b_determinstic,nr_features);\n",
    "print ('Learned probability of features in category B with deterministic label:')\n",
    "print (training_b_p_hats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c695c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_a_p_hats_pr = category_learning(obs_sample_a_probabilistic,nr_features);\n",
    "print ('Learned probability of features in category A with probabilistic label:')\n",
    "print (training_a_p_hats_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e931d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_b_p_hats_pr = category_learning(obs_sample_b_probabilistic,nr_features);\n",
    "print ('Learned probability of features in category B with probabilistic label:')\n",
    "print (training_b_p_hats_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7092d5a8",
   "metadata": {},
   "source": [
    "## The Bayesian Model\n",
    "4. With learned posterior probabilities, the likelihood of Category A vs. Category B can be calculated when the model encounters any testing stimuli <font color = 'blue'>*(function likelihood)* </font>\n",
    "5. Classification decision can be made by comparing the likelihood of Category A vs. the likelihood of Category B <font color = 'blue'>*(function decision_LLR)* </font>\n",
    "6. Classification decisions for the whole testing sample can be made <font color = 'blue'>*(function classification_LLR)* </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a77354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the likelihood of a stimulus generated from a particular category\n",
    "\n",
    "def likelihood(test_item,nr_features,training_p_hats):\n",
    "    prob = zeros(nr_features)\n",
    "    for j in range(nr_features):\n",
    "        if test_item[j]==1:\n",
    "            prob[j]=training_p_hats[j];\n",
    "        else: \n",
    "            prob[j]=1-training_p_hats[j];\n",
    "    likelihood = prod(prob);\n",
    "    return likelihood\n",
    "\n",
    "# Calculate the log likelihood ratio of the stimuli coming from Category A and coming from Category B\n",
    "# Make the classification decision\n",
    "\n",
    "def decision_LLR(test_item,nr_features,training_a_p_hats,training_b_p_hats):\n",
    "    LL_a = likelihood(test_item,nr_features,training_a_p_hats);\n",
    "    LL_b = likelihood(test_item,nr_features,training_b_p_hats);\n",
    "    LLR = log(LL_a+1e-10)-log(LL_b+1e-10)\n",
    "    if LLR>0:\n",
    "        return 1;\n",
    "    return 0  \n",
    "\n",
    "# Make classification decisions for the testing sample using the rule of log likelihood ratio\n",
    "\n",
    "def classification_LLR(test_trials,test_sample,nr_features,training_a_p_hats,training_b_p_hats):\n",
    "    category_decision_LLR = zeros(test_trials);\n",
    "    for i in range(test_trials):\n",
    "        test_item = test_sample[i,:];\n",
    "        category_decision_LLR[i] = decision_LLR(test_item,nr_features,training_a_p_hats,training_b_p_hats);\n",
    "    return category_decision_LLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f90b426",
   "metadata": {},
   "source": [
    "## The Similarity Model\n",
    "1. Calculate the prototypes for Category A and Category B <font color = 'blue'>*(function prototype)* </font>\n",
    "2. Calculate the similarities between testing stimuli and the prototypes <font color = 'blue'>*(function similarity)* </font>\n",
    "3. Make the classification decision based the similarities <font color = 'blue'>*(function decision_similarity)* </font>\n",
    "4. For the whole testing dataset, classification decisions can be maded <font color = 'blue'>*(function classification_similarity)* </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30329468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the prototype for each category\n",
    "# The prototype is calculated as the mean of training stimuli on each feature\n",
    "\n",
    "def prototype(obs_sample,nr_features,nr_trials):\n",
    "    prototype = zeros(nr_features);\n",
    "    for j in range(nr_features):\n",
    "        prototype[j] = obs_sample[:,j].sum(0)/nr_trials;\n",
    "    return prototype\n",
    "\n",
    "prototype_a_determinstic = prototype(obs_sample_a_determinstic,nr_features,nr_trials);\n",
    "prototype_b_determinstic = prototype(obs_sample_b_determinstic,nr_features,nr_trials);\n",
    "\n",
    "prototype_a_probabilistic = prototype(obs_sample_a_probabilistic,nr_features,nr_trials);\n",
    "prototype_b_probabilistic = prototype(obs_sample_b_probabilistic,nr_features,nr_trials);\n",
    "\n",
    "# Calculate the similarity between test stimulus and the category prototype\n",
    "\n",
    "def similarity(test_item,prototype,nr_features):\n",
    "    distance = sum([(test_item[j]-prototype[j])**2 for j in range(nr_features)])**0.5; # Euclidean Distance\n",
    "    similarity = e**(-distance); # transform to similarity using exponential decay function\n",
    "    return similarity\n",
    "\n",
    "# Make classification decision based on the similarities\n",
    "\n",
    "def decision_similarity(test_item,prototype_a,prototype_b,nr_features):\n",
    "    similarity_a = similarity(test_item,prototype_a,nr_features);\n",
    "    similarity_b = similarity(test_item,prototype_b,nr_features);\n",
    "    if similarity_a > similarity_b:\n",
    "        return 1;\n",
    "    if similarity_a == similarity_b:\n",
    "        return 0.5;\n",
    "    if similarity_a < similarity_b:\n",
    "        return 0;\n",
    "\n",
    "# Classify the whole testing dataset based the rule of similarity\n",
    "    \n",
    "def classification_similarity(test_trials,test_sample,prototype_a,prototype_b,nr_features):\n",
    "    category_decision_similarity = zeros(test_trials);\n",
    "    for i in range(test_trials):\n",
    "        test_item = test_sample[i,:];\n",
    "        category_decision_similarity[i] = decision_similarity(test_item,prototype_a,prototype_b,nr_features);\n",
    "    return category_decision_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416f489f",
   "metadata": {},
   "source": [
    "## Generate Testing Data Using the Observation Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6227a6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating test stimuli across all possible ps\n",
    "\n",
    "test_sample = zeros((nr_ps,test_trials,nr_features));\n",
    "for idx in range(nr_ps):\n",
    "    # Here category label is treated as a regular feature\n",
    "    test_sample[idx,:,:] = obs_generator(test_trials,nr_features,possible_ps[idx],possible_ps[idx]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8930ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for better visualization\n",
    "# This function uses spline interpolation to plot smooth curves\n",
    "\n",
    "def smooth_plot(x,y,color):\n",
    "    import scipy.interpolate as interpolate;\n",
    "    t, c, k = interpolate.splrep(x, y, s=0.5, k=4);\n",
    "    N = 100;\n",
    "    xmin, xmax = x.min(), x.max();\n",
    "    xx = linspace(xmin, xmax, N);\n",
    "    spline = interpolate.BSpline(t, c, k, extrapolate=False);\n",
    "    plot(xx, spline(xx), color);\n",
    "    grid();\n",
    "    legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f2bccd",
   "metadata": {},
   "source": [
    "## Testing The Bayesian Model\n",
    "* Posteriors are learned from training data\n",
    "* Testing data are fed in the classification algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad970d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function demonstrates the classification performance of the Bayesian Model\n",
    "\n",
    "def classification_acc_LLR(test_trials,test_sample,nr_features,training_a_p_hats,training_b_p_hats):\n",
    "    classifications = zeros((nr_ps,test_trials));\n",
    "    count = zeros(nr_ps);\n",
    "    percent = zeros(nr_ps);\n",
    "    acc = zeros(nr_ps);\n",
    "    for idx in range(nr_ps):\n",
    "        classifications[idx,:] = classification_LLR(test_trials,test_sample[idx,:,:],nr_features,training_a_p_hats,training_b_p_hats);\n",
    "        count[idx] = classifications[idx,:].sum(0);\n",
    "        percent[idx] = count[idx]/float(test_trials);\n",
    "        acc[idx]= 1- abs(possible_ps[idx]-percent[idx]);\n",
    "    return percent, acc \n",
    "\n",
    "# Model performances are computed for both deterministic labels and probabilistic labels\n",
    "A_deterministic_LLR = classification_acc_LLR(test_trials,test_sample,nr_features,training_a_p_hats,training_b_p_hats);\n",
    "A_probabilistic_LLR = classification_acc_LLR(test_trials,test_sample,nr_features,training_a_p_hats_pr,training_b_p_hats_pr);\n",
    "\n",
    "# Plot model performances\n",
    "fig = figure(figsize = (14,6));\n",
    "subplot(1,2,1);\n",
    "plot(possible_ps,A_deterministic_LLR[0],'ro',label='Deterministic Label',markersize =2);\n",
    "smooth_plot(possible_ps,A_deterministic_LLR[0],'r');\n",
    "plot(possible_ps,A_probabilistic_LLR[0],'mo',label='Probablistic Label',markersize =2);\n",
    "smooth_plot(possible_ps,A_probabilistic_LLR[0],'m');\n",
    "xlabel(r'Test Stimuli Generating from Possible $ps$');\n",
    "ylabel(r'Classification Probabilities of Category A');\n",
    "title(r'Model Sensitivity to Category A Based on Probability');\n",
    "legend(loc='best');\n",
    "axis([0,1,0,1]);\n",
    "\n",
    "subplot(1,2,2);\n",
    "plot(possible_ps,A_deterministic_LLR[1],'ro',label='Deterministic Label',markersize =2);\n",
    "smooth_plot(possible_ps,A_deterministic_LLR[1],'r');\n",
    "plot(possible_ps,A_probabilistic_LLR[1],'mo',label='Probablistic Label',markersize =2);\n",
    "smooth_plot(possible_ps,A_probabilistic_LLR[1],'m');\n",
    "xlabel(r'Test Stimuli Generating from Possible $ps$');\n",
    "ylabel(r'Classification Accuracy');\n",
    "title(r'Model Classification Accuracy Based on Probability');\n",
    "legend(loc='best');\n",
    "axis([0,1,0,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2e36dd",
   "metadata": {},
   "source": [
    "## Testing The Similarity Model\n",
    "* Prototyes are learned from training data\n",
    "* Testing data are fed in the classification algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95c715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function demonstrates the classification performances of the Similarity Model\n",
    "\n",
    "def classification_acc_similarity(test_trials,test_sample,prototype_a,prototype_b,nr_features):\n",
    "    classifications = zeros((nr_ps,test_trials));\n",
    "    count = zeros(nr_ps);\n",
    "    percent = zeros(nr_ps);\n",
    "    acc = zeros(nr_ps);\n",
    "    for idx in range(nr_ps):\n",
    "        classifications[idx,:] = classification_similarity(test_trials,test_sample[idx,:,:],prototype_a,prototype_b,nr_features);\n",
    "        count[idx] = classifications[idx,:].sum(0);\n",
    "        percent[idx] = count[idx]/float(test_trials);\n",
    "        acc[idx]= 1- abs(possible_ps[idx]-percent[idx]);\n",
    "    return percent, acc\n",
    "\n",
    "# Model performances are computed for both deterministic labels and probabilistic labels\n",
    "A_deterministic_similarity = classification_acc_similarity(test_trials,test_sample,prototype_a_determinstic,prototype_b_determinstic,nr_features);\n",
    "A_probabilistic_similarity = classification_acc_similarity(test_trials,test_sample,prototype_a_probabilistic,prototype_b_probabilistic,nr_features);\n",
    "\n",
    "# Plot model performances\n",
    "fig = figure(figsize = (14,6));\n",
    "subplot(1,2,1);\n",
    "plot(possible_ps,A_deterministic_similarity[0],'go',label='Deterministic Label', markersize =2);\n",
    "smooth_plot(possible_ps,A_deterministic_similarity[0],'g');\n",
    "plot(possible_ps,A_probabilistic_similarity[0],'bo',label='Probablistic Label', markersize =2);\n",
    "smooth_plot(possible_ps,A_probabilistic_similarity[0],'b');\n",
    "xlabel(r'Test Stimuli Generating from Possible $ps$');\n",
    "ylabel(r'Classification Probabilities of Category A');\n",
    "title(r'Model Sensitivity to Category A Based on Similarity');\n",
    "legend(loc='best');\n",
    "axis([0,1,0,1]);\n",
    "\n",
    "subplot(1,2,2);\n",
    "plot(possible_ps,A_deterministic_similarity[1],'go',label='Deterministic Label', markersize =2);\n",
    "smooth_plot(possible_ps,A_deterministic_similarity[1],'g');\n",
    "plot(possible_ps,A_probabilistic_similarity[1],'bo',label='Probablistic Label', markersize =2);\n",
    "smooth_plot(possible_ps,A_probabilistic_similarity[1],'b');\n",
    "xlabel(r'Test Stimuli Generating from Possible $ps$');\n",
    "ylabel(r'Classification Accuracy');\n",
    "title(r'Model Classification Accuracy Based on Similarity');\n",
    "legend(loc='best');\n",
    "axis([0,1,0,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb11c7f",
   "metadata": {},
   "source": [
    "## Visual Comparison of the Bayesian Model and the Similarity Model with Deterministic and Probabilistic Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db5c022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For better visual comparison, plotting the performances of the two models together\n",
    "\n",
    "fig = figure(figsize = (14,6));\n",
    "subplot(1,2,1);\n",
    "plot(possible_ps,A_deterministic_LLR[0],'ro',label='Bayesian Model - Deterministic Label',markersize =2);\n",
    "smooth_plot(possible_ps,A_deterministic_LLR[0],'r');\n",
    "plot(possible_ps,A_probabilistic_LLR[0],'mo',label='Bayesian Model - Probablistic Label',markersize =2);\n",
    "smooth_plot(possible_ps,A_probabilistic_LLR[0],'m');\n",
    "plot(possible_ps,A_deterministic_similarity[0],'go',label='Similarity Model - Deterministic Label', markersize =2);\n",
    "smooth_plot(possible_ps,A_deterministic_similarity[0],'g');\n",
    "plot(possible_ps,A_probabilistic_similarity[0],'bo',label='Similarity Model - Probablistic Label', markersize =2);\n",
    "smooth_plot(possible_ps,A_probabilistic_similarity[0],'b');\n",
    "plot([0,0.6,1], [0,0.6,1], 'k-', lw=1,dashes=[2, 2]);\n",
    "xlabel(r'Test Stimuli Generating from Possible $ps$');\n",
    "ylabel(r'Classification Probabilities of Category A');\n",
    "title(r'Model Sensitivity to Category A Based on Similarity');\n",
    "legend(loc='best');\n",
    "axis([0,1,0,1]);\n",
    "\n",
    "subplot(1,2,2);\n",
    "plot(possible_ps,A_deterministic_LLR[1],'ro',label='Bayesian Model - Deterministic Label',markersize =2);\n",
    "smooth_plot(possible_ps,A_deterministic_LLR[1],'r');\n",
    "plot(possible_ps,A_probabilistic_LLR[1],'mo',label='Bayesian Model - Probablistic Label',markersize =2);\n",
    "smooth_plot(possible_ps,A_probabilistic_LLR[1],'m');\n",
    "plot(possible_ps,A_deterministic_similarity[1],'go',label='Similarity Model - Deterministic Label', markersize =2);\n",
    "smooth_plot(possible_ps,A_deterministic_similarity[1],'g');\n",
    "plot(possible_ps,A_probabilistic_similarity[1],'bo',label='Similarity Model - Probablistic Label', markersize =2);\n",
    "smooth_plot(possible_ps,A_probabilistic_similarity[1],'b');\n",
    "xlabel(r'Test Stimuli Generating from Possible $ps$');\n",
    "ylabel(r'Classification Accuracy');\n",
    "title(r'Model Classification Accuracy Based on Similarity');\n",
    "legend(loc='best');\n",
    "axis([0,1,0,1]);"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
