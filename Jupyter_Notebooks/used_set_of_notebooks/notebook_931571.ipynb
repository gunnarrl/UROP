{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "896747cb",
   "metadata": {},
   "source": [
    "# PyTorch DataLoaders\n",
    "\n",
    "\n",
    "As an example dataset the [old kaggle competition](https://www.kaggle.com/c/dogs-vs-cats) is used. It is a classification problem with two target classes: cats vs dogs.\n",
    "\n",
    "Please, group the images of different classes in different folders. You can use the following bash script:\n",
    "```\n",
    "unzip train.zip\n",
    "\n",
    "cd train\n",
    "\n",
    "mkdir cats\n",
    "mkdir dogs\n",
    "\n",
    "find . -name \"cat*.jpg\" -exec mv {} ./cats \\;\n",
    "find . -name \"dog*.jpg\" -exec mv {} ./dogs \\;\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec326ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268d580e",
   "metadata": {},
   "source": [
    "\n",
    "__Dataset__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af47a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data'\n",
    "\n",
    "# ignore it for now\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                transforms.ToTensor()])\n",
    "\n",
    "dataset = datasets.ImageFolder(data_dir,\n",
    "                               transform=transform)\n",
    "# ignore it for now\n",
    "dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                         batch_size=1,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cbb13e",
   "metadata": {},
   "source": [
    "To display the image properly we need to cast some numpy magic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e34d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(dataloader):\n",
    "    image = np.rollaxis(next(iter(dataloader))[0][0].detach().numpy(), 0, 3)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb52ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(get_image(dataloader), interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf7cb80",
   "metadata": {},
   "source": [
    "__Transformations__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f6a43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "expample_transforms = transforms.Compose([transforms.RandomRotation(90),\n",
    "                                          transforms.RandomResizedCrop(224),\n",
    "                                          transforms.ToTensor()]) \n",
    "\n",
    "transformed_data = datasets.ImageFolder(data_dir, transform=expample_transforms)\n",
    "\n",
    "# ignore it for now\n",
    "transformed_dataloader = torch.utils.data.DataLoader(transformed_data,\n",
    "                                                     batch_size=1,\n",
    "                                                     shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c447c966",
   "metadata": {},
   "source": [
    "Let's see what we have got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0c9b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(10, 30))\n",
    "for i in range(3):\n",
    "    ax[i].imshow(get_image(transformed_dataloader), interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0db9238",
   "metadata": {},
   "source": [
    "__Data Loaders__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cecf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "expample_transforms = transforms.Compose([transforms.RandomRotation(15),\n",
    "                                          transforms.RandomResizedCrop(224),\n",
    "                                          transforms.ToTensor()]) \n",
    "\n",
    "transformed_data = datasets.ImageFolder(data_dir, transform=expample_transforms)\n",
    "\n",
    "final_dataloader = torch.utils.data.DataLoader(transformed_data,\n",
    "                                               batch_size=32,\n",
    "                                               shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f030804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_example, labels_example = next(iter(final_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a74c6d",
   "metadata": {},
   "source": [
    "We have got bath of size 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9496da6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_example.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09205a52",
   "metadata": {},
   "source": [
    "Label values are based on the path in the provided image folder (2 folders with images: Cats and Dogs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d09825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b4c9aa",
   "metadata": {},
   "source": [
    "__Data augmentation__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a628fbd9",
   "metadata": {},
   "source": [
    "At the end of this notebook, please define your transformation using the `Lambda` class and apply it to the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db303ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_tranformation_function(torch_tensor):\n",
    "    \"\"\"\n",
    "        Simply inverse the colormap of the image (actually, it's a tensor)\n",
    "    \"\"\"\n",
    "    return 1. - torch_tensor\n",
    "\n",
    "example_transform = transforms.Lambda(example_tranformation_function)\n",
    "transforms = transforms.Compose([transforms.RandomRotation(15),\n",
    "                                          transforms.CenterCrop(224),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          example_transform\n",
    "                                         ]) \n",
    "\n",
    "transformed_data = datasets.ImageFolder(data_dir, transform=transforms)\n",
    "\n",
    "final_dataloader = torch.utils.data.DataLoader(transformed_data,\n",
    "                                               batch_size=1,\n",
    "                                               shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aff5335",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(get_image(final_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592118a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_transformation_function(data):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    \n",
    "# Add your transformation to the pipeline and check the final image."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
