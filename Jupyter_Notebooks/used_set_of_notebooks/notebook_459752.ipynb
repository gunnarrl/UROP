{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8bbe13a",
   "metadata": {},
   "source": [
    "run everything with BAN (in MICCAI19-MedVQA):\n",
    "\n",
    "python3 test.py --model BAN --use_RAD --RAD_dir data_RAD --maml --autoencoder --input saved_models/BAN_MEVF --epoch 19 --output results/BAN_MEVF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b3860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./MICCAI19-MedVQA\")\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import dataset_RAD\n",
    "import base_model\n",
    "import utils\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from bunch import Bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d0124",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_RAD = './MICCAI19-MedVQA/data_RAD'\n",
    "model_path = './MICCAI19-MedVQA/BAN_MEVF'\n",
    "batch_size = 1\n",
    "constructor = 'build_BAN'\n",
    "rnn = 'LSTM'\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95089c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = dataset_RAD.Dictionary.load_from_file(os.path.join(data_RAD , 'dictionary.pkl'))\n",
    "\n",
    "args = Bunch()\n",
    "args.RAD_dir = data_RAD\n",
    "args.autoencoder = True\n",
    "args.maml = False\n",
    "args.autoencoder = True\n",
    "args.feat_dim = 64\n",
    "args.op = 'c'\n",
    "args.num_hid = 1024\n",
    "args.rnn = rnn\n",
    "args.gamma = 2\n",
    "args.ae_model_path = 'pretrained_ae.pth'\n",
    "args.maml_model_path = 'pretrained_maml.weights'\n",
    "args.activation = 'relu'\n",
    "args.dropout = 0.5\n",
    "args.maml = True\n",
    "args.eps_cnn = 1e-5\n",
    "args.momentum_cnn = 0.05\n",
    "\n",
    "# There is a dataset implementation that handles the pipeline (including tokenization and tensor formatting)\n",
    "eval_dset = dataset_RAD.VQAFeatureDataset('test', args, dictionary)\n",
    "\n",
    "model = base_model.build_BAN(eval_dset, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6943a9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dataloader for the dataset\n",
    "eval_loader = DataLoader(eval_dset, batch_size, shuffle=False, num_workers=0, pin_memory=True, collate_fn=utils.trim_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b22e12",
   "metadata": {},
   "source": [
    "Each VQA input question is trimmed to a 12-word sentence. The question is zero-padded in case its length is less than 12. Each word is represented by a 600D vector which is a concatenation of the 300D GloVe word embeddings.\n",
    "\n",
    "The word embeddings is fed into a 1024D LSTM in order to produce the question embeddings [1].\n",
    "\n",
    "The model contains two convolutional modules - a denoising autoencoder (DAE) and a model-agnostic meta-learning (MAML) algorithm trained with meta-tasks for better representations. More on section 3.1 and 3.2 from [1].\n",
    "\n",
    "[1]: https://arxiv.org/pdf/1909.11867.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d49cc48",
   "metadata": {},
   "source": [
    "One sample of the dataset object looks like this:\n",
    "\n",
    "1. The list contains two tensors with the image data - the first one is the flattened MAML image (shape 84 x 84 -> 84 * 84) data and the second one is the flattened DAE image tensor (shape 128 x 128 -> 128 * 128).\n",
    "2. The second tensor is the tokenized text. (obtained by calling dataset_RAD.VQAFeatureDataset.tokenize())\n",
    "3. The third tensor is the OHE target of the answer (if the question is closed).\n",
    "4. Forth tensor is the answer type.\n",
    "5. Fifth tensor is the question type.\n",
    "6. Sixth is the phrase type.\n",
    "\n",
    "More on the questions, answer and phrase type at section 4.1 in [1].\n",
    "\n",
    "[1]: https://arxiv.org/pdf/1909.11867.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a865fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_sample = next(iter(eval_loader))\n",
    "dataloader_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b125c760",
   "metadata": {},
   "source": [
    "Let's look at the image+question in the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ce25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_plaintext(dset, sample):\n",
    "    text = ''\n",
    "    for token in sample[1][0].numpy():   \n",
    "        if token == 1177: # pad token\n",
    "            break\n",
    "        text += f'{dset.dictionary.idx2word[token]} '\n",
    "    print(text)\n",
    "    return text\n",
    "\n",
    "def show_image(sample):\n",
    "    plt.imshow(sample[0][1].reshape(128,128).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1605d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(dataloader_sample)\n",
    "convert_to_plaintext(eval_dset, dataloader_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027d5792",
   "metadata": {},
   "source": [
    "Now, let's do inference on this sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e90da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'MICCAI19-MedVQA/saved_models/BAN_MEVF/model_epoch19.pth'\n",
    "model_data = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4b8a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model.load_state_dict(model_data.get('model_state', model_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e853300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acf8d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "v, q, a, ans_type, q_types, p_type = dataloader_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e171944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add channel\n",
    "v[0] = v[0].reshape(v[0].shape[0], 84, 84).unsqueeze(1).to(device)   # MAML\n",
    "v[1] = v[1].reshape(v[1].shape[0], 128, 128).unsqueeze(1).to(device) # Autoencoder\n",
    "\n",
    "q = q.to(device)\n",
    "a = a.to(device)\n",
    "\n",
    "features, _ = model(v, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa2ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model.classifier(features)\n",
    "prediction = torch.max(logits, 1)[1].data #argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5ac2e6",
   "metadata": {},
   "source": [
    "... and get the result using dataset.label2ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e334cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dset.label2ans[prediction.item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b21c09",
   "metadata": {},
   "source": [
    "MISC:\n",
    "\n",
    "The test image data is in the ae_image_data attribute. Let's look at a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eb6d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dset.ae_images_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ffba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(eval_dset.ae_images_data[0].squeeze(2).numpy())"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
