{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "393dcc6b",
   "metadata": {},
   "source": [
    "# Kmeans\n",
    "written by Eric Bridgeford and Theodor Marinov\n",
    "\n",
    "In this notebook, we will investigate the K-means clustering implementation provided for our course project.\n",
    "\n",
    "## Algorithm Basics\n",
    "\n",
    "The algorithm used here is standard K-means with K-means++ inialization. At a high level, the algorithm for K-means is as follows:\n",
    "\n",
    "### K-means\n",
    "\n",
    "1. Initialize centers\n",
    "2. while not converged:\n",
    "    + Find the closest center for each data point.\n",
    "    + for each center, average all of the points that are assigned and use this as the new center\n",
    "    + check for convergence condition\n",
    "3. end\n",
    "\n",
    "Here, we define convergence as either the algorithm reaching the maximum number of iterations (defaulted to 100) or the sum from each point to its closest center not changing on a given iteration. To initialize the centers, we perform the following algorithm:\n",
    "\n",
    "### K-means++ Initialization\n",
    "\n",
    "1. choose one data point randomly.\n",
    "2. while fewer than K centers chosen:\n",
    "    + compute the squared distance from each point to its respective center.\n",
    "    + normalize the squared distances to a pmf.\n",
    "    + randomly choose a new center using the squared distance pmf.\n",
    "3. end\n",
    "\n",
    "Essentially, what we are doing is just choosing center points randomly, but evenly distributed throughout our data. K-means can perform poorly when the initial centers are too close or overlapping. This is not the focus of our paper, so we will try to keep the details brief. For more details, see the wikipedia page [kmeans++](https://en.wikipedia.org/wiki/K-means%2B%2B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b897462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from clustering import Kmeans, Spectral\n",
    "from clustering.kernel import RBF_kernel\n",
    "from clustering.utils import purity, plot_laplacian\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sample_ball(r, dim=2, n=100, s=0):\n",
    "    \"\"\"\n",
    "    A function to sample from the unit ball, given an input radius.\n",
    "    \"\"\"\n",
    "    u = np.random.randn(dim, n)\n",
    "    unorm = np.sqrt(np.sum(np.square(u), axis=0))*np.identity(n)\n",
    "    return r*u.dot(np.linalg.pinv(unorm)) + s*np.random.randn(dim, n)\n",
    "\n",
    "def kmeans_analyze(data, true_labels, nc, title=\"\"):\n",
    "    \"\"\"\n",
    "    A script that does data comparisons for kmeans and spectral clustering.\n",
    "    \n",
    "    **Positional Arguments:**\\\n",
    "        - data:\n",
    "            - the data you want to cluster. should be FxFxn.\n",
    "        - true_labels:\n",
    "            - the labels for the data you want to cluster. Should be and n-vector\n",
    "                that matches the ordering of the above data.\n",
    "        - nc:\n",
    "            - the number of classes\n",
    "    \"\"\"    \n",
    "    km = Kmeans(K=nc)\n",
    "    km.initialize(data)\n",
    "    Cent = km.get_centers()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    scat1 = ax.scatter(data[0, :], data[1, :])\n",
    "    scat3 = ax.scatter(Cent[0, :], Cent[1, :], marker='*', linewidths=5)\n",
    "    ax.set_title('{}: Initialized'.format(title))\n",
    "    ax.legend((scat1, scat2, scat3),\n",
    "               ('Data', 'Initialized Centers'),\n",
    "               loc='lower right')\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    \n",
    "    km.fit()\n",
    "    Cent = km.get_centers()\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    scat1 = ax.scatter(data[0, :], data[1, :])\n",
    "    scat3 = ax.scatter(Cent[0, :], Cent[1, :], marker='*', linewidths=5)\n",
    "    fig.legend((scat1, scat3),\n",
    "               ('Data', 'Fitted Centers'),\n",
    "               loc='lower right')\n",
    "    ax.set_title('{}: Fitted'.format(title))\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    \n",
    "    pred_labels = km.get_assignments(data)\n",
    "\n",
    "    # visualize\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    scat = []\n",
    "    lab = []\n",
    "    for pred in np.unique(pred_labels):\n",
    "        scat.append(ax.scatter(data[0, pred_labels == pred], data[1, pred_labels == pred]))\n",
    "        lab.append(\"Predicted Class {}\".format(pred))\n",
    "    scat.append(ax.scatter(Cent[0, :], Cent[1, :], marker='*', linewidths=5))\n",
    "    lab.append(\"Fitted Centers\")\n",
    "    ax.legend(tuple(scat), tuple(lab), loc='lower right')\n",
    "    ax.set_title('{}: Predictions'.format(title))\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(range(km.niter), km.dist[0:km.niter])\n",
    "    ax.set_ylabel('Distance')\n",
    "    ax.set_xlabel('Iteration Number')\n",
    "    ax.set_title('Check of Convergence')\n",
    "    fig.show()\n",
    "\n",
    "    (p, conf_mtx, p_mtx, fig_conf) = purity(true_labels, pred_labels)\n",
    "    print (\"purity = {}\".format(p))\n",
    "    fig_conf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f7fcd2",
   "metadata": {},
   "source": [
    "# Simulation 1: Euclidian Center Separation\n",
    "\n",
    "In this simulation, we have good euclidian center separation. As $K-$means attempts to maximize this notion of distance, we expect a good purity score here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8cc02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d=2  # 2d data sampled\n",
    "n=100  # number of points to sample\n",
    "covar1 = 1*np.identity(d)  # simulated data with I covariance\n",
    "# simulate n samples of zero-mean, gaussian noise, just like the model\n",
    "# specifies\n",
    "data1 = np.random.multivariate_normal(mean=np.zeros((d,)), cov=covar1,\n",
    "                                      size=n).transpose()\n",
    "covar2 = 1*np.identity(d)\n",
    "mean2 = 5*np.ones((d,))  # place far away so perfectly separable\n",
    "data2 = np.random.multivariate_normal(mean=mean2, cov=covar2,\n",
    "                                      size=n).transpose()\n",
    "data = np.hstack((data1, data2))\n",
    "# the true labels are class 0 for first half, and then class 1 for second half\n",
    "true_labels = np.concatenate((0*np.ones((n,)), 1*np.ones((n,))))\n",
    "# randomly permute the labels\n",
    "idx = np.array(range(0, len(true_labels)))\n",
    "true_labels = true_labels[idx]\n",
    "np.random.shuffle(idx)\n",
    "data = data[:, idx]\n",
    "true_labels = true_labels[idx]\n",
    "\n",
    "# visualize\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "scat1 = ax.scatter(data[0, true_labels == 0], data[1, true_labels == 0])\n",
    "scat2 = ax.scatter(data[0, true_labels == 1], data[1, true_labels == 1])\n",
    "ax.set_title('Euclidian Center Separation')\n",
    "ax.legend((scat1, scat2), ('True Class 0', 'True Class 1'), loc='lower right')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c372f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_analyze(data, true_labels, 2, title=\"Simulation 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dea4f64",
   "metadata": {},
   "source": [
    "As we can see in this simple example, our cluster initializations give us initial estimates in line with our intuition of finding cluster centers that are maximally separated by the squared distance. Fitting our parameters appears to optimize these good-first-estimates, allowing us to make perfect predictions on our data. As we can see, our model converges very rapidly towards the optimal solution, and performs perfectly with purity of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c8ddd3",
   "metadata": {},
   "source": [
    "# Simulation 2: Euclidian Center difference, Less Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9423a8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from clustering import Kmeans\n",
    "\n",
    "d=2  # 2d data sampled\n",
    "n=100  # number of points to sample\n",
    "covar1 = 1*np.identity(d)  # simulated data with I covariance\n",
    "# simulate n samples of zero-mean, gaussian noise, just like the model\n",
    "# specifies\n",
    "data1 = np.random.multivariate_normal(mean=np.zeros((d,)), cov=covar1,\n",
    "                                      size=n).transpose()\n",
    "covar2 = 1*np.identity(d)\n",
    "mean2 = 2*np.ones((d,))  # place far away so perfectly separable\n",
    "data2 = np.random.multivariate_normal(mean=mean2, cov=covar2,\n",
    "                                      size=n).transpose()\n",
    "data = np.hstack((data1, data2))\n",
    "# the true labels are class 0 for first half, and then class 1 for second half\n",
    "true_labels = np.concatenate((0*np.ones((n,)), 1*np.ones((n,))))\n",
    "# randomly permute the labels\n",
    "idx = np.array(range(0, len(true_labels)))\n",
    "true_labels = true_labels[idx]\n",
    "np.random.shuffle(idx)\n",
    "data = data[:, idx]\n",
    "true_labels = true_labels[idx]\n",
    "\n",
    "# visualize\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "scat1 = ax.scatter(data[0, true_labels == 0], data[1, true_labels == 0])\n",
    "scat2 = ax.scatter(data[0, true_labels == 1], data[1, true_labels == 1])\n",
    "ax.set_title('Euclidian Center difference, Less Separation')\n",
    "ax.legend((scat1, scat2), ('True Class 0', 'True Class 1'), loc='lower right')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13e23ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_analyze(data, true_labels, 2, title=\"Simulation 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf56101d",
   "metadata": {},
   "source": [
    "Again, we see here that we obtain good performance for our slightly more difficult model where our data is no longer perfectly separable, but we still get $p=$0.91. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c10dd73",
   "metadata": {},
   "source": [
    "# Simulation 3: Euclidian Center Inseparable Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1422bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d=2  # 2d data sampled\n",
    "n=100  # number of points to sample\n",
    "covar1 = 2*np.identity(d)  # simulated data with I covariance\n",
    "# simulate n samples of zero-mean, gaussian noise, just like the model\n",
    "# specifies\n",
    "data1 = np.random.multivariate_normal(mean=np.zeros((d,)), cov=covar1,\n",
    "                                      size=n).transpose()\n",
    "covar2 = 3*np.identity(d)\n",
    "mean2 = 1.5*np.ones((d,))  # place far away so perfectly separable\n",
    "data2 = np.random.multivariate_normal(mean=mean2, cov=covar2,\n",
    "                                      size=n).transpose()\n",
    "data = np.hstack((data1, data2))\n",
    "# the true labels are class 0 for first half, and then class 1 for second half\n",
    "true_labels = np.concatenate((0*np.ones((n,)), 1*np.ones((n,))))\n",
    "# randomly permute the labels\n",
    "idx = np.array(range(0, len(true_labels)))\n",
    "true_labels = true_labels[idx]\n",
    "np.random.shuffle(idx)\n",
    "data = data[:, idx]\n",
    "true_labels = true_labels[idx]\n",
    "\n",
    "# visualize\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "scat1 = ax.scatter(data[0, true_labels == 0], data[1, true_labels == 0])\n",
    "scat2 = ax.scatter(data[0, true_labels == 1], data[1, true_labels == 1])\n",
    "ax.set_title('Completely Unseparable Example')\n",
    "ax.legend((scat1, scat2), ('True Class 0', 'True Class 1'), loc='lower right')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c8f4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_analyze(data, true_labels, 2, title=\"Simulation 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ce9320",
   "metadata": {},
   "source": [
    "Here, we can see that $K-$means begins to struggle fairly significantly to separate the data given that there is a good bit of overlap. It appears as though our cluster centers are fairly accurate, however, since we are just assigning each point to the closest cluster we lose a lot of the fine detail in the overlap. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbec03bc",
   "metadata": {},
   "source": [
    "# Simulation 4: Multi Class with Linear Separability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9606e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d=2  # 2d data sampled\n",
    "n=100  # number of points to sample\n",
    "K=5\n",
    "xmean = np.random.uniform(low=0, high=7, size=K).tolist()\n",
    "ymean = np.random.uniform(low=0, high=7, size=K).tolist()\n",
    "variance = np.array([0.3, 0.3, 0.2, .3, 0.3]).tolist()\n",
    "\n",
    "data_per_class = []\n",
    "for i, (x, y, v) in enumerate(zip(xmean, ymean, variance)):\n",
    "    data_per_class.append(np.random.multivariate_normal(mean = np.array([x, y]),\n",
    "                                                        cov = v*np.identity(d),\n",
    "                                                        size=n).transpose())\n",
    "\n",
    "for i, d in enumerate(data_per_class):\n",
    "    if i == 0:\n",
    "        data = data_per_class[i]\n",
    "        true_labels = i*np.ones((n,))\n",
    "    else:\n",
    "        data = np.hstack((data, data_per_class[i]))\n",
    "        true_labels = np.concatenate((true_labels, i*np.ones((n,))))\n",
    "\n",
    "idx = np.array(range(0, len(true_labels)))\n",
    "np.random.shuffle(idx)\n",
    "np.random.shuffle(idx)\n",
    "data = data[:, idx]\n",
    "true_labels = true_labels[idx]\n",
    "\n",
    "# visualize\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "label = []\n",
    "scat = []\n",
    "for i in range(0, K):\n",
    "    scat.append(ax.scatter(data[0, true_labels == i],\n",
    "                           data[1, true_labels == i]))\n",
    "    label.append('True Class {}'.format(i))\n",
    "\n",
    "ax.set_title('K>2 Example')\n",
    "ax.legend(tuple(scat), tuple(label), loc='lower right')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbbddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_analyze(data, true_labels, 5, title=\"Simulation 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae9bd8b",
   "metadata": {},
   "source": [
    "Here, we can see that the algorithm does a surprisingly good job at finding all of the cluster centers nearly spot on despite relatively large overlap in the clusters, particularly the bottom cluster of 3. We aren't getting perfect classification by any means, but our estimations of centers is at least close. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0114c6f1",
   "metadata": {},
   "source": [
    "# Simulation 5: Radial Difference, K=2\n",
    "\n",
    "In simulations $5 -> 7$, we see varieties of radially separated clusters. As $K-$means has no way to handle this notion (it only considers euclidian center separation), we expect (and find) that $K-$means performs incredibly poorly on all examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937e0b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n=300\n",
    "dim=2\n",
    "s=0.3\n",
    "data1 = sample_ball(r=2, dim=dim, n=n, s=s)\n",
    "data2 = sample_ball(r=4, dim=dim, n=n, s=s)\n",
    "true_labels = np.concatenate((0*np.ones((n,)), 1*np.ones((n,))))\n",
    "data = np.hstack((data1, data2))\n",
    "idx = range(0, len(true_labels))\n",
    "#np.random.shuffle(idx)\n",
    "# randomly permute\n",
    "data = data[:, idx]\n",
    "true_labels = true_labels[idx]\n",
    "\n",
    "# visualize\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "scat1 = ax.scatter(data[0, true_labels == 0], data[1, true_labels == 0])\n",
    "scat2 = ax.scatter(data[0, true_labels == 1], data[1, true_labels == 1])\n",
    "ax.set_title('Radial Difference, K=2')\n",
    "ax.legend((scat1, scat2), ('True Class 0', 'True Class 1'), loc='lower right')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785d67e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_analyze(data, true_labels, 2, title=\"Simulation 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeda0f0c",
   "metadata": {},
   "source": [
    "# Simulation 6: Radial Difference, K=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5a823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=300\n",
    "dim=2\n",
    "s=0.3\n",
    "data1 = sample_ball(r=1, dim=dim, n=n, s=s)\n",
    "data2 = sample_ball(r=4.5, dim=dim, n=n, s=s)\n",
    "data3 = sample_ball(r=9, dim=dim, n=n, s=s)\n",
    "true_labels = np.concatenate((0*np.ones((n,)), 1*np.ones((n,)),\n",
    "                              2*np.ones((n,))))\n",
    "data = np.hstack((data1, data2, data3))\n",
    "idx = range(0, len(true_labels))\n",
    "np.random.shuffle(idx)\n",
    "#randomly permute\n",
    "#data = data[:, idx]\n",
    "#true_labels = true_labels[idx]\n",
    "\n",
    "# visualize\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "scat1 = ax.scatter(data[0, true_labels == 0], data[1, true_labels == 0])\n",
    "scat2 = ax.scatter(data[0, true_labels == 1], data[1, true_labels == 1])\n",
    "scat3 = ax.scatter(data[0, true_labels == 2], data[1, true_labels == 2])\n",
    "ax.set_title('Radial Difference, K=3')\n",
    "ax.legend((scat1, scat2, scat3), ('True Class 0', 'True Class 1', 'True Class 2'),\n",
    "          loc='lower right')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc31db79",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_analyze(data, true_labels, 3, title=\"Simulation 6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f9d8fd",
   "metadata": {},
   "source": [
    "# Simulation 7: Non-Concentric Spheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62776902",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=300\n",
    "dim=2\n",
    "s=0.2\n",
    "data1 = sample_ball(r=2, dim=dim, n=n, s=s)\n",
    "new_mean = np.identity(dim)\n",
    "new_mean[0] = 0\n",
    "data2 = sample_ball(r=3, dim=dim, n=n, s=s) + new_mean.dot(np.ones((dim, n)))\n",
    "true_labels = np.concatenate((0*np.ones((n,)), 1*np.ones((n,))))\n",
    "data = np.hstack((data1, data2))\n",
    "idx = range(0, len(true_labels))\n",
    "#np.random.shuffle(idx)\n",
    "# randomly permute\n",
    "data = data[:, idx]\n",
    "true_labels = true_labels[idx]\n",
    "\n",
    "# visualize\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "scat1 = ax.scatter(data[0, true_labels == 0], data[1, true_labels == 0])\n",
    "scat2 = ax.scatter(data[0, true_labels == 1], data[1, true_labels == 1])\n",
    "ax.set_title('Non-Concentric Spheres')\n",
    "ax.legend((scat1, scat2), ('True Class 0', 'True Class 1'), loc='lower right')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770eb7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_analyze(data, true_labels, 2, title=\"Simulation 7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd2e6a3",
   "metadata": {},
   "source": [
    "# Simulation 8: "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
