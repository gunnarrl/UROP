{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a259a4f5",
   "metadata": {},
   "source": [
    "## Working with the python DyNet package\n",
    "\n",
    "The DyNet package is intended for training and using neural networks, and is particularly suited for applications with dynamically changing network structures. It is a python-wrapper for the DyNet C++ package.\n",
    "\n",
    "In neural network packages there are generally two modes of operation:\n",
    "\n",
    "* __Static networks__, in which a network is built and then being fed with different inputs/outputs. Most NN packages work this way.\n",
    "* __Dynamic networks__, in which a new network is built for each training example (sharing parameters with the networks of other training examples).  This approach is what makes DyNet unique, and where most of its power comes from.\n",
    "\n",
    "We will describe both of these modes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e2de8b",
   "metadata": {},
   "source": [
    "### Package Fundamentals\n",
    "\n",
    "The main piece of DyNet is the `ComputationGraph`, which is what essentially defines a neural network.\n",
    "The `ComputationGraph` is composed of expressions, which relate to the inputs and outputs of the network,\n",
    "as well as the `Parameters` of the network. The parameters are the things in the network that are optimized over time, and all of the parameters sit inside a `ParameterCollection`. There are `trainers` (for example `SimpleSGDTrainer`) that are in charge of setting the parameter values.\n",
    "\n",
    "We will not be using the `ComputationGraph` directly, but it is there in the background, as a singleton object.\n",
    "When `dynet` is imported, a new `ComputationGraph` is created. We can then reset the computation graph to a new state\n",
    "by calling `renew_cg()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c6c2bd",
   "metadata": {},
   "source": [
    "### Static Networks\n",
    "\n",
    "The life-cycle of a DyNet program is:\n",
    "1. Create a `ParameterCollection`, and populate it with `Parameters`.\n",
    "2. Renew the computation graph, and create `Expression` representing the network\n",
    "      (the network will include the `Expression`s for the `Parameters` defined in the parameter collection).\n",
    "3. Optimize the model for the objective of the network.\n",
    "\n",
    "As an example, consider a model for solving the \"xor\" problem. The network has two inputs, which can be 0 or 1, and a single output which should be the xor of the two inputs.\n",
    "We will model this as a multi-layer perceptron with a single hidden layer.\n",
    "\n",
    "Let $x = x_1, x_2$ be our input. We will have a hidden layer of 8 nodes, and an output layer of a single node. The activation on the hidden layer will be a $\\tanh$. Our network will then be:\n",
    "\n",
    "$\\sigma(V(\\tanh(Wx+b)))$\n",
    "\n",
    "Where $W$ is a $8 \\times 2$ matrix, $V$ is an $8 \\times 1$ matrix, and $b$ is an 8-dim vector.\n",
    "\n",
    "We want the output to be either 0 or 1, so we take the output layer to be the logistic-sigmoid function, $\\sigma(x)$, that takes values between $-\\infty$ and $+\\infty$ and returns numbers in $[0,1]$.\n",
    "\n",
    "We will begin by defining the model and the computation graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d261fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we assume that we have the dynet module in your path.\n",
    "import dynet as dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b6d6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a parameter collection and add the parameters.\n",
    "m = dy.ParameterCollection()\n",
    "W = m.add_parameters((8,2))\n",
    "V = m.add_parameters((1,8))\n",
    "b = m.add_parameters((8))\n",
    "\n",
    "dy.renew_cg() # new computation graph. not strictly needed here, but good practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff038bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b[1:-1].value()\n",
    "b.value()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f20855",
   "metadata": {},
   "source": [
    "The first block creates a parameter collection and populates it with parameters.\n",
    "The second block creates a computation graph.\n",
    "\n",
    "The model parameters can be used as expressions in the computation graph. We now make use of V, W, and b in order to create the complete expression for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8552f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dy.vecInput(2) # an input vector of size 2. Also an expression.\n",
    "output = dy.logistic(V*(dy.tanh((W*x)+b)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4042a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now query our network\n",
    "x.set([0,0])\n",
    "output.value()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21def6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to be able to define a loss, so we need an input expression to work against.\n",
    "y = dy.scalarInput(0) # this will hold the correct answer\n",
    "loss = dy.binary_log_loss(output, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d85d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.set([1,0])\n",
    "y.set(0)\n",
    "print(loss.value())\n",
    "\n",
    "y.set(1)\n",
    "print(loss.value())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b059192e",
   "metadata": {},
   "source": [
    "### Training\n",
    "We now want to set the parameter weights such that the loss is minimized. \n",
    "\n",
    "For this, we will use a trainer object. A trainer is constructed with respect to the parameters of a given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583934a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = dy.SimpleSGDTrainer(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85668671",
   "metadata": {},
   "source": [
    "To use the trainer, we need to:\n",
    "* **call the `forward_scalar`** method of `ComputationGraph`. This will run a forward pass through the network, calculating all the intermediate values until the last one (`loss`, in our case), and then convert the value to a scalar. The final output of our network **must** be a single scalar value. However, if we do not care about the value, we can just use `cg.forward()` instead of `cg.forward_sclar()`.\n",
    "* **call the `backward`** method of `ComputationGraph`. This will run a backward pass from the last node, calculating the gradients with respect to minimizing the last expression (in our case we want to minimize the loss). The gradients are stored in the parameter collection, and we can now let the `trainer` take care of the optimization step.\n",
    "* **call `trainer.update()`** to optimize the values with respect to the latest gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc78ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.set([1,0])\n",
    "y.set(1)\n",
    "loss_value = loss.value() # this performs a forward through the network.\n",
    "print(\"the loss before step is:\",loss_value)\n",
    "\n",
    "# now do an optimization step\n",
    "loss.backward()  # compute the gradients\n",
    "trainer.update()\n",
    "\n",
    "# see how it affected the loss:\n",
    "loss_value = loss.value(recalculate=True) # recalculate=True means \"don't use precomputed value\"\n",
    "print(\"the loss after step is:\",loss_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c495e868",
   "metadata": {},
   "source": [
    "The optimization step indeed made the loss decrease. We now need to run this in a loop.\n",
    "To this end, we will create a `training set`, and iterate over it.\n",
    "\n",
    "For the xor problem, the training instances are easy to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a596014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xor_instances(num_rounds=2000):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for round in range(num_rounds):\n",
    "        for x1 in 0,1:\n",
    "            for x2 in 0,1:\n",
    "                answer = 0 if x1==x2 else 1\n",
    "                questions.append((x1,x2))\n",
    "                answers.append(answer)\n",
    "    return questions, answers \n",
    "\n",
    "questions, answers = create_xor_instances()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec556bb",
   "metadata": {},
   "source": [
    "We now feed each question / answer pair to the network, and try to minimize the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c381b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0\n",
    "seen_instances = 0\n",
    "for question, answer in zip(questions, answers):\n",
    "    x.set(question)\n",
    "    y.set(answer)\n",
    "    seen_instances += 1\n",
    "    total_loss += loss.value()\n",
    "    loss.backward()\n",
    "    trainer.update()\n",
    "    if (seen_instances > 1 and seen_instances % 100 == 0):\n",
    "        print(\"average loss is:\",total_loss / seen_instances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a555bfa",
   "metadata": {},
   "source": [
    "Our network is now trained. Let's verify that it indeed learned the xor function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cce0f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.set([0,1])\n",
    "print(\"0,1\",output.value())\n",
    "\n",
    "x.set([1,0])\n",
    "print(\"1,0\",output.value())\n",
    "\n",
    "x.set([0,0])\n",
    "print(\"0,0\",output.value())\n",
    "\n",
    "x.set([1,1])\n",
    "print(\"1,1\",output.value())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e86bf6",
   "metadata": {},
   "source": [
    "In case we are curious about the parameter values, we can query them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e4afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2d98d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "V.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7506f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.value()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26543bff",
   "metadata": {},
   "source": [
    "### To summarize\n",
    "Here is a complete program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ea2a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters\n",
    "m = dy.ParameterCollection()\n",
    "W = m.add_parameters((8,2))\n",
    "V = m.add_parameters((1,8))\n",
    "b = m.add_parameters((8))\n",
    "\n",
    "# renew the computation graph\n",
    "dy.renew_cg()\n",
    "\n",
    "# create the network\n",
    "x = dy.vecInput(2) # an input vector of size 2.\n",
    "output = dy.logistic(V*(dy.tanh((W*x)+b)))\n",
    "# define the loss with respect to an output y.\n",
    "y = dy.scalarInput(0) # this will hold the correct answer\n",
    "loss = dy.binary_log_loss(output, y)\n",
    "\n",
    "# create training instances\n",
    "def create_xor_instances(num_rounds=2000):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for round in range(num_rounds):\n",
    "        for x1 in 0,1:\n",
    "            for x2 in 0,1:\n",
    "                answer = 0 if x1==x2 else 1\n",
    "                questions.append((x1,x2))\n",
    "                answers.append(answer)\n",
    "    return questions, answers \n",
    "\n",
    "questions, answers = create_xor_instances()\n",
    "\n",
    "# train the network\n",
    "trainer = dy.SimpleSGDTrainer(m)\n",
    "\n",
    "total_loss = 0\n",
    "seen_instances = 0\n",
    "for question, answer in zip(questions, answers):\n",
    "    x.set(question)\n",
    "    y.set(answer)\n",
    "    seen_instances += 1\n",
    "    total_loss += loss.value()\n",
    "    loss.backward()\n",
    "    trainer.update()\n",
    "    if (seen_instances > 1 and seen_instances % 100 == 0):\n",
    "        print(\"average loss is:\",total_loss / seen_instances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11775f9",
   "metadata": {},
   "source": [
    "### Dynamic Networks\n",
    "\n",
    "Dynamic networks are very similar to static ones, but instead of creating the network once and then calling \"set\" in each training example to change the inputs, we just create a new network for each training example.\n",
    "\n",
    "We present an example below. While the value of this may not be clear in the `xor` example, the dynamic approach\n",
    "is very convenient for networks for which the structure is not fixed, such as recurrent or recursive networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8671d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dynet as dy\n",
    "# create training instances, as before\n",
    "def create_xor_instances(num_rounds=2000):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for round in range(num_rounds):\n",
    "        for x1 in 0,1:\n",
    "            for x2 in 0,1:\n",
    "                answer = 0 if x1==x2 else 1\n",
    "                questions.append((x1,x2))\n",
    "                answers.append(answer)\n",
    "    return questions, answers \n",
    "\n",
    "questions, answers = create_xor_instances()\n",
    "\n",
    "# create a network for the xor problem given input and output\n",
    "def create_xor_network(W, V, b, inputs, expected_answer):\n",
    "    dy.renew_cg() # new computation graph\n",
    "    x = dy.vecInput(len(inputs))\n",
    "    x.set(inputs)\n",
    "    y = dy.scalarInput(expected_answer)\n",
    "    output = dy.logistic(V*(dy.tanh((W*x)+b)))\n",
    "    loss =  dy.binary_log_loss(output, y)\n",
    "    return loss\n",
    "\n",
    "m2 = dy.ParameterCollection()\n",
    "W = m2.add_parameters((8,2))\n",
    "V = m2.add_parameters((1,8))\n",
    "b = m2.add_parameters((8))\n",
    "trainer = dy.SimpleSGDTrainer(m2)\n",
    "\n",
    "seen_instances = 0\n",
    "total_loss = 0\n",
    "for question, answer in zip(questions, answers):\n",
    "    loss = create_xor_network(W, V, b, question, answer)\n",
    "    seen_instances += 1\n",
    "    total_loss += loss.value()\n",
    "    loss.backward()\n",
    "    trainer.update()\n",
    "    if (seen_instances > 1 and seen_instances % 100 == 0):\n",
    "        print(\"average loss is:\",total_loss / seen_instances)\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
