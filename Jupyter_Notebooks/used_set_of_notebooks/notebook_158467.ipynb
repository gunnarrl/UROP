{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "938a74d6",
   "metadata": {},
   "source": [
    "### Predicting future Bitcoin stock price\n",
    "----------------------------------\n",
    "Bitcoin is one of the most famous cryptocurrency out there. It has reached the mainstream media when its market value has sky rocketed almost reading \\$20,000. It's value has dropped since early 2018 due to different circumstances and as of 5/3/2018 it's market value is \\$9,232.19 based on https://finance.yahoo.com/quote/BTC-USD . Because of the fluctuating nature it's stock value, we will try to see if a recurrent neural network could predict future prices. But first, let's load the required libraries. As you progress through the demo, you will see how each library is utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1842e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing relevant modules\n",
    "import matplotlib.pyplot   as plt\n",
    "\n",
    "from pandas                import read_csv, to_datetime\n",
    "from numpy                 import reshape, array\n",
    "from datetime              import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers          import Dense, Dropout, LSTM, Input, TimeDistributed\n",
    "from keras.models          import Model\n",
    "from keras_tqdm            import TQDMNotebookCallback\n",
    "from IPython.display       import SVG, display\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.callbacks       import EarlyStopping\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf31d7a",
   "metadata": {},
   "source": [
    "The sample Data used for this demo contains bitcoin stock price from 12/4/2016 to 3/27/2018. The input attribute in order are Timestamp (in unix time), starting price(open), highest price(high), lowest price(low), closing price(close), volume of bitcoin exchanged(volume_BTC), volume of currency for the bitcoins(volume_currency), weighted price. All prices are in US dollars. Sample of the dataset is shown below.\n",
    "All datasets were taken from https://www.kaggle.com/mczielinski/bitcoin-historical-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101e4b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Demonstration purpose only, \n",
    "rawData = read_csv('sample_data.csv')\n",
    "rawData[0:5]  #first five records in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de98e601",
   "metadata": {},
   "source": [
    "Now, let's define a function that can preprocess the sample dataset. In the first line we utilize the **read_csv()** function imported from **pandas**. This will return a **DataFrame** which will allow us to manipulate the sample dataset. If you've experience with database, a **DataFrame** is similar to a table. Once we read the file, we use another **pandas** imported function **to_datetime()** which will convert the unix timestampe to \"human date\". The result is appended to the **DataFrame** as a new column called (wait for it....) *date*.\n",
    "The *date* is used to group the values in the **DataFrame** to calculate the average mean price of Bitcoin per day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9959de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_File(file: int = 0, split: float = 0.05, reduce: int = 0):          #File size (in MB)\n",
    "  \n",
    "    # Read input sample data into a data frame.\n",
    "    df = read_csv('sample_data.csv')\n",
    "    \n",
    "    # Append 'date' column to the sample dataframe.\n",
    "    # Uses the Unix timestamp to create equivalent date\n",
    "    # This attribute will be used to calculate the daily weighted average\n",
    "    df['date'] = to_datetime(df['Timestamp'],unit='s').dt.date\n",
    "    \n",
    "    # Store the average weighted price per day (in USD).\n",
    "    daily_average = [df.groupby('date')['Weighted_Price'].mean(),\n",
    "                     df.groupby('date')['Weighted_Price'].mean()][0].values\n",
    "    \n",
    "    #reduce file, if desired.\n",
    "    if reduce and reduce < len(daily_average):\n",
    "        daily_average = daily_average[len(daily_average) - 1 - reduce:]\n",
    "        \n",
    "    #Gaurantees at least one testing example.\n",
    "    partition     = min(-int(split*len(daily_average)), -1)\n",
    "    \n",
    "    training_set, testing_set = daily_average[:partition], daily_average[partition:]\n",
    "    \n",
    "    # Create instance of min-max scaler.\n",
    "    sc                        = MinMaxScaler(feature_range = (0, 1))\n",
    "    \n",
    "    # Reshape training and testing sets and then perform min-max scaling.\n",
    "    training_set, testing_set = sc.fit_transform(reshape(training_set, (len(training_set), 1))), \\\n",
    "                                sc.transform(reshape(testing_set, (len(testing_set), 1)))\n",
    "    \n",
    "    \n",
    "    return training_set[:-1], training_set[1:], testing_set[:-1], testing_set[1:], sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41588591",
   "metadata": {},
   "source": [
    "#### The function is defined to help choose the color and thickness of the lines that we will use to plot the graph later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c29e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot results.\n",
    "def Plot(data1, data2, title: str = '', label1: str = '', label2: str = '', ylabel: str = '', scatter: bool = True):\n",
    "    plt.figure(figsize   = (25,15),\n",
    "               dpi       = 80,\n",
    "               facecolor = 'w',\n",
    "               edgecolor = 'k')\n",
    "    \n",
    "    ax = plt.gca()\n",
    "\n",
    "    x  = [_ for _ in range(len(data1))]\n",
    "    \n",
    "    if scatter:\n",
    "        plt.plot(data1, \n",
    "                 color = '#AA00FF',\n",
    "                 ls    = 'dashed')\n",
    "        plt.plot(data2,\n",
    "                 color = '#000000',\n",
    "                 ls    = 'dashed')\n",
    "        plt.scatter(x,\n",
    "                   data1,\n",
    "                   label = label1,\n",
    "                   color = '#AA00FF',\n",
    "                   s     = 50)\n",
    "        plt.scatter(x,\n",
    "                   data2,\n",
    "                   label = label2,\n",
    "                   color = '#000000',\n",
    "                   s     = 50)\n",
    "    else:\n",
    "        plt.bar(x,\n",
    "                reshape(data1, (len(data1))),\n",
    "                label = label1,\n",
    "                color = '#AA00FF')\n",
    "        plt.bar(x,\n",
    "                reshape(data2, (len(data2))),\n",
    "                label = label2,\n",
    "                color = '#000000',\n",
    "                alpha = 0.5)\n",
    "\n",
    "    plt.title(title,\n",
    "              fontsize = 40)\n",
    "\n",
    "    for tick in ax.xaxis.get_major_ticks():\n",
    "        tick.label1.set_fontsize(18)\n",
    "    for tick in ax.yaxis.get_major_ticks():\n",
    "        tick.label1.set_fontsize(18)\n",
    "\n",
    "    plt.xlabel('Time (days)',\n",
    "               fontsize = 40)\n",
    "    plt.ylabel(ylabel,\n",
    "               fontsize = 40)\n",
    "    plt.legend(loc  = 'best',\n",
    "               prop = {'size': 25})\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3ceb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get percent change results for plotting.\n",
    "Get_Percent_Change = lambda p: [100*(e2 - e1)/e1 for e1, e2 in zip(reshape(p[:-1], p[:-1].shape), reshape(p[1:], p[1:].shape))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cdcd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training/testing arrays and store min-max scaler so we can perform inverse transform later.\n",
    "x_train, y_train, x_test, y_test, sc = Read_File()\n",
    "\n",
    "#Reshape the training and testing input so that it meets the specifications of the net input.\n",
    "x_train                              = reshape(x_train, (len(x_train), 1, 1))\n",
    "x_test                               = reshape(x_test, (len(x_test), 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89229b4e",
   "metadata": {},
   "source": [
    "#### Let's Build the Network\n",
    "The model uses Long Short Term Memory as it's first hidden layer then dropout and finally to a dense layer.It is crucial the dense layer to have a linear activation because the output won't get diminished.\n",
    "\n",
    "Try changing the activation for the LSTM layers to **softmax** or **sigmoid** and see the difference in the behavior. Another thing to look at is how the network behaves when the dropout percentage is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714f6a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "lstm_units  = 100\n",
    "inputs      = Input(shape = x_train[0].shape)\n",
    "\n",
    "layer       = LSTM(units            = lstm_units,\n",
    "                   return_sequences = False,\n",
    "                   activation       = 'selu')(inputs)\n",
    "\n",
    "layer       = Dropout(0.2)(layer)\n",
    "\n",
    "predictions = Dense(units      = 1,\n",
    "                    activation = 'linear')(layer)\n",
    "\n",
    "model       = Model(inputs, predictions)\n",
    "\n",
    "model.compile(loss      = 'mse',\n",
    "              optimizer = 'adadelta',\n",
    "              metrics   = ['accuracy'])\n",
    "\n",
    "# Visual representation of net.\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845143da",
   "metadata": {},
   "source": [
    "#### Training time\n",
    "The epoch size should be somewhere between 500 - 1500 (depends on which activation you choose). The learning curve will reach a plateau right around 1500 epochs so there is no need to train above that point. And also you don't want to shuffle while training because order matters for this specific situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d814378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the RNN to the Training set\n",
    "\n",
    "epoch_size= 500\n",
    "history = model.fit(x                = x_train,\n",
    "                    y                = y_train,\n",
    "                    batch_size       = len(x_train),\n",
    "                    epochs           = epoch_size,\n",
    "                    validation_split = 0.2,\n",
    "                    verbose          = 0,\n",
    "                    shuffle          = False,\n",
    "                    callbacks        = [TQDMNotebookCallback() ])#EarlyStopping(patience = 3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea9c08b",
   "metadata": {},
   "source": [
    "#### Plotting the loss-accuracy\n",
    "Because we used a 0.2 validation split (meaning it will train on 80% of the data we provide it and will test on the 20%), we can plot how accurate it was. We accomplish this using the **matplotlib** package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df518f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# summarize history for accuracy\n",
    "plt.subplot(211)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "# summarize history for loss\n",
    "plt.subplot(212)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad51a783",
   "metadata": {},
   "source": [
    "#### Checking that actual values\n",
    "It looks like it didn't perform good but let's do more investigation. Let's compute the **inverse_transform** of the actual price, and predicted price using our model for the trained dataset. Let's plot that graph and see if it tells us anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa1cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing our model's output, given the training data, to the expected output.\n",
    "actual_prices_train       = sc.inverse_transform(y_train)[:-1]\n",
    "predicted_BTC_price_train = sc.inverse_transform(model.predict(x_train))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c6bcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(actual_prices_train, predicted_BTC_price_train, \\\n",
    "     \"BTC Price Prediction for Training Data\", \"Real BTC Price\", \"Predicted BTC Price\", \"BTC Price (USD)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c383f8",
   "metadata": {},
   "source": [
    "#### The Test Dataset\n",
    "Looks like it did a decent job with the trained dataset. Now let's do the same thing but with data the network has never seen before. The actual value is retrieved from the preprocessed dataset, and the predicted value is computed using the model. After that we made the graph to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6904f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the future.\n",
    "actual_prices_test       = sc.inverse_transform(y_test)[:-1]\n",
    "predicted_BTC_price_test = sc.inverse_transform(model.predict(x_test))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2516261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(actual_prices_test, predicted_BTC_price_test, \"BTC Price Prediction for Testing Data\", \"Real BTC Price\", \"Predicted BTC Price\", \"BTC Price (USD)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2782920f",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Although the accuracy graph didn't show an outstanding result, checking the actual points gave us an insight on why it wasn't accurate. It looks like our prediction was shooting higher or lower than the actual price but not by too much. When we look at the bigger picture the network performed surprisingly well. In fact one might say too well. This leads us to ask questions on were there some unknown bias or did the network memorize the dataset... it requires more investigation. But for now the network did predict the future price of Bitcoin to reasonable accuracy."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
