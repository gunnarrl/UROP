{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64feb6ea",
   "metadata": {},
   "source": [
    "# Distributed Tensorflow\n",
    "\n",
    "\n",
    "![TensorFlowing](./files/tensors_flowing.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0134b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ad5e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext version_information\n",
    "%version_information numpy, scipy, matplotlib, pandas, tensorflow, sklearn, skflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fea2b25",
   "metadata": {},
   "source": [
    "## Overview of Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3195760f",
   "metadata": {},
   "source": [
    "### Cluster\n",
    "\n",
    "To define a distributed computation in tensorflow we need to specify two kinds of jobs:\n",
    "\n",
    "- worker jobs\n",
    "- parameter server (ps) jobs\n",
    "\n",
    "Each **job** is defined by one ore more **tasks**. Each task is usually specified with a simple numerical index, i.e. `0,1,2,3, ..`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8f6979",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_SPEC= \"\"\"\n",
    "{\n",
    "    'ps' : ['tensorflow0.pipeline.io:8888', 'tensorflow1.pipeline.io:8888'],\n",
    "    'worker' : [ 'tensorflow2.pipeline.io:8888','tensorflow3.pipeline.io:8888'],\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b199d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "cluster_def = ast.literal_eval(CLUSTER_SPEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0483a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5fdb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = tf.train.ClusterSpec(cluster_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6346f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec.jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e68f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in spec.jobs:\n",
    "    print(job, spec.job_tasks(job))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b1d733",
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = ['/job:worker/task:{}'.format(i) for i in range(len(cluster_def['worker']))]\n",
    "param_servers = ['/job:ps/task:{}'.format(i) for i in range(len(cluster_def['ps']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3f3524",
   "metadata": {},
   "outputs": [],
   "source": [
    "workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e43cb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_servers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5b6957",
   "metadata": {},
   "source": [
    "### Pinning of Variables\n",
    "Each Variable is assigned to a specific device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fde1e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = tf.Variable(\"local_cpu\")\n",
    "l.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0850b6",
   "metadata": {},
   "source": [
    "We can enforce the assigned device using the `tf.device` context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92354af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ps in param_servers:\n",
    "    with tf.device(ps):\n",
    "        v = tf.Variable(\"my_var\")\n",
    "v.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bd4602",
   "metadata": {},
   "source": [
    "## Tensorflow Server\n",
    "\n",
    "The server is responsible to handle the actual communication. On each of the cluster's node we will spawn a simple gRPC Server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57df2255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_worker(job_name, task_id, cluster_def):\n",
    "    server = tf.train.Server(\n",
    "        cluster_def,\n",
    "        job_name=job_name,\n",
    "        task_index=task_id\n",
    "    )\n",
    "    server.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5460f06e",
   "metadata": {},
   "source": [
    "### Connecting to a Server\n",
    "\n",
    "to connect to _any_ server you can specify the 'target' of the session,direct ip:port of the server when creating a [Session](https://www.tensorflow.org/versions/r0.8/api_docs/python/client.html#Session) object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5443d2d",
   "metadata": {},
   "source": [
    "Note that the server is generic and can assume either the role of parameter server or of worker.The Cluster configuration decides the role.\n",
    "\n",
    "![ps workers](./ps_workers.png)\n",
    "\n",
    "The best practice is to create a single Image launching the tensorflow worker. \n",
    "\n",
    "Environment variables then specify the exact role for the worker at run time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd465b9e",
   "metadata": {},
   "source": [
    "### gRPC\n",
    "\n",
    "[gRPC](http://www.grpc.io) Is a Remote Procedure Call protocol based on [Protocol Buffers](https://developers.google.com/protocol-buffers/).\n",
    "\n",
    "\n",
    "Each object in tensorflow that has to be sent over the wire has a gRPC definition. \n",
    "\n",
    "1. Client figures out what variables need to be serialized to gRPC.\n",
    "1. Client makes the gRPC remote call to the Server and sends the values.\n",
    "1. If the Server accepts the call, the serialized tensors are de-serialized\n",
    "1. The Server runs the requested operation on the graph and all its dependencies\n",
    "1. The Server serializes the result and sends it back on the same connection to the Client\n",
    "1. The Client receives the results and deserializes.\n",
    "\n",
    "![gRPC Communicaton](./grpc_communication.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5127cbae",
   "metadata": {},
   "source": [
    "Example of a gRPC declaration for the [Variable ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/variable.proto)\n",
    "\n",
    "\n",
    "```javascript\n",
    "syntax = \"proto3\";\n",
    "\n",
    "package tensorflow;\n",
    "\n",
    "// Protocol buffer representing a Variable.\n",
    "message VariableDef {\n",
    "  // Name of the variable tensor.\n",
    "  string variable_name = 1;\n",
    "\n",
    "  // Name of the initializer op.\n",
    "  string initializer_name = 2;\n",
    "\n",
    "  // Name of the snapshot tensor.\n",
    "  string snapshot_name = 3;\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "Each variable can then be serialized using the `to_proto` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5f7c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "v.to_proto()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf8816",
   "metadata": {},
   "source": [
    "## Simple reduce sum Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580b673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "        \n",
    "    with tf.device('/job:ps/task:0'):\n",
    "        input_array = tf.placeholder(tf.int32, shape=[None])\n",
    "        final_result = tf.Variable(0)\n",
    "        \n",
    "    # divide the input across the cluster:\n",
    "    all_reduce = []\n",
    "    splitted = tf.split(0, len(workers), input_array)\n",
    "    for idx, (portion, worker) in enumerate(zip(splitted,workers)):\n",
    "        with tf.device(worker):\n",
    "           print(worker)\n",
    "           local_reduce = tf.reduce_sum(portion)\n",
    "           local_reduce = tf.Print(portion, [local_reduce], message=\"portion is\")\n",
    "           all_reduce.append(local_reduce)\n",
    "    \n",
    "    final_result = tf.reduce_sum(tf.pack(all_reduce))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0340d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_config = tf.ConfigProto(\n",
    "    allow_soft_placement=True,\n",
    "    log_device_placement=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ce8ad6",
   "metadata": {},
   "source": [
    "We can now run the graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5456f26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "\n",
    "with tf.Session(\"grpc://tensorflow3.pipeline.io:8888\", graph=graph, config=sess_config) as session:\n",
    "    result = session.run(final_result, feed_dict={ input_array: np.ones([1000]) }, options=run_options)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4a72d6",
   "metadata": {},
   "source": [
    "We can also inspect any remote variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf05c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29a8bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(\"grpc://tensorflow3.pipeline.io:8888\", graph=graph, config=sess_config) as session:\n",
    "    result = session.run(local_reduce, feed_dict={ input_array: np.ones([1000]) }, options=run_options)\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
