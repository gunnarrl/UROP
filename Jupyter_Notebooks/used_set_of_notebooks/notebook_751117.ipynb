{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "766f188b",
   "metadata": {},
   "source": [
    "# Astroinformatics \"Machine Learning Basics\"\n",
    "## Class 3: \n",
    "In this tutorial, we'll see basics concepts of machine learning. (We will not see classification yet, but these concepts applies to those problems too). All this concepts are very well explained in the [Deep Learning Book, Chapter 5](http://www.deeplearningbook.org/contents/ml.html)\n",
    "\n",
    "First a brief discussion about the frequentist and bayesians approach of machine learning. Then a basic problem of linear regression in order to give some insight about the approach of frequentist and bayesians of this problem and its connection. Then we'll see the concept of capacity, overfitting and underfitting and how to solve it using regularization (explained from a frequentist and bayesian point of view). Finally, we use cross validation to select the hyperparameters of our optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2373faf8",
   "metadata": {},
   "source": [
    "# Frequentiest and Bayesians\n",
    "\n",
    "\n",
    "Please read the discussion in [this great book](http://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter1_Introduction/Ch1_Introduction_PyMC3.ipynb), It'll give you a great insight about the difference between the two approaches. I also recommend  [this reading](https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading20.pdf) from an MIT class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a2635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bbf03b",
   "metadata": {},
   "source": [
    "# Linear regression least square\n",
    "\n",
    "Given some data $(x_{i}, y_{i})_{i=1}^{N}$, we want to find the best affine transformation of $x$ that better predicts $y$, the affine model is $\\hat{y} = w^{T}x$ where we add a 1 to $x$ in order to have an offset of the linear model (affine transformation). Let's define $(X,Y)$ as the dataset where each row of $X$ is $x_{i}^{T}$ and $Y$ has $y_{i}$ as components. Now, we define the mean squared error ($MSE$) as our measure of performance of the model:\n",
    "\n",
    "$$ MSE = \\frac{1}{N}\\| \\hat{Y}-Y \\|^{2}_{2} $$\n",
    "\n",
    "where $\\hat{Y}$ is the estimated values of the linear model. In order to find the best model in the least square sense, we can minimize MSE by gradient:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{w} MSE = 0 \\\\\n",
    "\\nabla_{w} \\frac{1}{N} \\| \\hat{Y} - Y \\|^{2}_{2} = 0 \\\\\n",
    "\\frac{1}{N} \\nabla_{w} \\| Xw - Y \\|^{2}_{2} = 0 \\\\\n",
    "\\nabla_{w} (Xw-Y)^{T}(Xw-Y) = 0 \\\\\n",
    "\\nabla_{w} (w^{T}X^{T}Xw-2w^{T}X^{T}Y-Y^{T}Y) = 0 \\\\\n",
    "2X^{T}Xw-2X^{T}Y=0 \\\\\n",
    "w = (X^{T}X)^{-1}X^{T}Y\n",
    "\\end{equation}\n",
    "\n",
    "We know that this is the solution for the linear regression because the $MSE$ is a convex function of $w$m we can check this by taking the second derivative, $\\nabla_{w}^{2}MSE = 2X^{T}X$ which is a positive semi definite matrix (the eigenvalues are bigger or equal to zero). In this case, we need all the eigenvalues bigger than zero, if there is at least one eigenvalue equal to zero, that means that the value of the MSE is \"flat\" in the direction of that eigenvector, when the eigenvalue is \"close\" to zero, it can produce numerical instability on the optimization (this can be fixed with regularization). Now let's make an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0399b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_function(w, x):\n",
    "    return np.dot(x, w)\n",
    "\n",
    "w = np.array([0.7, 0.3])[...,np.newaxis]\n",
    "print(w.shape)\n",
    "noise = 0.10\n",
    "n_points = 20\n",
    "np.random.seed(500)\n",
    "\n",
    "# we add an extra dimension to make it a column vector\n",
    "x_samples = np.linspace(3, 5, n_points)[..., np.newaxis]\n",
    "# then we add a column of ones in order to have the constant term a*x + b*1 = y\n",
    "augmented_x = np.concatenate([x_samples, np.ones(shape=(n_points,1))], axis=1)\n",
    "print(\"samples shape: \"+str(augmented_x.shape))\n",
    "# adding gaussian noise to the data\n",
    "y_samples = linear_function(w, augmented_x) + np.random.normal(loc=0.0, scale=noise, size=(n_points,1))\n",
    "print(\"target shape: \"+str(y_samples.shape))\n",
    "fig, ax = plt.subplots(figsize=(12,7))\n",
    "ax.plot(x_samples, linear_function(w, augmented_x), label=\"Real solution\")\n",
    "ax.scatter(x_samples, y_samples, label=\"Samples\", s=70)\n",
    "ax.legend(fontsize=14)\n",
    "ax.set_xlabel(\"x\", fontsize=14)\n",
    "ax.set_ylabel(\"y\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbba0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least square solution\n",
    "estimated_w = inv(augmented_x.T @ augmented_x) @ augmented_x.T @ y_samples\n",
    "# MSE\n",
    "error = np.linalg.norm(y_samples - linear_function(estimated_w, augmented_x))**2/len(y_samples)\n",
    "# eigenvectors and eigenvalues of the covariance matrix\n",
    "eg_values, eg_vectors = np.linalg.eig(augmented_x.T @ augmented_x)\n",
    "print(\"estimated w:\" +str(estimated_w))\n",
    "print(\"mean squared error: \"+str(error))\n",
    "print(\"eigenvalues: \"+str(eg_values))\n",
    "print(\"eigenvectos: \"+str(eg_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2ca362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making error maningfold\n",
    "X_array = np.arange(-1, 2.5, 0.05)\n",
    "Y_array = np.arange(-1, 2.5, 0.05)\n",
    "X, Y = np.meshgrid(X_array, Y_array)\n",
    "Z = np.zeros(shape=(len(X_array), len(Y_array)))\n",
    "\n",
    "for i, x in enumerate(X_array):\n",
    "    for j, y in enumerate(Y_array):\n",
    "        w_loop = np.array([x, y])[..., np.newaxis]\n",
    "        Z[i, j] = np.linalg.norm(y_samples - linear_function(w_loop, augmented_x))**2/len(y_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838bd41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax, ax2) = plt.subplots(1, 2, figsize=(15,7))\n",
    "ax.plot(x_samples, linear_function(w, augmented_x), label=\"Real solution\")\n",
    "ax.scatter(x_samples, y_samples, label=\"Samples\", s=70)\n",
    "ax.plot(x_samples, linear_function(estimated_w, augmented_x), label=\"Estimated solution\")\n",
    "ax.legend(fontsize=14)\n",
    "ax.set_xlabel(\"x\", fontsize=14)\n",
    "ax.set_ylabel(\"y\", fontsize=14)\n",
    "\n",
    "levels = np.linspace(0, np.amax(Z), 100)\n",
    "cont = ax2.contourf(X, Y, Z, levels = levels)#,cmap=\"inferno\")\n",
    "soa = np.concatenate([np.roll(np.repeat(estimated_w.T, 2, axis=0), shift=1), eg_vectors], axis=1)*1.0\n",
    "X2, Y2, U, V = zip(*soa)\n",
    "ax2.quiver(X2, Y2, U, V, angles='xy', scale_units='xy', scale=1,\n",
    "           color=\"y\", label=\"eigen vectors of covariance matrix\")\n",
    "ax2.legend(fontsize=14)\n",
    "ax2.set_xlabel(\"MSE for each w\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31988bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 7))\n",
    "ax2 = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "surf = ax2.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "ax2.set_ylabel(\"w[1]\", fontsize=14)\n",
    "ax2.set_xlabel(\"x[0]\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5968ac4",
   "metadata": {},
   "source": [
    "As we can see here, there is one direction where the MSE is relatively flat (small eigenvalue), that is why we see little variation in that direction by changing w, but it is still convex enough to have a unique solution. This is a frequentist way to solve a linear regression, is just a data driven solution with the only assumption of the model shape and that the MSE is a good way to measure performance, let's see now what is the bayesian way, which involves some distribution assumptions expressed using probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b1e898",
   "metadata": {},
   "source": [
    "# Linear Regression Maximum Likelihood Estimation\n",
    "Now, we define our model in a probabilistic way, that is, for example:\n",
    "\n",
    "$$ \\hat{y} = w^{T}x + \\epsilon \\hspace{0.5cm} \\text{with} \\hspace{0.5cm} \\epsilon \\sim \\mathcal{N}(0, \\sigma) $$\n",
    "\n",
    "This means that we are assuming that the data has gaussian noise of a given variance $\\sigma^{2}$. Now, we can write what is the probability of occurrence of the data Y, for a given input X and a model:\n",
    "\n",
    "$$ P(Y \\mid X, w, \\sigma) $$\n",
    "\n",
    "We call this \"Likelihood\". Now, it is very intuitive that if a particular value of w produces a high probability of seeing the data (high likelihood value), then w is a good choice for our model, so we define the maximum likelihood estimation of w as:\n",
    "\n",
    "$$ \\hat{w} = \\text{argmax}_{w}  P(Y \\mid X, w, \\sigma) = \\text{argmax}_{w} \\prod_{i}^{N}P(y_{i}, x_{i},w,\\sigma) = \\text{argmax}_{w} \\prod_{i}^{N}\\mathcal{N}(y_{i}; wx_{i}, \\sigma) $$\n",
    "\n",
    "where the last equality comes from the assumption of i.i.d samples. Now, this product over many probabilities can produce numerical underflow, so we can obtain a more convenient optimization problem. Instead of maximizing the likelihood, we minimize the \"negative log likelihood\" which is $NLL = -\\log(P(Y \\mid X, w, \\sigma))$, so the optimization problem is:\n",
    "\n",
    "$$ \\hat{w} = \\text{argmin} -\\log(P(Y \\mid X, w, \\sigma)) $$\n",
    "\n",
    "This expression to minimize arise naturally when we start by the optimization by minimizing the Kullback Leibler divergence between the estimated distribution of the data and the distribution produced by the model (we'll skip this for now). Something interesting happens when we work a little bit the expression of the NLL\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "NLL = & -\\log(\\prod_{i}^{N}\\mathcal{N}(y_{i}; wx_{i}, \\sigma)) \\\\\n",
    "= & -\\sum_{i=1}^{N}\\log(\\mathcal{N}(y_{i}; wx_{i}, \\sigma))\n",
    "= & -\\sum_{i=1}^{N}\\left [ \\log \\frac{1}{\\sqrt{2\\pi}\\sigma} - \\frac{(y_{i}-wx_{i})^{2}}{2 \\sigma^{2}} \\right ] \\propto  \\| \\hat{Y} - Y \\|^{2}_{2}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Least Mean Squared Error is equivalent to the Maximum Likelihood Estimation when gaussian noise in the data is assumed!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bba53d",
   "metadata": {},
   "source": [
    "# Capacity, overfitting and Underfitting\n",
    "\n",
    "Generally, in machine learning, the purpose of fitting a model to a dataset, is to evaluate the model on new data, not just the one that we use to train the model. The ability to perform well on new data is called generalization.\n",
    "\n",
    "In order to measure the generalization capacity of a trained model, we subtract a subset of the original dataset which is not used to train the model but to test it. Those sets are called train set (used to find the parameters) and test set (used to measure the generalization performance), then we can estimate the generalization ability of our model by measuring the error (or another metric of performance) on the test set. Here we are assuming that both sets, train and test sets are generated by the same data-generating process, that means i.i.d assumptions over every sample (as we did before in the maximum likelihood case)\n",
    "\n",
    "The objective of the optimization process is to make the training error small enough and make the gap between the training and test error small. The following concepts are useful to understand this process:\n",
    "\n",
    "- Capacity: ability of the model to fit a wide variety of functions (like relations between inputs and outputs)\n",
    "- Overfitting: Occurs when the gap between the training error and test error is too large, when the Capacity of the model is too high compared with the real complexity of the data-generating process\n",
    "- Underfitting: Happens when the model is not able to obtain a sufficiently low error value on the training set, may occur because the Capacity of the model is not enough to encode the complexity of the data-generating process\n",
    "\n",
    "Let's understand this with an example using a polinomial fitting over data. In the following example, the true complexity of the data is a quadratic polynomial. We fit models with low and high capacities compare with the real complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff15d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([-2, 0.6, 0.7])[...,np.newaxis]\n",
    "noise = 1.2\n",
    "n_points = 20\n",
    "train_size = 10\n",
    "test_size = n_points - train_size\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "x_samples = np.linspace(-2, 2, n_points)[..., np.newaxis]\n",
    "# Making quadratic polinomial\n",
    "augmented_x = np.concatenate([x_samples**2, x_samples, x_samples**0], axis=1)\n",
    "y_samples = linear_function(w, augmented_x) + np.random.normal(loc=0.0, scale=noise, size=(n_points,1))\n",
    "x_plot = np.linspace(-2,2,100)[..., np.newaxis]\n",
    "aug_x_plot = np.concatenate([x_plot**2, x_plot, x_plot**0], axis=1)\n",
    "\n",
    "# Dividing in train and test set\n",
    "indexes = np.arange(start=0, stop=n_points,step=1)\n",
    "np.random.shuffle(indexes)\n",
    "train_index = indexes[:train_size]\n",
    "test_index = indexes[train_size:]\n",
    "x_train = x_samples[train_index, ...]\n",
    "aug_x_train = augmented_x[train_index, ...]\n",
    "y_train = y_samples[train_index, ...]\n",
    "x_test = x_samples[test_index, ...]\n",
    "aug_x_test = augmented_x[test_index, ...]\n",
    "y_test = y_samples[test_index, ...]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,7))\n",
    "ax.plot(x_plot, linear_function(w, aug_x_plot), label=\"Real solution\")\n",
    "ax.scatter(x_train, y_train, label=\"train samples\", s=70)\n",
    "ax.scatter(x_test, y_test, label=\"test samples\", s=70)\n",
    "ax.legend(fontsize=14)\n",
    "ax.set_xlabel(\"x\", fontsize=14)\n",
    "ax.set_ylabel(\"y\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421570c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear, Quadratic 5 and 10 degree polynomial fit.\n",
    "linear_coef = np.polyfit(x_train[:, 0], y_train[:, 0], deg=1, full=True) \n",
    "qd_coef = np.polyfit(x_train[:, 0], y_train[:, 0], deg=2,full=True)\n",
    "deg5_coef = np.polyfit(x_train[:, 0], y_train[:, 0], deg=5, full=True) \n",
    "deg10_coef = np.polyfit(x_train[:, 0], y_train[:, 0], deg=10, full=True) \n",
    "p1 = np.poly1d(linear_coef[0])\n",
    "p2 = np.poly1d(qd_coef[0])\n",
    "p3 = np.poly1d(deg5_coef[0])\n",
    "p4 = np.poly1d(deg10_coef[0])\n",
    "error1 = np.linalg.norm(y_test[:, 0] - p1(x_test[:, 0]))**2/len(y_test)\n",
    "error2 = np.linalg.norm(y_test[:, 0] - p2(x_test[:, 0]))**2/len(y_test)\n",
    "error3 = np.linalg.norm(y_test[:, 0] - p3(x_test[:, 0]))**2/len(y_test)\n",
    "error4 = np.linalg.norm(y_test[:, 0] - p4(x_test[:, 0]))**2/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd27811",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generalization errors\")\n",
    "print(\"linear: \"+str(error1))\n",
    "print(\"quadratic: \"+str(error2))\n",
    "print(\"deg5: \"+str(error3))\n",
    "print(\"deg10: \"+str(error4))\n",
    "fig, ax = plt.subplots(figsize=(12,7))\n",
    "ax.plot(x_plot, linear_function(w, aug_x_plot), label=\"Real solution\", lw=3)\n",
    "ax.scatter(x_train, y_train, label=\"train samples\", s=70)\n",
    "ax.scatter(x_test, y_test, label=\"test samples\", s=70)\n",
    "ax.plot(x_plot, p1(x_plot),'--' ,label=\"linear (Underfitting)\")\n",
    "ax.plot(x_plot, p2(x_plot),'--' , label=\"quadratic (Appropiate Capacity)\")\n",
    "ax.plot(x_plot, p3(x_plot),'--' , label=\"5 deg (Not too overfitted)\" )\n",
    "ax.plot(x_plot, p4(x_plot),'--' , label=\"10 deg (Overfitting)\")\n",
    "ax.legend(fontsize=14)\n",
    "ax.set_xlabel(\"x\", fontsize=14)\n",
    "ax.set_ylabel(\"y\", fontsize=14)\n",
    "ax.set_ylim([-15, 5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5075edc3",
   "metadata": {},
   "source": [
    "In the last plot, we can see that the linear model does not have enough capacity to express the relation between x and y. Quadratic and deg 5 polynomial are good enough to find the relation. Some times, a high capacity model with a large family of functions that can represent (this is representational capacity), does not find the best solution during the optimization process, these additional limitations reduce the capacity of the actual solution, this is called the effective capacity. In the case of deg 10 polynomial, the capacity of the model is too high, so fits perfectly the train data but has a poor generalization ability.\n",
    "\n",
    "The capacity of the model can be modified by modifying the model or changing the effective capacity by adding restrictions to the loss function. This is called Regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b8e53f",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "## Regularized least squares\n",
    "In this example, we'll use the weight decay regularization, which tends to choose parameters on the solution space that are close to the origin (small euclidean norm). We just need to modify the loss function by adding one term:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    " J(w) = & MSE + R \\\\\n",
    " = & \\frac{1}{N} \\| \\hat{Y} - Y \\|^{2}_{2} + \\lambda \\| w \\|^{2}_{2}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Let's see how the solution looks by taking the gradient of J with respect to w\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{w} J(w) = 0 \\\\\n",
    "2X^{T}Xw-2X^{T}Y + 2\\lambda w=0 \\\\\n",
    "w = (X^{T}X + \\lambda I)^{-1}X^{T}Y\n",
    "\\end{equation}\n",
    "\n",
    "As we can see in the last expression, now we take the inverse of the correlation matrix plus the identity ponderated by $\\lambda$. This means that we are adding convexity to the problem because the eigenvalues of this new matrix are going to be bigger if we increase $\\lambda$ (we are making the matrix less singular), the convexity of this new manifold as a combination of a parabola because of the regularization term and the convexity of the original problem without the regularization. The optimization will tend to choose small norm w if we increase $\\lambda$\n",
    "\n",
    "Now, let's see an example by fitting a high capacity model but with this regularization term in the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d88bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same model as before\n",
    "w = np.array([-2, 0.6, 0.7])[...,np.newaxis]\n",
    "noise = 1.2\n",
    "n_points = 20\n",
    "train_size = 10\n",
    "test_size = n_points - train_size\n",
    "np.random.seed(0)    \n",
    "\n",
    "x_samples = np.linspace(-2, 2, n_points)[..., np.newaxis]\n",
    "augmented_x = np.concatenate([x_samples**2, x_samples, x_samples**0], axis=1)\n",
    "y_samples = linear_function(w, augmented_x) + np.random.normal(loc=0.0, scale=noise, size=(n_points,1))\n",
    "x_plot = np.linspace(-2,2,100)[..., np.newaxis]\n",
    "aug_x_plot = np.concatenate([x_plot**2, x_plot, x_plot**0], axis=1)\n",
    "\n",
    "indexes = np.arange(start=0, stop=n_points,step=1)\n",
    "np.random.shuffle(indexes)\n",
    "train_index = indexes[:train_size]\n",
    "test_index = indexes[train_size:]\n",
    "x_train = x_samples[train_index, ...]\n",
    "aug_x_train = augmented_x[train_index, ...]\n",
    "y_train = y_samples[train_index, ...]\n",
    "x_test = x_samples[test_index, ...]\n",
    "aug_x_test = augmented_x[test_index, ...]\n",
    "y_test = y_samples[test_index, ...]\n",
    "\n",
    "# Now we do it for high capacity model\n",
    "deg = 10\n",
    "x_deg = []\n",
    "for i in range(deg+1):\n",
    "    x_deg.append(x_samples**(deg-i))\n",
    "x_deg = np.concatenate(x_deg, axis=1)\n",
    "x_deg_train = x_deg[train_index, ...]\n",
    "x_deg_test = x_deg[test_index, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fcebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least square solution\n",
    "reg_values = [10**7, 0.5, 0]\n",
    "labels = [\"Too large lambda (Underfitting)\", \"appropiate lambda\", \"no regularization (Overfitting)\"]\n",
    "reg_w = []\n",
    "solution = []\n",
    "for i, lam in enumerate(reg_values):\n",
    "    # we save the regularized solution for each lambda\n",
    "    reg_w.append(inv(x_deg_train.T @ x_deg_train + lam*np.identity(deg+1)) @ x_deg_train.T @ y_train)\n",
    "    solution.append(np.poly1d(reg_w[-1][:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079e18e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_array = plt.subplots(1,3,figsize=(15,5))\n",
    "for i, lam in enumerate(reg_values):\n",
    "    ax_array[i].plot(x_plot, linear_function(w, aug_x_plot), label=\"Real solution\", lw=3)\n",
    "    ax_array[i].scatter(x_train, y_train, label=\"train samples\", s=70)\n",
    "    ax_array[i].scatter(x_test, y_test, label=\"test samples\", s=70)\n",
    "    p = solution[i]\n",
    "    ax_array[i].plot(x_plot, p(x_plot), label=\"Estimated solution\")\n",
    "    ax_array[i].set_ylim([-10, 5])\n",
    "    ax_array[i].set_title(labels[i], fontsize=14)\n",
    "    ax_array[i].legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952d7181",
   "metadata": {},
   "source": [
    "The last plot shows how the regularization term modifies the effective capacity of the model. For very high $\\lambda$ (left plot), the solutions are reduced to a very small region of the original space, so the capacity of the model is reduced too much and produce underfitting on the data. For very small $\\lambda$ (right plot), there is no penalization for the size of the weights and the model is able to look for solutions using its original capacity, so the model overfits. For a medium $\\lambda$ (middle plot), the effective capacity is probably close to the necessary one to find the correct function of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3ddfa9",
   "metadata": {},
   "source": [
    "## Probabilistic Perspective of Regularization, Maximum a Posteriori\n",
    "\n",
    "In this case, we will add information about the distribution of the parameters p(w) as a prior knowledge, by using Bayes' theorem to modify the likelihood in the following way:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "P(\\theta \\mid D) = & \\frac{P(D \\mid \\theta) P(\\theta)}{P(D)} \\\\\n",
    "\\propto & P(D \\mid w) P(w)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Where $D$ is the data and $\\theta$ the parameters of the model. $P(\\theta)$ is called \"prior\" since it is prior knowledge added to the model about how the parameters distribute, in some application, the designer of the model might have some idea where to look for the parameters for a particular problem. $P(D \\mid \\theta)$ is the likelihood as we already know. $P(\\theta \\mid D)$ is called \"posterior\" probability, it is the distribution of the parameters for a given data, basically  an update of our prior after seeing evidence of the real process (samples from the data-generating process)\n",
    "\n",
    "Let's consider our previous model and assume a gaussian distribution for the prior of the parameters\n",
    "\n",
    "$$ \\hat{y} = w^{T}x + \\epsilon \\hspace{0.5cm} \\text{with} \\hspace{0.5cm} \\epsilon \\sim \\mathcal{N}(0, \\sigma) \\hspace{0.5cm} \\text{with} \\hspace{0.5cm} p(w) \\sim \\mathcal{N(0, \\tau)} $$\n",
    "\n",
    "Then, the posterior probability of the parameters is proportional to:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "P(w \\mid Y,X,\\sigma , \\tau) \\propto P(Y \\mid X, w, \\sigma) P(w \\mid \\tau)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "If we find $w$ where the posterior probability is maximized, it means that for the given dataset and the prior knowledge, there is a high probability that the model with that value of $w$ is the one that produce the data. So the solution to the maximum a posteriori (MAP) is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\hat{w} = & \\text{argmax}_{w}  P(w \\mid Y, X, \\sigma, \\tau) \\\\\n",
    "= & \\text{argmin}_{w} -\\log(P(w \\mid Y, X, \\sigma, \\tau))\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Something interesting happens (again) when we work a little bit this expression\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "-\\log(P(w \\mid Y, X, \\sigma, \\tau)) = & -\\sum_{i=1}^{N}\\log \\mathcal{N}(y_{i}, wx_{i}, \\sigma)-\\log \\mathcal{N}(w; 0, \\tau) \\\\\n",
    "= & n \\log \\sqrt{2 \\pi} \\sigma + \\sum_{i=1}^{N} \\left ( \\frac{(y_{i}-wx_{i})^{2}}{2\\sigma^{2}} \\right ) + n\\log \\sqrt{2 \\pi \\tau} + \\sum_{i=1}^{N}\\left ( \\frac{w^{2}}{2 \\tau^{2}} \\right ) \\\\\n",
    "\\propto & \\| \\hat{Y} - Y \\|^{2}_{2} + \\frac{N \\sigma^{2}}{2 \\tau^{2}} \\| w \\|^{2}_{2} \\hspace{0.2cm} \\text{check this math please}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Regularized least mean squared is the same as maximum a posteriori with gaussian noise and gaussian prior! The amount of regularization, in this case, is controlled by the width of the gaussian prior $\\tau$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09337a29",
   "metadata": {},
   "source": [
    "# Hyperparameters and Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fdc8e1",
   "metadata": {},
   "source": [
    "Many models and regularization factors are subject to hyperparameters that must be chosen by the designer. With hyperparameters I mean, for example, the lambda for regularization, the degree of the polynomial, number of layer and neurons in a neural network,  gamma coefficient for support vector machines, etc.\n",
    "\n",
    "We should choose the hyperparameters that produce a better model to generalize over new data (test set). I good way to do this is used cross-validation, which is a procedure to have a better estimation of the generalization performance. Some times we do not have too many examples, so the random choice for the test set could be very sensitive to the realization, and of course, the generalization performance estimation too, cross-validation try to fix this problem by doing the following:\n",
    "\n",
    "#### K-fold cross-validation\n",
    "\n",
    "For a given dataset $D$, performance metric F and number of subsets k, we do:\n",
    "- Split D into k mutually exclusive subsets $D_{i}$ with $\\bigcup_{i=1}^{K} D_{i} = D$\n",
    "- For i from 1 to k:\n",
    "    - train the model with $D\\backslash D_{i}$\n",
    "    - Compute performance F over $D_{i}$\n",
    "- end for\n",
    "- Return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d28624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(lam, x_subsets, y_subsets):\n",
    "    train_error = []\n",
    "    test_error = []\n",
    "    for i, x_test in enumerate(x_subsets):\n",
    "        x_train = np.concatenate([x for j, x in enumerate(x_subsets) if j!=i], axis=0)\n",
    "        y_train = np.concatenate([y for j, y in enumerate(y_subsets) if j!=i], axis=0)\n",
    "        y_test = y_subsets[i]\n",
    "        w = inv(x_train.T @ x_train + lam*np.identity(x_train.shape[1])) @ x_train.T @ y_train\n",
    "        p = np.poly1d(w[:,0])\n",
    "        test_error.append(np.linalg.norm(y_test[:, 0] - p(x_test[:, -2]))**2/len(y_test))\n",
    "        train_error.append(np.linalg.norm(y_train[:, 0] - p(x_train[:, -2]))**2/len(y_train))\n",
    "    return np.array(train_error), np.array(test_error)\n",
    "\n",
    "def kfold_cv(x_data, y_data, lam_array, kfold=4):\n",
    "    x_subsets = np.split(x_data, kfold)\n",
    "    y_subsets = np.split(y_data, kfold)\n",
    "    \n",
    "    train_error_mean = []\n",
    "    test_error_mean = []\n",
    "    train_error_std = []\n",
    "    test_error_std = []\n",
    "    for j, lam in enumerate(lam_array):\n",
    "        print('\\r{}'.format(float(j/len(lam_array))*100), end='')\n",
    "        train_error, test_error = cross_validation(lam, x_subsets, y_subsets)\n",
    "        train_error_mean.append(np.mean(train_error))\n",
    "        train_error_std.append(np.std(train_error))\n",
    "        test_error_mean.append(np.mean(test_error))\n",
    "        test_error_std.append(np.std(test_error))\n",
    "    \n",
    "    return [np.array(train_error_mean), \n",
    "           np.array(train_error_std), \n",
    "           np.array(test_error_mean), \n",
    "           np.array(test_error_std)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2a6897",
   "metadata": {},
   "outputs": [],
   "source": [
    "lam_array = np.linspace(0.01, 10**8, 10000)\n",
    "one_over_lambda = 1.0/lam_array\n",
    "train_error_mean, train_error_std, test_error_mean, test_error_std = kfold_cv(x_deg, y_samples, lam_array)\n",
    "optimal_lambda = lam_array[np.where(test_error_mean==np.amin(test_error_mean))[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f7a865",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,7))\n",
    "ax.plot(lam_array, test_error_mean, label=\"test error\")\n",
    "ax.plot(lam_array, train_error_mean, label=\"train error\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"lambda (Less capacity <--- ---> More capacity)\", fontsize=14)\n",
    "ax.set_ylabel(\"errors log scale\", fontsize=14)\n",
    "ax.set_title(\"Cross validation results\", fontsize=14)\n",
    "ax.set_xlim([np.amax(lam_array), np.amin(lam_array)])\n",
    "ax.axvline(x=optimal_lambda, color='r', linestyle='--',\n",
    "           lw=4, label=\"Optimal lambda = \"+str(optimal_lambda))\n",
    "ax.legend(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee37aaf8",
   "metadata": {},
   "source": [
    "It is easy to see how the parameter $\\lambda$ change the effective capacity and produce a smooth transition between underfitting (left side of the optimal $\\lambda$, too large $\\lambda$) and overfitting (right side of the optimal $\\lambda$, too small $\\lambda$) regime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad780f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_w = (inv(x_deg_train.T @ x_deg_train + optimal_lambda*np.identity(deg+1)) @ x_deg_train.T @ y_train)\n",
    "deg_p = np.poly1d(w[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f91e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,7))\n",
    "ax.plot(x_plot, linear_function(w, aug_x_plot), label=\"Real solution\", lw=4)\n",
    "ax.scatter(x_train, y_train, label=\"train samples\", s=70)\n",
    "ax.scatter(x_test, y_test, label=\"test samples\", s=70)\n",
    "ax.plot(x_plot, deg_p(x_plot),'-o' ,label=\"regularized high capacity model\", lw=1,ms=4)\n",
    "ax.legend(fontsize=14)\n",
    "ax.set_xlabel(\"x\", fontsize=14)\n",
    "ax.set_ylabel(\"y\", fontsize=14)\n",
    "ax.set_ylim([-15, 5])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
