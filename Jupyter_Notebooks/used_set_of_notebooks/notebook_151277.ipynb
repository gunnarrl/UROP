{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f36af5a9",
   "metadata": {},
   "source": [
    "# 线性回归模型中使用梯度下降法\n",
    "\n",
    "\n",
    "输入数据，迭代计算，到一定次数，`theta`就不变了\n",
    "\n",
    "找到最小值了，找到最低点\n",
    "\n",
    "最小值不是0，没有必要做无谓的挣扎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3852ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eef2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不是随机生成，具有可重复性\n",
    "np.random.seed(666)\n",
    "x = 2*np.random.random(size=100)\n",
    "y = x*3 + 4 + np.random.normal(size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a497de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x.reshape(-1, 1) # -1 代表任意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29db464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fa2fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed1ff6c",
   "metadata": {},
   "source": [
    "## 使用梯度下降法训练\n",
    "![](images/01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace1e8f6",
   "metadata": {},
   "source": [
    "#### 求偏导(梯度)\n",
    "#### $$ \\frac{\\delta J(\\theta)}{\\delta\\theta_{j}} = \\frac{1}{m}\\sum_{i=1}^{m} ( h_\\theta (x^{(i)})-y^{(i)})x^{(i)}_{j} $$ \n",
    "#### 向量化的偏导(梯度)\n",
    "#### $$ \\frac{\\delta J(\\theta)}{\\delta\\theta_{j}} = \\frac{1}{m} X^T(g(X\\theta)-y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9251029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数\n",
    "def J(theta, X_b, y):\n",
    "    try:\n",
    "        return np.sum((y - X_b.dot(theta))**2) / len(X_b)\n",
    "    except:\n",
    "        return float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5503794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度计算\n",
    "def dJ(theta, X_b, y):\n",
    "    res = np.empty(len(theta))\n",
    "    res[0] = np.sum(X_b.dot(theta) - y) # 第0个参数\n",
    "    \n",
    "    for i in range(1, len(theta)):\n",
    "        # 每一个样本取出 对应特征 对应的列\n",
    "        res[i] = np.sum((X_b.dot(theta) -y).dot(X_b[:, i]))\n",
    "        \n",
    "    return res * 2 / len(X_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c40beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e3 ,epsilon=1e-8):\n",
    "    \n",
    "    theta = initial_theta\n",
    "    i_iter = 0\n",
    "    \n",
    "    while i_iter < n_iters:\n",
    "        gradient = dJ(theta, X_b, y)\n",
    "        last_theta = theta\n",
    "        theta = theta - eta * gradient # 向导数的负方向移1步\n",
    "    \n",
    "        # 是不是最小值的点，导数等于0 的点 --- 可能永远达不到这个精度\n",
    "        # 每一次损失函数都要小一点\n",
    "        if(abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):\n",
    "            break\n",
    "        \n",
    "        i_iter += 1\n",
    "        \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bc6679",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = np.hstack([np.ones((len(x), 1)), x.reshape(-1, 1)]) # 变成列向量\n",
    "initial_theta = np.zeros(X_b.shape[1])\n",
    "eta = 0.01\n",
    "\n",
    "theta = gradient_descent(X_b, y, initial_theta, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a378d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a4475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e619346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0aac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 就是我们要计算的 b w\n",
    "# 截距 斜率\n",
    "# 找到最低点，我的theta就不变了\n",
    "# 此时的theta=[b, w]决定的直线，与原始数据误差最小！！\n",
    "theta "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40273835",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae74b2a6",
   "metadata": {},
   "source": [
    "## 封装自己的线性回归算法\n",
    "参见 [代码](playML/LinearRegression.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879970c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from playML.LinearRegression import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit_gd(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9285e806",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg._theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3164414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 系数\n",
    "lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba2cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 系数\n",
    "lin_reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9e30a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg2 = LinearRegression()\n",
    "lin_reg2.fit_normal(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ead7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg2.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b853fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg2.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a0f251",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac12daf",
   "metadata": {},
   "source": [
    "## 梯度下降法向量化提速\n",
    "\n",
    "以前要for循环\n",
    "\n",
    "形同形式，矩阵之间的乘法\n",
    "![](images/02.png)\n",
    "\n",
    "- 算法表示都是列向量\n",
    "- numpy中向量不区分行、列向量的（没有列向量），基本都是1维向量\n",
    "- 数学表示上，都是列向量，转置变成行向量！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d03430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dJ(theta, X_b, y):\n",
    "    # res = np.empty(len(theta))\n",
    "    # res[0] = np.sum(X_b.dot(theta) - y)\n",
    "    # for i in range(1, len(theta)):\n",
    "    #     res[i] = (X_b.dot(theta) - y).dot(X_b[:, i])\n",
    "    # return res * 2 / len(X_b)\n",
    "    return X_b.T.dot(X_b.dot(theta) - y) * 2. / len(X_b)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
