{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "417cd2a0",
   "metadata": {},
   "source": [
    "# Fleet Clustering\n",
    "\n",
    "### Tim Hochberg, 2019-01-16\n",
    "\n",
    "## Purse Seine Edition\n",
    "\n",
    "We cluster vessel using HDBSCAN and a custom metric to derive fleets\n",
    "that are related in the sense that they spend a lot of time in the same\n",
    "location while at sea.\n",
    "\n",
    "## See Also\n",
    "\n",
    "* Other notebooks in https://github.com/GlobalFishingWatch/fleet-clustering for \n",
    "examples of clustering Squid Jiggers, etc.\n",
    "* This workspace that Nate put together: https://globalfishingwatch.org/map/workspace/udw-v2-85ff8c4f-fbfe-4126-b067-4d94cdd2b737"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa2cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from collections import Counter, OrderedDict\n",
    "import datetime as dt\n",
    "import hdbscan\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as mpl_animation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import color\n",
    "from IPython.display import HTML\n",
    "from fleet_clustering import bq\n",
    "from fleet_clustering import filters\n",
    "from fleet_clustering import distances\n",
    "from fleet_clustering import animation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8731de",
   "metadata": {},
   "source": [
    "## Load AIS Clustering Data\n",
    "\n",
    "Load the AIS data that we use for clustering. Note that it onlyu includes vessels away\n",
    "from shores so as to exclude clustering on ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4ecfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_by_date = bq.load_ais_by_date(('purse_seines', 'tuna_purse_seines', 'other_purse_seines'), \n",
    "                                  dt.date(2016, 1, 1), dt.date(2018, 12, 31),\n",
    "                                 fishing_only=False, min_km_from_shore=0)    \n",
    "pruned_by_date = {k : filters.remove_near_shore(10,\n",
    "                            filters.remove_chinese_coast(v)) for (k, v) in all_by_date.items()}\n",
    "valid_ssvid = sorted(filters.find_valid_ssvid(pruned_by_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5390640",
   "metadata": {},
   "source": [
    "## Create Distance Metrics\n",
    "\n",
    "Create an array of distance metrics. The details are still evolving, but in general\n",
    "we want to deal with two things.  Days on which a boat is missing and days where the\n",
    "boat is away from the fleet.\n",
    "\n",
    "* Distances to/from a boat on days when it is missing are represented by $\\infty$ in \n",
    "  the distance matrix. HDBSCAN ignores these values.\n",
    "* Only the closest N days are kept for each boat pair, allowing boats to leave the fleet\n",
    "  for up to half the year without penalty.\n",
    "  \n",
    "In addition, distances have a floor of 1 km to prevent overclustering when boats tie up\n",
    "up together, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d835ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists_by_date = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08e281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for start_date, end_date in [('20160101', '20161231'),\n",
    "                             ('20170101', '20171231'), \n",
    "                             ('20180101', '20181231')]:\n",
    "    if start_date in dists_by_date:\n",
    "        continue\n",
    "    print(\"computing distance for\", start_date, end_date)\n",
    "    subset_by_date = {k : v for (k, v) in pruned_by_date.items() if start_date <= k <= end_date}\n",
    "    C = distances.create_composite_lonlat_array(subset_by_date, valid_ssvid)\n",
    "    dists = distances.compute_distances_4(C, gamma=2)\n",
    "    dists_by_date[start_date] = dists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44768fe6",
   "metadata": {},
   "source": [
    "## Load Carrier Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d76437",
   "metadata": {},
   "outputs": [],
   "source": [
    "carriers_by_date = bq.load_carriers_by_year(2017, 2018)\n",
    "pruned_carriers_by_date = {k : filters.remove_chinese_coast(v) for (k, v) in carriers_by_date.items()}\n",
    "query = \"\"\"\n",
    "               SELECT CAST(mmsi AS STRING) FROM\n",
    "               `world-fishing-827.vessel_database.all_vessels_20190102`\n",
    "               WHERE  iscarriervessel AND confidence = 3\n",
    "        \"\"\"\n",
    "valid_carrier_ssvid_df = pd.read_gbq(query, dialect='standard', project_id='world-fishing-827')\n",
    "valid_carrier_ssvid = valid_carrier_ssvid_df.f0_\n",
    "valid_carrier_ssvid_set = set(valid_carrier_ssvid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b531623",
   "metadata": {},
   "source": [
    "## Load Encounters Data And Country Codes\n",
    "\n",
    "This is used to filter the carrier vessels down to only those\n",
    "that meet with target vessels and to add iso3 labels to outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b470e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters = bq.load_carriers(2017, 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427860de",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT code, iso3 FROM `world-fishing-827.gfw_research.country_codes`\"\"\"\n",
    "country_codes_df = pd.read_gbq(query, dialect='standard', project_id='world-fishing-827')\n",
    "iso3_map = {x.code : x.iso3 for x in country_codes_df.itertuples()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5318375",
   "metadata": {},
   "source": [
    "## Fit the Clusterer\n",
    "\n",
    "This is pretty straightforward -- all the complicated stuff is\n",
    "embedded in the matrix computations. Fleet size can be tweaked\n",
    "using `min_cluster_size` and `min_sample_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7443ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_clusterers = {}\n",
    "for start_date, dists in dists_by_date.items():\n",
    "    clusterer = hdbscan.HDBSCAN(metric='precomputed', \n",
    "                                min_cluster_size=9,\n",
    "                               )\n",
    "    clusterer.fit(dists)\n",
    "    raw_clusterers[start_date] = clusterer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd39f5c8",
   "metadata": {},
   "source": [
    "## Create Psuedo Distance From Fleet Membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7458403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdists_by_date = {}\n",
    "for date in ['20160101', '20170101', '20180101']:\n",
    "    pdists = np.zeros_like(dists_by_date[date])\n",
    "    raw_labels = np.asarray(raw_clusterers[date].labels_)\n",
    "    SCALE = 1000\n",
    "    UNKNOWN_FLEET_DIST = 1 * SCALE\n",
    "    OTHER_FLEET_DIST = 2 * SCALE\n",
    "    mask = (raw_labels == -1)\n",
    "    for i, fid in enumerate(raw_labels):\n",
    "        if fid == -1:\n",
    "            pdists[i] = UNKNOWN_FLEET_DIST\n",
    "        else:\n",
    "            pdists[i] = OTHER_FLEET_DIST * (raw_labels != fid)\n",
    "            pdists[i, mask] = UNKNOWN_FLEET_DIST\n",
    "    pdists_by_date[date] = pdists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1918a4c7",
   "metadata": {},
   "source": [
    "## Set up Fleets\n",
    "\n",
    "Set up the fleets for viewing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af383094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rgb(string):\n",
    "    string = string.strip('#')\n",
    "    r = string[:2]\n",
    "    g = string[2:4]\n",
    "    b = string[4:]\n",
    "    return [int(x, 16) / 225.0 for x in (r, g, b)]\n",
    "\n",
    "\n",
    "def find_labels(dists):\n",
    "    clusterer = hdbscan.HDBSCAN(metric='precomputed', \n",
    "                                min_cluster_size=9).fit(dists)\n",
    "    \n",
    "    all_fleet_ssvid_set = set([s for (s, f) in zip(valid_ssvid, clusterer.labels_) if f >= 0])\n",
    "    valid_ssvid_set = set(valid_ssvid)\n",
    "    all_fleet_reefer_ssvid_set = set()\n",
    "    for x in encounters.itertuples():\n",
    "        if x.ssvid_1 in all_fleet_ssvid_set and x.ssvid_2 in valid_carrier_ssvid_set:\n",
    "            all_fleet_reefer_ssvid_set.add(x.ssvid_2)\n",
    "        if x.ssvid_2 in all_fleet_ssvid_set and x.ssvid_1 in valid_carrier_ssvid_set:\n",
    "            all_fleet_reefer_ssvid_set.add(x.ssvid_1)\n",
    "    all_fleet_reefer_ssvid = sorted(all_fleet_reefer_ssvid_set)\n",
    "\n",
    "    valid_ssvid_set = set(valid_ssvid)\n",
    "    carrier_ids = [x for x in all_fleet_reefer_ssvid if x not in valid_ssvid_set]\n",
    "    joint_ssvid = valid_ssvid + sorted(carrier_ids) \n",
    "    labels = list(clusterer.labels_) + [max(clusterer.labels_) + 1] * len(carrier_ids) \n",
    "\n",
    "    # Remove vessels that have no connection to other vessels\n",
    "    for i, ssvid in enumerate(valid_ssvid):\n",
    "        connections = (~np.isinf(dists[i])).sum()\n",
    "        if connections == 0:\n",
    "            labels[i] = -1\n",
    "            \n",
    "    return joint_ssvid, labels\n",
    "\n",
    "\n",
    "def create_fleet_mapping(labels, include_carriers=False):\n",
    "    counts = []\n",
    "    skip = []\n",
    "    for i in range(max(labels) + 1):\n",
    "        if i in skip:\n",
    "            counts.append(0)\n",
    "        else:\n",
    "            counts.append((np.array(labels) == i).sum())\n",
    "\n",
    "    fleet_ids = [x for x in np.argsort(counts)[::-1] if counts[x] > 0]\n",
    "    fleet_ids_without_carriers = [x for x in fleet_ids if x != max(labels)]\n",
    "\n",
    "    fleets = OrderedDict()\n",
    "    n_hues = int(np.ceil(len(fleet_ids) / 4.0))\n",
    "    used = set()\n",
    "    for i, fid in enumerate(fleet_ids_without_carriers):\n",
    "        b = (i // (2 * n_hues)) % 2\n",
    "        c = (i // 2)% n_hues\n",
    "        d = i  % 2\n",
    "        symbol = 'H^'[d]\n",
    "        assert (b, c, d) not in used, (i, b, c, d)\n",
    "        used.add((b, c, d))\n",
    "        sat = 1\n",
    "        val = 1\n",
    "        raw_hue = c / float(n_hues)\n",
    "        # We remap the raw hue in order to avoid the 60 degree segment around blue\n",
    "        hue = 5. / 6. * raw_hue\n",
    "        if hue > 7. / 12.:\n",
    "            hue += 1. / 6.\n",
    "        assert 0 <= hue < 1, hue\n",
    "        [[clr]] = color.hsv2rgb([[(hue, sat, val)]])\n",
    "        fg = [[0.1511111111111111, 0.2, 0.3333333333333333], clr][b]\n",
    "        bg = [clr, [0.1511111111111111, 0.2, 0.3333333333333333]][b]\n",
    "        w = [1, 2][b]\n",
    "        sz = [9, 7][b]\n",
    "        fleets[fid] = (symbol, tuple(fg), tuple(bg), sz, w,  str(i + 1))\n",
    "    if include_carriers:\n",
    "        fleets[max(labels)] = ('1', 'k', 'k', 8, 2, 'Carrier Vessel')\n",
    "    print(len(set([x for x in fleets if x != -1])), \"fleets\")\n",
    "    return fleets\n",
    "    \n",
    "    \n",
    "def iou(a, b):\n",
    "    a = set(a)\n",
    "    b = set(b)\n",
    "    return len(a & b) / len(a | b)\n",
    "\n",
    "def best_match(a, bs):\n",
    "    ious = [iou(a, b) for b in bs]\n",
    "    i = np.argmax(ious)\n",
    "    if ious[i] == 0:\n",
    "        return None\n",
    "    return i\n",
    "\n",
    "    \n",
    "def adapt_fleet_mapping(base_fleets, base_ssvid, base_labels, new_ssivd, new_labels):\n",
    "    new_labels = np.array(new_labels)\n",
    "    ssvid_base = []\n",
    "    base_fleet_ids = sorted(base_fleets)\n",
    "    for fid in base_fleet_ids:\n",
    "        mask = (base_labels == fid)\n",
    "        ssvid_base.append(np.array(base_ssvid)[mask])\n",
    "\n",
    "    ssvid_new = []\n",
    "    new_fleet_ids = sorted(set([x for x in new_labels if x != -1]))\n",
    "    for fid in new_fleet_ids:\n",
    "        mask = (new_labels == fid)\n",
    "        ssvid_new.append(np.array(new_ssivd)[mask])\n",
    "\n",
    "    rev_mapping = {}\n",
    "    for fid, ssvid_list in zip(new_fleet_ids, ssvid_new):\n",
    "        i = best_match(ssvid_list, ssvid_base)\n",
    "        if i is None:\n",
    "            rev_mapping[fid] = None\n",
    "        else:\n",
    "            rev_mapping[fid] = base_fleet_ids[i]\n",
    "            \n",
    "    mapping = {}\n",
    "    for k, v in rev_mapping.items():\n",
    "        if v in mapping:\n",
    "            mask = (new_labels == k)\n",
    "            new_labels[mask] = mapping[v]\n",
    "        else:\n",
    "            mapping[v] = k\n",
    "                         \n",
    "    new_fleets = OrderedDict()\n",
    "    for i, fid in enumerate(base_fleets):\n",
    "        if fid in mapping and mapping[fid] is not None:\n",
    "            k = mapping[fid]\n",
    "            if k in new_fleets:\n",
    "                print(\"Skipping\", k, fid, \"because of double match\")\n",
    "                new_fleets[i + max(base_fleets)] = base_fleets[fid]\n",
    "            else:\n",
    "                new_fleets[mapping[fid]] = base_fleets[fid]\n",
    "        else:\n",
    "            new_fleets[i + max(base_fleets)] = base_fleets[fid]\n",
    "            \n",
    "    return new_fleets, new_labels\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5412b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_ssvid_2017, labels_2017 = find_labels(dists_by_date['20170101'])\n",
    "fleets_2017 = create_fleet_mapping(labels_2017)\n",
    "all_by_date_2017 = {k : v for (k, v) in all_by_date.items() if '20170101' <= k <= '20171231'}\n",
    "\n",
    "anim = animation.make_anim(joint_ssvid_2017, \n",
    "                           labels_2017, \n",
    "                           all_by_date_2017, \n",
    "                           interval=100,\n",
    "                           fleets=fleets_2017, \n",
    "                           show_ungrouped=True,\n",
    "                           alpha=1,\n",
    "                           legend_cols=12,\n",
    "                           ungrouped_legend=\"Ungrouped\")\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f466fed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_ssvid_2016, labels_2016 = find_labels(dists_by_date['20160101'] +                                \n",
    "                                            pdists_by_date['20170101'])\n",
    "fleets_2016, labels_2016 = adapt_fleet_mapping(fleets_2017, joint_ssvid_2017, labels_2017, \n",
    "                                  joint_ssvid_2016, labels_2016)\n",
    "all_by_date_2016 = {k : v for (k, v) in all_by_date.items() if '20160101' <= k <= '20161231'}\n",
    "\n",
    "anim = animation.make_anim(joint_ssvid_2016, \n",
    "                           labels_2016, \n",
    "                           all_by_date_2016, \n",
    "                           interval=100,\n",
    "                           fleets=fleets_2016, \n",
    "                           show_ungrouped=True,\n",
    "                           alpha=1,\n",
    "                           legend_cols=12,\n",
    "                           ungrouped_legend=\"Ungrouped\")\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca87f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_ssvid_2018, labels_2018 = find_labels(dists_by_date['20180101'] +                                \n",
    "                                            pdists_by_date['20170101'])\n",
    "fleets_2018, labels_2018 = adapt_fleet_mapping(fleets_2017, joint_ssvid_2017, labels_2017, \n",
    "                                  joint_ssvid_2018, labels_2018)\n",
    "all_by_date_2018 = {k : v for (k, v) in all_by_date.items() if '20180101' <= k <= '20181231'}\n",
    "\n",
    "anim = animation.make_anim(joint_ssvid_2018, \n",
    "                           labels_2018, \n",
    "                           all_by_date_2018, \n",
    "                           interval=100,\n",
    "                           fleets=fleets_2018, \n",
    "                           show_ungrouped=True,\n",
    "                           alpha=1,\n",
    "                           legend_cols=12,\n",
    "                           ungrouped_legend=\"Ungrouped\")\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7f6121",
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animation.make_anim(joint_ssvid_2017, \n",
    "                           labels_2017, \n",
    "                           all_by_date_2017, \n",
    "                           interval=1,\n",
    "                           fleets=fleets_2017, \n",
    "                           show_ungrouped=True,\n",
    "                           alpha=1,\n",
    "                           legend_cols=12,\n",
    "                           ungrouped_legend=\"Ungrouped\")\n",
    "Writer = mpl_animation.writers['ffmpeg']\n",
    "writer = Writer(fps=8, metadata=dict(artist='Me'), bitrate=1800)\n",
    "anim.save('fleet_purse_seines_2017.mp4', writer=writer,  \n",
    "          savefig_kwargs={'facecolor':'#222D4B'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d259ec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animation.make_anim(joint_ssvid_2018, \n",
    "                           labels_2018, \n",
    "                           all_by_date_2018, \n",
    "                           interval=1,\n",
    "                           fleets=fleets_2018, \n",
    "                           show_ungrouped=True,\n",
    "                           alpha=1,\n",
    "                           legend_cols=12,\n",
    "                           ungrouped_legend=\"Ungrouped\")\n",
    "Writer = mpl_animation.writers['ffmpeg']\n",
    "writer = Writer(fps=8, metadata=dict(artist='Me'), bitrate=1800)\n",
    "anim.save('fleet_purse_seines_2018.mp4', writer=writer,  \n",
    "          savefig_kwargs={'facecolor':'#222D4B'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a189f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animation.make_anim(joint_ssvid_2016, \n",
    "                           labels_2016, \n",
    "                           all_by_date_2016, \n",
    "                           interval=1,\n",
    "                           fleets=fleets_2016, \n",
    "                           show_ungrouped=True,\n",
    "                           alpha=1,\n",
    "                           legend_cols=12,\n",
    "                           ungrouped_legend=\"Ungrouped\")\n",
    "Writer = mpl_animation.writers['ffmpeg']\n",
    "writer = Writer(fps=8, metadata=dict(artist='Me'), bitrate=1800)\n",
    "anim.save('fleet_purse_seines_2016.mp4', writer=writer,  \n",
    "          savefig_kwargs={'facecolor':'#222D4B'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97adf872",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(joint_ssvid_2017) == len(labels_2017)\n",
    "with open('fleet_purse_seiners_2017.csv', 'w') as f:\n",
    "    f.write('mmsi,fleet\\n')\n",
    "    for mmsi, fid in zip(joint_ssvid_2017, labels_2017):\n",
    "        if fid != -1 and fid in fleets_2017:\n",
    "            f.write(\"{},{}\\n\".format(mmsi, fleets_2017[fid][-1]))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
