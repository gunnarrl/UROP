{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "320ab93e",
   "metadata": {},
   "source": [
    "#### This code generates large dataframe containing multiple timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4097faea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "#from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from pandas.tseries.offsets import *\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "#from graphviz import Source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d300afa",
   "metadata": {},
   "source": [
    "#### parameters to set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72490efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_series = 6\n",
    "start_date = '2017-08-01 00:00:00'\n",
    "end_date = '2017-08-07 23:59:59'\n",
    "\n",
    "# regular behaviour\n",
    "max_noise_amplitude = 0.05 # all the timeseries will have values between 0 and 1\n",
    "\n",
    "# anomalies\n",
    "p_anomaly = 10E-6\n",
    "max_anomaly_duration = 4*3600 # 4 h\n",
    "\n",
    "# tuning parameters\n",
    "cut = 0.55\n",
    "window = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324d2abb",
   "metadata": {},
   "source": [
    "#### generate normal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500cf7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dti=pd.DatetimeIndex(start=start_date,end=end_date, freq='s')\n",
    "n_timesteps = len(dti)\n",
    "df = pd.DataFrame()\n",
    "for s in range(n_series):\n",
    "    v = np.random.normal(random.random()/2, max_noise_amplitude/random.randint(1, 8), n_timesteps) \n",
    "    df['link '+str(s)] = pd.Series(v)\n",
    "df['Flag']=0\n",
    "df['auc_score']=0.5\n",
    "df.index = dti\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294b62cd",
   "metadata": {},
   "source": [
    "#### generate anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cac661",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_generate = int(n_timesteps * p_anomaly)\n",
    "for a in range(to_generate):\n",
    "    affects = random.sample(range(n_series), random.randint(1, n_series))\n",
    "    duration = int(max_anomaly_duration * random.random())\n",
    "    start = int(n_timesteps * random.random())\n",
    "    end = min(start+duration, n_timesteps)\n",
    "    print('affected:', affects, df.iloc[start].name, df.iloc[end].name)\n",
    "    for s in affects:\n",
    "        df.iloc[start:end,s] = df.iloc[start:end,s] + random.random() * 0.2\n",
    "    df.iloc[start:end,n_series]=1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79228824",
   "metadata": {},
   "source": [
    "#### enforce range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21b06c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df<0] = 0\n",
    "df[df>1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94caf38d",
   "metadata": {},
   "source": [
    "#### plot timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecde810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.plot(figsize=(20,7))\n",
    "fig = plt.figure(figsize=(20,7))\n",
    "plt.plot(df)\n",
    "fig.suptitle('Simulated Data', fontsize=20)\n",
    "plt.xlabel('Time', fontsize=18)\n",
    "plt.ylabel('Variance in Data', fontsize=16)\n",
    "plt.legend(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086dc09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=[16, 17])\n",
    "#gs = gridspec.GridSpec(4, 1)\n",
    "\n",
    "#ax2.plot(df)\n",
    "#ax2.set_xlabel('time')\n",
    "#ax2.set_ylabel('variance in data')\n",
    "#ax2.legend()\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c91ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['auc_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fdad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b61005",
   "metadata": {},
   "source": [
    "#### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79b7a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_anomaly(ref, sub):\n",
    "    \n",
    "    y_ref = pd.Series([0] * ref.shape[0])\n",
    "    X_ref = ref\n",
    "    #print(\"y_ref before: \", y_ref)\n",
    "    #print(\"x_ref before: \", x_ref)\n",
    "    del X_ref['Flag']\n",
    "    del X_ref['auc_score']\n",
    "    #print(\"x_ref after: \", X_ref)\n",
    "    \n",
    "    y_sub = pd.Series([1] * sub.shape[0])\n",
    "    X_sub=sub\n",
    "    #print(\"y_sub before: \", y_sub)\n",
    "    #print(\"x_sub before: \", X_sub)\n",
    "    del X_sub['Flag']\n",
    "    del X_sub['auc_score']\n",
    "    #print(\"X_sub after: \", X_sub)\n",
    "    \n",
    "    # separate Reference and Subject into Train and Test\n",
    "    X_ref_train, X_ref_test, y_ref_train, y_ref_test = train_test_split(X_ref, y_ref, test_size=0.3, random_state=42)\n",
    "    X_sub_train, X_sub_test, y_sub_train, y_sub_test = train_test_split(X_sub, y_sub, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # combine training ref and sub samples\n",
    "    X_train = pd.concat([X_ref_train, X_sub_train])\n",
    "    y_train = pd.concat([y_ref_train, y_sub_train])\n",
    "\n",
    "    # combine testing ref and sub samples\n",
    "    X_test = pd.concat([X_ref_test, X_sub_test])\n",
    "    y_test = pd.concat([y_ref_test, y_sub_test])\n",
    "    \n",
    "#     dtc=DecisionTreeClassifier()\n",
    "    clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=6)) #dtc\n",
    "#     clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),algorithm=\"SAMME\",n_estimators=200)\n",
    "    \n",
    "    #train an AdaBoost model to be able to tell the difference between the reference and subject data\n",
    "    clf.fit(X_train, y_train) \n",
    "\n",
    "    #Predict using the combined test data\n",
    "    y_predict = clf.predict(X_test)\n",
    "    \n",
    "    # scores = cross_val_score(clf, X, y)\n",
    "    # print(scores)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_predict) # calculate the false positive rate and true positive rate\n",
    "    auc_score = auc(fpr, tpr) #calculate the AUC score\n",
    "    print (\"auc_score = \", auc_score, \"\\tfeature importances:\", clf.feature_importances_)\n",
    "    \n",
    "    if auc_score > cut: \n",
    "        plot_roc(fpr, tpr, auc_score)\n",
    "        #filename='tree_'+sub.index.min().strftime(\"%Y-%m-%d_%H\")\n",
    "        #tree.export_graphviz(clf.estimators_[0] , out_file=filename +'_1.dot') \n",
    "        #tree.export_graphviz(clf.estimators_[1] , out_file=filename +'_2.dot') \n",
    "        \n",
    "    return auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca20b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(fpr,tpr, roc_auc):\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='r',label='Luck', alpha=.8)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831fcc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = df.index.min()\n",
    "end = df.index.max()\n",
    "\n",
    "#print(df)\n",
    "\n",
    "print(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59ba85f",
   "metadata": {},
   "source": [
    "#### Looping over time intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a83d893",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#find min and max timestamps\n",
    "\n",
    "start = df.index.min()\n",
    "end = df.index.max()\n",
    "\n",
    "#round start \n",
    "start.seconds=0\n",
    "start.minutes=0\n",
    "\n",
    "ref = window * Hour()\n",
    "sub = 1 * Hour()\n",
    "\n",
    "# loop over them\n",
    "ti=start+ref+sub\n",
    "count=0\n",
    "while ti < end + 1 * Minute():\n",
    "    ref_start = ti-ref-sub\n",
    "    ref_end = ti-sub\n",
    "    ref_df = df[(df.index >= ref_start) & (df.index < ref_end)]\n",
    "    #print(\"In while loop: ref_df: \", ref_df)\n",
    "    sub_df = df[(df.index >= ref_end) & (df.index < ti)]\n",
    "    #print(\"In while loop: sub_df: \", sub_df)\n",
    "    auc_score = check_for_anomaly(ref_df, sub_df)\n",
    "    df.loc[(df.index>=ref_end) & (df.index<=ti),['auc_score']] = auc_score\n",
    "    #print(ti,\"\\trefes:\" , ref_df.shape[0], \"\\tsubjects:\", sub_df.shape[0], '\\tauc:', auc_score)\n",
    "    ti = ti + sub\n",
    "    count=count+1\n",
    "    #if count>2: break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea60716",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d52c629",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(start)\n",
    "print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3070f2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.plot(figsize=(20,7))\n",
    "\n",
    "df.plot(figsize=(20,7))\n",
    "plt.xlabel('Time', fontsize=18)\n",
    "plt.ylabel('Variance in Data', fontsize=16)\n",
    "plt.legend(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d15e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,7))\n",
    "df.loc[:,'Detected'] = 0\n",
    "df.loc[df.auc_score>0.55,'Detected']=1\n",
    "df.head()\n",
    "ax.plot(df.Flag, 'r')\n",
    "ax.plot(df.auc_score,'g')\n",
    "ax.fill( df.Detected, 'b', alpha=0.3)\n",
    "ax.legend(loc='upper left')\n",
    "plt.xlabel('Time', fontsize=18)\n",
    "plt.ylabel('AUC score', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
