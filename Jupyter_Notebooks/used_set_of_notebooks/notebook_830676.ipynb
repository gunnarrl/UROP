{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0831960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import data\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.contrib.learn import learn_runner\n",
    "from tensorflow.contrib.learn import make_export_strategy\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d4e4a7",
   "metadata": {},
   "source": [
    "## Steps to use the TF Estimator APIs\n",
    "1. Define dataset **metadata**\n",
    "2. Define **data input function** to read the data from the source (csv) + **apply pre-processing**\n",
    "3. Instantiate an **estimator** (KMeans) with **parameters**\n",
    "4. **Fit** the estimator\n",
    "5. **Predict** cluster index of each instance\n",
    "6. **Save** the model and serve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d559450",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_files = ['data/train-data.csv']\n",
    "test_data_files = ['data/test-data.csv']\n",
    "\n",
    "model_name = 'clust-model-01'\n",
    "\n",
    "resume = False\n",
    "train = True\n",
    "preprocess_features = False\n",
    "extend_feature_colums = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8a098c",
   "metadata": {},
   "source": [
    "## 1. Define Dataset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114f4c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER = ['key', 'x1', 'x2', 'x3', 'cluster']  \n",
    "HEADER_DEFAULTS = [[0], [0.0], [0.0], [0.0], ['NA']]\n",
    "\n",
    "FEATURE_NAMES = ['x1', 'x2', 'x3']  \n",
    "\n",
    "UNUSED_FEATURE_NAMES = list(set(HEADER) - set(FEATURE_NAMES))\n",
    "\n",
    "print(\"Header: {}\".format(HEADER))\n",
    "print(\"Input Features: {}\".format(FEATURE_NAMES))\n",
    "print(\"Unused Features: {}\".format(UNUSED_FEATURE_NAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756ba3de",
   "metadata": {},
   "source": [
    "## 2. Define Data Input Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a1d050",
   "metadata": {},
   "source": [
    "### a. parsing and preprocessing logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39ba59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv_row(csv_row):\n",
    "    \n",
    "    columns = tf.decode_csv(csv_row, record_defaults=HEADER_DEFAULTS)\n",
    "    columns = [tf.expand_dims(tensor, -1) for tensor in columns]\n",
    "    features = dict(zip(HEADER, columns))\n",
    "    \n",
    "    for column in UNUSED_FEATURE_NAMES:\n",
    "        features.pop(column)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def process_features(features):\n",
    "    \n",
    "    if preprocess_features:\n",
    "        features = features\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6576e681",
   "metadata": {},
   "source": [
    "### b. data pipeline input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661a0c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_input_fn(file_names, mode=tf.estimator.ModeKeys.TRAIN, \n",
    "                 skip_header_lines=0, \n",
    "                 num_epochs=None, \n",
    "                 batch_size=200):\n",
    "    \n",
    "    shuffle = False\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"* data input_fn:\")\n",
    "    print(\"================\")\n",
    "    print(\"Input file(s): {}\".format(file_names))\n",
    "    print(\"Batch size: {}\".format(batch_size))\n",
    "    print(\"Epoch Count: {}\".format(num_epochs))\n",
    "    print(\"Mode: {}\".format(mode))\n",
    "    print(\"Shuffle: {}\".format(shuffle))\n",
    "    print(\"================\")\n",
    "    print(\"\")\n",
    "\n",
    "    dataset = data.TextLineDataset(filenames=file_names)\n",
    "    dataset = dataset.skip(skip_header_lines)\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=2 * batch_size + 1)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(lambda csv_row: parse_csv_row(csv_row))\n",
    "    dataset = dataset.map(lambda features: process_features(features))\n",
    "    \n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    iterator = dataset.make_one_shot_iterator() \n",
    "    \n",
    "    features = iterator.get_next()\n",
    "    return features, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4a7ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, _ = csv_input_fn(file_names=train_data_files)\n",
    "print(\"Feature read from CSV: {}\".format(list(features.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7ec941",
   "metadata": {},
   "source": [
    "## 3. Build an Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaaded4",
   "metadata": {},
   "source": [
    "### a. Define Estimator Creation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa6693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_estimator(run_config, hparams):\n",
    "    \n",
    "    estimator = tf.contrib.learn.KMeansClustering(\n",
    "        num_clusters = hparams.num_clusters,\n",
    "        initial_clusters= tf.contrib.factorization.RANDOM_INIT,\n",
    "        distance_metric= tf.contrib.factorization.SQUARED_EUCLIDEAN_DISTANCE,\n",
    "        use_mini_batch=True,\n",
    "        mini_batch_steps_per_iteration=1,\n",
    "        kmeans_plus_plus_num_retries=10,\n",
    "        relative_tolerance=None,\n",
    "        config= run_config\n",
    "    )\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Estimator Type: {}\".format(type(estimator)))\n",
    "    print(\"\")\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d54a058",
   "metadata": {},
   "source": [
    "### b. Set HParam and RunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51ecddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams  = tf.contrib.training.HParams(\n",
    "    num_epochs = 1000,\n",
    "    batch_size = 500,\n",
    "    num_clusters=3\n",
    ")\n",
    "\n",
    "model_dir = 'trained_models/{}'.format(model_name)\n",
    "\n",
    "run_config = tf.contrib.learn.RunConfig(\n",
    "    save_checkpoints_steps=100,\n",
    "    tf_random_seed=19850610,\n",
    "    model_dir=model_dir\n",
    ")\n",
    "\n",
    "print(run_config.model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a279fa1c",
   "metadata": {},
   "source": [
    "## 4. Create Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8422d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = lambda: csv_input_fn(\n",
    "            train_data_files,\n",
    "            mode = tf.contrib.learn.ModeKeys.TRAIN,\n",
    "            num_epochs=hparams.num_epochs,\n",
    "            batch_size=hparams.batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411b2cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not resume:\n",
    "    print(\"Removing previous artifacts...\")\n",
    "    shutil.rmtree(model_dir, ignore_errors=True)\n",
    "else:\n",
    "    print(\"Resuming training...\") \n",
    "\n",
    "if train:\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    time_start = datetime.utcnow() \n",
    "    print(\"Training started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "    print(\".......................................\") \n",
    "    \n",
    "    estimator = create_estimator(hparams=hparams, run_config=run_config)\n",
    "    estimator.fit(input_fn=train_input_fn,\n",
    "                  max_steps=None)\n",
    "\n",
    "    time_end = datetime.utcnow() \n",
    "    print(\".......................................\")\n",
    "    print(\"Training finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "    print(\"\")\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(\"Training elapsed time: {} seconds\".format(time_elapsed.total_seconds()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc18c13b",
   "metadata": {},
   "source": [
    "## 5. Perform Predictions (Assign Instance to Clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e4fe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = lambda: csv_input_fn(\n",
    "            train_data_files,\n",
    "            num_epochs=1,\n",
    "            batch_size=1500\n",
    "        )\n",
    "\n",
    "test_input_fn = lambda: csv_input_fn(\n",
    "            test_data_files,\n",
    "            num_epochs=1,\n",
    "            batch_size=500\n",
    "        )\n",
    "\n",
    "train_assignments = list(estimator.predict_cluster_idx(input_fn=train_input_fn))\n",
    "test_assignments = list(estimator.predict_cluster_idx(input_fn=test_input_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8666daae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_df = pd.read_csv(train_data_files[0], header=None, index_col=0)\n",
    "test_df = pd.read_csv(test_data_files[0], header=None, index_col=0)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "\n",
    "ax.scatter(train_df.iloc[:,0], train_df.iloc[:,1], train_df.iloc[:,2], c=train_assignments, marker='o')\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "ax.scatter(test_df.iloc[:,0], test_df.iloc[:,1], test_df.iloc[:,2], c=test_assignments, marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de3061",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = estimator.clusters()\n",
    "print(\"Cluster Centriods:\")\n",
    "print(\"==================\")\n",
    "print(clusters)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
