{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aadfe96f",
   "metadata": {},
   "source": [
    "**Advanced Lane Finding Project**\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b64efe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db64d042",
   "metadata": {},
   "source": [
    "### Camera Calibration\n",
    "I start by preparing \"object points\", which will be the (x, y, z) coordinates of the chessboard corners in the world. Here I am assuming the chessboard is fixed on the (x, y) plane at z=0, such that the object points are the same for each calibration image.  Thus, `obj` is just a replicated array of coordinates, and `object_points` will be appended with a copy of it every time I successfully detect all chessboard corners in a test image.  `image_points` will be appended with the (x, y) pixel position of each of the corners in the image plane with each successful chessboard detection.  \n",
    "\n",
    "I then used the output `object_points` and `image_points` to compute the camera calibration and distortion coefficients using the `cv2.calibrateCamera()` function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bde127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_camera(path):\n",
    "    object_points=[]\n",
    "    image_points=[]\n",
    "    images=glob.glob(path)\n",
    "    img_size=()\n",
    "    obj=np.zeros((9*6,3),np.float32)\n",
    "    obj[:,:2]=np.mgrid[0:9,0:6].T.reshape(-1,2)\n",
    "    for idx,image in enumerate(images):\n",
    "        img=cv2.imread(image)\n",
    "        gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        ret,corners=cv2.findChessboardCorners(gray,(9,6),None)\n",
    "        if ret:\n",
    "            image_points.append(corners)\n",
    "            object_points.append(obj)\n",
    "        img_size=(img.shape[1],img.shape[0])\n",
    "    ret,mtx,dist,rvecs,tvecs=cv2.calibrateCamera(object_points,image_points,img_size,None,None)\n",
    "    return mtx,dist \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d406ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtx,dist=calibrate_camera('./camera_cal/calibration*.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4e1284",
   "metadata": {},
   "source": [
    "In the previous step I calculated *Camera Matrix* and *Distortion Coefficients* using `cv2.calibrateCamera` now I can use these values to undistort any image taken from the same camera using `cv2.undistort`. The result after distorting image looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d9f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image=cv2.imread('./camera_cal/calibration2.jpg')\n",
    "undistort=cv2.undistort(test_image,mtx,dist,None,mtx)\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "ax1.imshow(cv2.cvtColor(test_image,cv2.COLOR_BGR2RGB))\n",
    "ax1.set_title('Original Image', fontsize=30)\n",
    "ax2.imshow(cv2.cvtColor(undistort,cv2.COLOR_BGR2RGB))\n",
    "ax2.set_title('Undistorted Image', fontsize=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fce6207",
   "metadata": {},
   "source": [
    "### Perspective Transform\n",
    "I started with choosing four reference points in the image to use them as `src_points` and four points for `dest_points`. The source points reflects on destination points after the perspective transform. The first step is to calculate the transformation matrix using `cv2.getPerspectiveTransform()` this function takes source and destination points as input.\n",
    "\n",
    "| Source        | Destination   | \n",
    "|:-------------:|:-------------:| \n",
    "|  200, 720     | 300, 720      | \n",
    "|  600, 447     | 300, 0        |\n",
    "|  679, 447     | 900, 0        |\n",
    "|  1100,720     | 900, 720      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9f7e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate transformation matrix for perspective transform.\n",
    "def get_transformation_matrix(src,dest):\n",
    "    transformation_matrix=cv2.getPerspectiveTransform(src,dest)\n",
    "    return transformation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e503600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#warp the image\n",
    "def perspective_transform(image,transformation_matrix):\n",
    "    img_size=(image.shape[1],image.shape[0])\n",
    "    transformed_image=cv2.warpPerspective(image,transformation_matrix,img_size,flags=cv2.INTER_LINEAR)\n",
    "    return transformed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73df504e",
   "metadata": {},
   "source": [
    "#### Perform Perspective Transform\n",
    "Once we have the transformation matrix we can use `cv2.warpPerspective` to perform perspective transform of an image. The result after the transform looks like this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2aeab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#undistort Image\n",
    "transformation_test=cv2.imread('./frames/frame2.jpg')\n",
    "undistort=cv2.undistort(transformation_test,mtx,dist,None,mtx)\n",
    "\n",
    "\n",
    "#src_points=np.float32([[790,490],[1090,670],[250,670],[515,490]])\n",
    "#dest_points=np.float32([[1100,0],[1100,720],[200,720],[200,0]])\n",
    "#src_points=np.float32([[800,490],[1100,650],[210,650],[500,490]])\n",
    "#dest_points=np.float32([[1100,0],[1100,720],[200,720],[200,0]])\n",
    "\n",
    "#perform perspective transform\n",
    "src_points = np.float32([[200,720], [600, 447], [679,447], [1100,720]])\n",
    "dest_points = np.float32([[300, 720], [300, 0], [900, 0], [900, 720]])\n",
    "transformation_matrix=get_transformation_matrix(src_points,dest_points)\n",
    "transformed_image=perspective_transform(undistort,transformation_matrix)\n",
    "\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "ax1.imshow(cv2.cvtColor(transformation_test,cv2.COLOR_BGR2RGB))\n",
    "ax1.set_title('Original Image', fontsize=30)\n",
    "ax2.imshow(cv2.cvtColor(transformed_image,cv2.COLOR_BGR2RGB))\n",
    "ax2.set_title('Transformed Image', fontsize=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2e0a32",
   "metadata": {},
   "source": [
    "### Image Thresholding\n",
    "After trying different strategies to apply thresholding finally I applied HLS Colour space and Sobel thresholding to fecth features of interest from the frame, here features of intrest are Lane lines. I used Sobel along the X axis that is `sobelx` and later applied thresholding over it, and I also applied thresholding over HLS colour space to get a binary image. \n",
    "*Values for Thresholding*\n",
    "\n",
    "| Applied On    | Range         | \n",
    "|:-------------:|:-------------:| \n",
    "|  Sobel X      | 25 - 120      | \n",
    "|  l_channel    | 70 - 255       |\n",
    "|  s_channel    | 90 - 255        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40337bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sobel(image,thresholds,kernel_size=3,Color_Encoding='RGB'):\n",
    "    if(Color_Encoding=='BGR'):\n",
    "        gray=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray=cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n",
    "  \n",
    "    sobelx=cv2.Sobel(gray,cv2.CV_64F,1,0,ksize=kernel_size)\n",
    "    sobel_scaled=np.uint8(255*np.absolute(sobelx)/np.max(sobelx))\n",
    "    binary_image=np.zeros_like(sobel_scaled)\n",
    "    binary_image[(sobel_scaled>=thresholds[0]) & (sobel_scaled<=thresholds[1])]=1\n",
    "    return binary_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbb4fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sobel_and_color_thresholding(image,thresholds_sobel,threshold_l,thresholds_s,kernel_size=3,Color_Encoding='RGB'):\n",
    "    \n",
    "    if(Color_Encoding=='BGR'):\n",
    "        gray=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "        hls=cv2.cvtColor(image,cv2.COLOR_BGR2HLS)\n",
    "    else:\n",
    "        gray=cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n",
    "        hls=cv2.cvtColor(image,cv2.COLOR_RGB2HLS)\n",
    "    s_channel=hls[:,:,2]\n",
    "    l_channel=hls[:,:,1]\n",
    "    sobelx=cv2.Sobel(gray,cv2.CV_64F,1,0,ksize=kernel_size)\n",
    "    scaled_sobel=np.uint8(255*np.absolute(sobelx)/np.max(sobelx))\n",
    "    binary_image=np.zeros_like(scaled_sobel)\n",
    "    binary_image[((scaled_sobel>=thresholds_sobel[0]) & (scaled_sobel<=thresholds_sobel[1]))|((hls[:,:,2]>=thresholds_s[0]) & (hls[:,:,2]<=thresholds_s[1]) & (hls[:,:,1]>=threshold_l[0]) & (hls[:,:,1]<=threshold_l[1]))]=1\n",
    "    return binary_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6de39e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sobel_direction(image,thresholds_sobel,thresholds_direction,kernel_size=3,Color_Encoding='RGB'):\n",
    "    if(Color_Encoding=='BGR'):\n",
    "        gray=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray=cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n",
    "    sobelx=cv2.Sobel(gray,cv2.CV_64F,1,0,ksize=kernel_size)\n",
    "    sobely=cv2.Sobel(gray,cv2.CV_64F,0,1,ksize=kernel_size)\n",
    "    sobel_direction=np.arctan2(np.absolute(sobelx),np.absolute(sobely))\n",
    "    sobel_scaled=np.uint8(255*np.absolute(sobelx)/np.max(sobelx))\n",
    "    binary_image=np.zeros_like(sobel_scaled)\n",
    "    binary_image[(sobel_direction>=thresholds_direction[0]) & (sobel_direction<=thresholds_direction[1]) &(sobel_scaled>=thresholds_sobel[0]) & (sobel_scaled<=thresholds_sobel[1])]=1\n",
    "    return binary_image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab7405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_thresholding(image,l_threshold,s_threshold):\n",
    "    hls=cv2.cvtColor(image,cv2.COLOR_BGR2HLS)\n",
    "    binary_image=np.zeros_like(hls[:,:,0])\n",
    "    binary_image[(hls[:,:,2]>=s_threshold[0]) & (hls[:,:,2]<=s_threshold[1]) & (hls[:,:,1]>=l_threshold[0]) & (hls[:,:,1]<=l_threshold[1])]=1\n",
    "    return binary_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4c4c87",
   "metadata": {},
   "source": [
    "#### Test Thresholding\n",
    "Below is the image that my function returns after applying thresholding over it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0824df3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_image=apply_sobel_and_color_thresholding(transformation_test,[25,120],[70,255],[90,255])\n",
    "plt.imshow(binary_image,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17423e7",
   "metadata": {},
   "source": [
    "### Thresholding and Perspective Transform\n",
    "Below is the result that my pipeline returned after applying thresholding and later performing perspective transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48517386",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "transformed_image=perspective_transform(binary_image,transformation_matrix)\n",
    "output_image=np.dstack((transformed_image,transformed_image,transformed_image))*255\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "ax1.imshow(cv2.cvtColor(transformation_test,cv2.COLOR_BGR2RGB))\n",
    "ax1.set_title('Original Image', fontsize=30)\n",
    "ax2.imshow(output_image)\n",
    "ax2.set_title('Transformed & Thresholded Image', fontsize=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb30de2",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "I used a class to retain the data from previous frames, my pipeline function is also a member of this class.\n",
    "Steps performed:\n",
    "    1. For the First Frame calculate Camera matrix, distortion Coefficients, transformation matrix and inverse transformation matrix.\n",
    "    2. Undistort the Image\n",
    "    3. Apply Thresholding\n",
    "    4. Take Perspective Transform\n",
    "    5. Find out the pixel indices that are part of lane lines.\n",
    "    6. Fit the polynomial to the left and right lane pixels.\n",
    "    7. Take the mean of the previous polynomial coefficients and the coefficeint calculated in step 6.\n",
    "    8. Calculate the x points for each value of Y(0-719) using the equation AY^2+BY+C.\n",
    "    9. Then using `cv2.fillPoly()` fill the lane line area.\n",
    "    10. Take the inverse Perspective transform of the image received from step 9.\n",
    "    11. Add the original image and image from step 10.\n",
    "    12. Calculate Radius of curvature and deviation from center and put it over the image received after step 12.\n",
    "    \n",
    "### Deviation from center\n",
    "To calculate the deviation from center I assumed that the center of the image is the center of the car, then I simply calculated the center of the lane lines and the differnce between center of the image and center of the lane lines is required deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc32b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaneLineFinding(object):\n",
    "    #prev_left_x=np.array([])\n",
    "    #prev_left_y=np.array([])\n",
    "    #prev_right_x=np.array([])\n",
    "    #prev_right_y=np.array([])\n",
    "    prev_left_coeff=np.array([])\n",
    "    prev_right_coeff=np.array([])\n",
    "    last_left_fit=[]\n",
    "    last_right_fit=[]\n",
    "    mtx=None\n",
    "    dist=None\n",
    "    trans_matrix=None\n",
    "    inverse_trans_matrix=None\n",
    "    flag=True\n",
    "    def pipeline(self,image):\n",
    "        if(self.flag):\n",
    "            self.mtx,self.dist,self.trans_matrix,self.inverse_trans_matrix=get_variables()\n",
    "        undistort=cv2.undistort(image,self.mtx,self.dist,None,self.mtx)\n",
    "        #binary_image=apply_sobel_and_color_thresholding(transformed_image,[12,170],[48,255])\n",
    "        #binary_image=color_thresholding(undistort,[75,255],[85,255])\n",
    "        binary_image=apply_sobel_and_color_thresholding(undistort,[25,120],[70,255],[90,255])#[70,120],[120,255],[90,255])#[70,120],[100,255],[90,255])#[25,150],[90,255],[90,255])\n",
    "        transformed_image=perspective_transform(binary_image,self.trans_matrix)\n",
    "        if(self.flag):\n",
    "            self.flag=False\n",
    "            left_x,left_y,right_x,right_y=sliding_window(transformed_image)\n",
    "        else:\n",
    "            left_x,left_y,right_x,right_y=find_lane_lines(transformed_image,self.last_left_fit,self.last_right_fit)\n",
    "        \n",
    "        \n",
    "        #output= np.dstack((transformed_image,np.zeros_like(binary_image),np.zeros_like(binary_image)))*255\n",
    "        #plt.imshow(binary_image)\n",
    "        #print(output)\n",
    "        #if(len(self.prev_left_x)>=720*10):\n",
    "        #    self.prev_left_x=self.prev_left_x[720:len(self.prev_left_x)]\n",
    "        #if(len(self.prev_right_x)>=720*10):\n",
    "        #    self.prev_right_x=self.prev_right_x[720:len(self.prev_right_x)]\n",
    "        #if(len(self.prev_left_y)>=720*10):\n",
    "        #    self.prev_left_y=self.prev_left_y[720:len(self.prev_left_y)]\n",
    "        #if(len(self.prev_right_y)>=720*10):\n",
    "        #    self.prev_right_y=self.prev_right_y[720:len(self.prev_right_y)]\n",
    "        \n",
    "        #self.prev_left_x=np.append(self.prev_left_x,left_x)\n",
    "        \n",
    "        #self.prev_left_y=np.append(self.prev_left_y,left_y)\n",
    "        #self.prev_right_x=np.append(self.prev_right_x,right_x)\n",
    "        #self.prev_right_y=np.append(self.prev_right_y,right_y)\n",
    "       \n",
    "        #left_fit=np.polyfit(self.prev_left_y, self.prev_left_x,2)\n",
    "        #right_fit=np.polyfit(self.prev_right_y,self.prev_right_x,2)\n",
    "        \n",
    "        left_fit=np.polyfit(left_y, left_x,2)\n",
    "        right_fit=np.polyfit(right_y,right_x,2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if(len(self.prev_left_coeff)>=10*3):\n",
    "            self.prev_left_coeff=self.prev_left_coeff[3:len(self.prev_left_coeff)]\n",
    "        if(len(self.prev_right_coeff)>=3*10):\n",
    "            self.prev_right_coeff=self.prev_right_coeff[3:len(self.prev_right_coeff)]\n",
    "       \n",
    "        self.prev_right_coeff=np.append(self.prev_right_coeff,right_fit)\n",
    "        self.prev_left_coeff=np.append(self.prev_left_coeff,left_fit)\n",
    "        \n",
    "       \n",
    "        left_fit=np.mean(self.prev_left_coeff.reshape(-1,3),axis=0)\n",
    "        right_fit=np.mean(self.prev_right_coeff.reshape(-1,3),axis=0)\n",
    "        self.last_left_fit=left_fit\n",
    "        self.last_right_fit=right_fit\n",
    "       \n",
    "    \n",
    "        ploty = np.linspace(0, image.shape[0]-1, image.shape[0] )\n",
    "        left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "        right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "        output_image=np.zeros_like(image)\n",
    "       \n",
    "        pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "        pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "        pts = np.hstack((pts_left, pts_right))\n",
    "        \n",
    "    # Draw the lane onto the warped blank image\n",
    "        cv2.fillPoly(output_image, np.int_([pts]), (0, 255, 0))\n",
    "        final_image=perspective_transform(output_image,self.inverse_trans_matrix)\n",
    "        output_image=cv2.addWeighted(image,1,final_image,0.5,0)\n",
    "        \n",
    "        \n",
    "        left_curverad,right_curverad=calculate_radius_of_curvature(left_fit,right_fit,np.max(ploty))\n",
    "        cv2.putText(output_image,'Radius Of Curvature = {0:.2f}(m)'.format(np.mean([left_curverad,right_curverad])),(50,50), 2, 1,(255,255,255),2,cv2.LINE_AA)\n",
    "        deviation_from_center=((image.shape[1]/2) - (((np.absolute(right_fitx[-1]-left_fitx[-1]))/2)+left_fitx[-1]))*(3.7/600)\n",
    "        if(deviation_from_center>0):\n",
    "            cv2.putText(output_image,'vehicle is {0:.2f}(m) left from the center'.format(np.absolute(deviation_from_center)),(50,100), 2, 1,(255,255,255),2,cv2.LINE_AA)\n",
    "        else:\n",
    "            cv2.putText(output_image,'vehicle is {0:.2f}(m) right from the center'.format(np.absolute(deviation_from_center)),(50,100), 2, 1,(255,255,255),2,cv2.LINE_AA)\n",
    "        return output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00652147",
   "metadata": {},
   "source": [
    "### Sliding Window\n",
    "I have used sliding window approach to find out the indices of the pixels those are part of the lane lines. This approach uses a window and only considers nonzero pixels within that window, in my implementaion I have used 9 windows hence there will be 9 windows for left lane and 9 windows for right lane, each of 80 pixel height and 200 pixels width. \n",
    "This function returns the `left_x`,`left_y` for left lane and `right_x`,`right_y` for right lane. These points I used in Pipeline to fit a polynomial.\n",
    "\n",
    "Sliding window is not being used by each frame, since consecutive frames will not have much difference in  lane line positions, hence we are serching in a given margin for the next frame say 100 in my case. I have defined `find_lane_lines()` for this purpose this function also returns the `left_x`,`left_y` for left lane and `right_x`,`right_y` for right lane. These points I used in Pipeline to fit a polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b35a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(binary_warped,visualize=False):\n",
    "    histogram=np.sum(binary_warped[binary_warped.shape[0]//2:,:],axis=0)\n",
    "    out_img = np.dstack((binary_warped, binary_warped, binary_warped))*255\n",
    "# Find the peak of the left and right halves of the histogram\n",
    "# These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]//2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "    # Choose the number of sliding windows\n",
    "    nwindows = 9\n",
    "    # Set height of windows\n",
    "    window_height = np.int(binary_warped.shape[0]//nwindows)\n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    # Current positions to be updated for each window\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 100\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 50\n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "\n",
    "    # Step through the windows one by one\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = binary_warped.shape[0] - (window+1)*window_height\n",
    "        win_y_high = binary_warped.shape[0] - window*window_height\n",
    "        win_xleft_low = leftx_current - margin\n",
    "        win_xleft_high = leftx_current + margin\n",
    "        win_xright_low = rightx_current - margin\n",
    "        win_xright_high = rightx_current + margin\n",
    "        # Draw the windows on the visualization image\n",
    "        #cv2.rectangle(out_img,(win_xleft_low,win_y_low),(win_xleft_high,win_y_high),\n",
    "        #(0,255,0), 2) \n",
    "        #cv2.rectangle(out_img,(win_xright_low,win_y_low),(win_xright_high,win_y_high),\n",
    "        #(0,255,0), 2) \n",
    "        # Identify the nonzero pixels in x and y within the window\n",
    "        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "        (nonzerox >= win_xleft_low) &  (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "        (nonzerox >= win_xright_low) &  (nonzerox < win_xright_high)).nonzero()[0]\n",
    "        # Append these indices to the lists\n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        # If you found > minpix pixels, recenter next window on their mean position\n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "        if len(good_right_inds) > minpix:        \n",
    "            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "    # Concatenate the arrays of indices\n",
    "    left_lane_inds = np.concatenate(left_lane_inds)\n",
    "    right_lane_inds = np.concatenate(right_lane_inds)\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds] \n",
    "    \n",
    "    if(visualize):\n",
    "    # Fit a second order polynomial to each\n",
    "        left_fit = np.polyfit(lefty, leftx, 2)\n",
    "        right_fit = np.polyfit(righty, rightx, 2)\n",
    "        ploty = np.linspace(0, binary_warped.shape[0]-1, binary_warped.shape[0] )\n",
    "        left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "        right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "        out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "        out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "        plt.imshow(out_img)\n",
    "        plt.plot(left_fitx, ploty, color='yellow')\n",
    "        plt.plot(right_fitx, ploty, color='yellow')\n",
    "        plt.xlim(0, 1280)\n",
    "        plt.ylim(720, 0)\n",
    "        return None\n",
    "    return leftx,lefty,rightx,righty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31bcc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lane_lines(binary_warped,left_fit,right_fit):\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    margin = 100\n",
    "    left_lane_inds = ((nonzerox > (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + \n",
    "    left_fit[2] - margin)) & (nonzerox < (left_fit[0]*(nonzeroy**2) + \n",
    "    left_fit[1]*nonzeroy + left_fit[2] + margin))) \n",
    "\n",
    "    right_lane_inds = ((nonzerox > (right_fit[0]*(nonzeroy**2) + right_fit[1]*nonzeroy + \n",
    "    right_fit[2] - margin)) & (nonzerox < (right_fit[0]*(nonzeroy**2) + \n",
    "    right_fit[1]*nonzeroy + right_fit[2] + margin)))  \n",
    "\n",
    "    # Again, extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "    # Fit a second order polynomial to each\n",
    "    #left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    #right_fit = np.polyfit(righty, rightx, 2)\n",
    "    # Generate x and y values for plotting\n",
    "    #ploty = np.linspace(0, binary_warped.shape[0]-1, binary_warped.shape[0] )\n",
    "    #left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    #right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    return leftx,lefty,rightx,righty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612c43c0",
   "metadata": {},
   "source": [
    "#### Fit the Polynomial\n",
    "Below is the result after identifying the lane line points and fitting a polynomial to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0443c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliding_window(transformed_image,visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b409b951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variables():\n",
    "    mtx,dist=calibrate_camera('./camera_cal/calibration*.jpg')\n",
    "    \n",
    "    #1src_points=np.float32([[800,470],[1100,720],[210,720],[500,470]])\n",
    "    #1dest_points=np.float32([[1100,0],[1100,720],[200,720],[200,0]])\n",
    "    #src_points=np.float32([[790,490],[1090,670],[250,670],[515,490]])\n",
    "    #dest_points=np.float32([[1100,0],[1100,720],[200,720],[200,0]])\n",
    "    \n",
    "    \n",
    "    src_points = np.float32([[200,720], [600, 447], [679,447], [1100,720]])\n",
    "    dest_points = np.float32([[300, 720], [300, 0], [900, 0], [900, 720]])\n",
    "    \n",
    "    \n",
    "    transformation_matrix=get_transformation_matrix(src_points,dest_points)\n",
    "    inverse_transformation_matrix=get_transformation_matrix(dest_points,src_points)\n",
    "    return  mtx,dist,transformation_matrix,inverse_transformation_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec350e9e",
   "metadata": {},
   "source": [
    "### Radius of curvature\n",
    "Once we have the polynomial coefficients we can use them to find out the radius of curvature. I have scaled the Radius of curvature in the meters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e93ad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_radius_of_curvature(left_fit_cr,right_fit_cr,y_eval):\n",
    "    ym_per_pix = 40/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/600 # meters per pixel in x dimension\n",
    "\n",
    "# Fit new polynomials to x,y in world space\n",
    "    #left_fit_cr = np.polyfit(ploty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "    #right_fit_cr = np.polyfit(ploty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "# Calculate the new radii of curvature\n",
    "    left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "    right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "# Now our radius of curvature is in meters\n",
    "    return left_curverad,right_curverad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba65ffa",
   "metadata": {},
   "source": [
    "### Pipeline Output\n",
    "Below is the example of Pipeline output for a frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334adc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as matimage\n",
    "test=LaneLineFinding()\n",
    "test_image=matimage.imread('./test_images/straight_lines1.jpg')\n",
    "test_output=test.pipeline(test_image)\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "ax1.imshow(test_image)\n",
    "ax1.set_title('Original Image', fontsize=30)\n",
    "ax2.imshow(test_output)\n",
    "ax2.set_title('Final Image', fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a4d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "video_processor=LaneLineFinding()\n",
    "white_output = 'project_video_output.mp4'\n",
    "\n",
    "clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "white_clip = clip1.fl_image(video_processor.pipeline) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(white_output, audio=False)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
