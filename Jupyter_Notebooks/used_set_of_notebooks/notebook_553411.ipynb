{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fe837e8",
   "metadata": {},
   "source": [
    "# VADER Sentiment Analysis of Bitcointalk Topics\n",
    "***Ronald DeLuca***<br>\n",
    "\n",
    "python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce29159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "from textblob.base import BaseSentimentAnalyzer\n",
    "from textblob.sentiments import NaiveBayesAnalyzer, PatternAnalyzer\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from operator import methodcaller\n",
    "from operator import attrgetter\n",
    "from spacy.tokens import Doc\n",
    "from collections import Counter\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from matplotlib import pyplot as plt\n",
    "pd.options.display.max_colwidth = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980c8539",
   "metadata": {},
   "source": [
    "### Input CSV Name Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafc6a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvName = 'ATS_20190925-235549'   # Change this string value to the file you want\n",
    "#csvName = 'GTC_20190925-021514'\n",
    "#csvName = 'PGT_20190923-153422'\n",
    "#csvName = 'TICS_20190925-045521'\n",
    "# The inputCsvName represents a .csv file located in the data/raw_data/*.csv folder\n",
    "inputCsvName = 'data/raw_data/' + csvName + '.csv'  # Change the relative or absolute path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa933074",
   "metadata": {},
   "outputs": [],
   "source": [
    "Analyzer = SentimentIntensityAnalyzer()\n",
    "# Custom Cryptocurrency word sentiment values\n",
    "new_words = {\n",
    "    'hold': 0.5,\n",
    "    'lambo': 1.5,\n",
    "    'moon': 1.5,\n",
    "    'mooning': 1.6,\n",
    "    'bull': 1,\n",
    "    'bear': -0.5,\n",
    "    'shill': -1,\n",
    "    'shilling': -1.5,\n",
    "    'pump': -0.75,\n",
    "    'decentralized': 0.5,\n",
    "    'noob': -0.5,\n",
    "    'whale': 0.5,\n",
    "    '51%': -1,\n",
    "    'denial': -1.4,\n",
    "    'fundamental': 0.1,\n",
    "    'analysis': 0.3,\n",
    "    'oracle': 0.25,\n",
    "    'shitcoin': -3,\n",
    "    'volatile': -0.75,\n",
    "}\n",
    "# Update the VADER lexicon with these additional sentiment values\n",
    "Analyzer.lexicon.update(new_words)\n",
    "\n",
    "# Rate the post content and categorize it based on compound score\n",
    "def vader_polarity(text):\n",
    "    \"\"\" Transform the output to a binary 0/1 result \"\"\"\n",
    "    score = Analyzer.polarity_scores(text)\n",
    "    total_positive_score = score['pos']\n",
    "    total_negative_score = score['neg']\n",
    "    total_neutral_score = score['neu']\n",
    "    compound_score = score['compound']\n",
    "    \n",
    "    if (total_neutral_score > 1 and total_positive_score > total_negative_score and total_positive_score >= total_neutral_score):\n",
    "        sentiment = 'Positive'\n",
    "    elif (total_neutral_score > 1 and total_negative_score > total_positive_score and total_negative_score >= total_neutral_score):\n",
    "        sentiment = 'Negative'\n",
    "    elif (total_neutral_score > 1 and total_neutral_score > total_positive_score and total_neutral_score > total_negative_score):\n",
    "        sentiment = 'Neutral'\n",
    "    elif (total_neutral_score > 1 and total_negative_score == total_positive_score and total_negative_score >= total_neutral_score):\n",
    "        sentiment = 'Neutral'\n",
    "    elif (total_neutral_score <= 1 and total_positive_score == total_negative_score and total_positive_score == total_neutral_score):\n",
    "        sentiment = \"Neutral\"\n",
    "    elif (total_neutral_score <= 1 and total_positive_score > total_negative_score):\n",
    "        sentiment = \"Positive\"\n",
    "    elif (total_neutral_score <= 1 and total_negative_score > total_positive_score):\n",
    "        sentiment = \"Negative\"\n",
    "    else:\n",
    "        if score['compound'] >= 0.5:\n",
    "            sentiment = 'Positive'\n",
    "        elif score['compound'] > -0.5 and score['compound'] < 0.5:\n",
    "            sentiment = 'Neutral'\n",
    "        elif score['compound'] <= -0.5:\n",
    "            sentiment = 'Negative'\n",
    "    return sentiment\n",
    "\n",
    "# A helper function that removes all the non ASCII characters\n",
    "# from the given string. Retuns a string with only ASCII characters.\n",
    "def strip_non_ascii(string):\n",
    "    ''' Returns the string without non ASCII characters'''\n",
    "    stripped = (c for c in string if 0 < ord(c) < 127)\n",
    "    return ''.join(stripped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a539104",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = []\n",
    "# Open the CSV \n",
    "with open(inputCsvName, 'r') as csvfile:\n",
    "        reader = csv.reader((x.replace('\\0', '') for x in csvfile), delimiter=',')\n",
    "        #reader.next()\n",
    "        for row in reader:\n",
    "            # Assign columns to usable names\n",
    "            post= dict()\n",
    "            post['id'] = row[0]\n",
    "            post['msg_id'] = row[1]\n",
    "            post['parent_id'] = row[2]\n",
    "            post['link_id'] = row[3]\n",
    "            post['Count_read'] = row[4]\n",
    "            post['Forum'] = row[5]\n",
    "            post['Time'] = row[6]\n",
    "            post['Author'] = row[7]\n",
    "            post['Rank'] = row[8]\n",
    "            post['Activity'] = row[9]\n",
    "            post['Merit'] = row[10]\n",
    "            post['Trust'] = row[11]\n",
    "            post['Title'] = row[12]\n",
    "            post['Body'] = row[13]\n",
    "            post['ScamHeader'] = row[14]\n",
    "\n",
    "            post['clean'] = post['Body']\n",
    "\n",
    "            # Remove all non-ascii characters\n",
    "            post['clean'] = strip_non_ascii(post['clean'])\n",
    "\n",
    "            # Normalize case\n",
    "            post['clean'] = post['clean'].lower()\n",
    "\n",
    "            # Remove URLS.\n",
    "            post['clean'] = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', post['clean'])\n",
    "            post['clean'] = re.sub(r'stratum[+]tcp?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', post['clean'])\n",
    "\n",
    "            # Fix classic post lingo\n",
    "            post['clean'] = re.sub(r'\\bthats\\b', 'that is', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bive\\b', 'i have', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bim\\b', 'i am', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bya\\b', 'yeah', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bcant\\b', 'can not', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bwont\\b', 'will not', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bid\\b', 'i would', post['clean'])\n",
    "            post['clean'] = re.sub(r'wtf', 'what the fuck', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bwth\\b', 'what the hell', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\br\\b', 'are', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bu\\b', 'you', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bk\\b', 'ok', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bsux\\b', 'sucks', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bno+\\b', 'no', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bcoo+\\b', 'cool', post['clean'])\n",
    "            \n",
    "            # Fix Cryptocurrency lingo\n",
    "            post['clean'] = re.sub(r'\\bath\\b', 'all time high', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\batl\\b', 'all time low', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bbtfd\\b', 'buy the fucking dip', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bico\\b', 'initial coin offering', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bfomo\\b', 'fear of missing out', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bfud\\b', 'fear uncertainty doubt', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bfucking\\b', 'fuck', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bfudster\\b', 'fear uncertainty doubt spreader', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\broi\\b', 'return on investment', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bmacd\\b', 'moving average convergence divergence', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bpoa\\b', 'proof of authority', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bpow\\b', 'proof of work', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bpos\\b', 'proof of stake', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bdapp\\b', 'decentralized application', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bdao\\b', 'decentralized autonomous organization', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bhodl\\b', 'hold', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bddos\\b', 'distributed denial of service', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bkyc\\b', 'know your customer', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\brekt\\b', 'wrecked', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bbullish\\b', 'bull', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bbearish\\b', 'bear', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bpumping\\b', 'pump', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\basic\\b', 'application specific integrated circuit', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bdyor\\b', 'do your own research', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\berc\\b', 'ethereum request for comments', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bfa\\b', 'fundamental analysis', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bjomo\\b', 'joy of missing out', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bmcap\\b', 'market capitalization', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bmsb\\b', 'money services business', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\boco\\b', 'one cancels the other order', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bpnd\\b', 'pump and dump', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\brsi\\b', 'relative strength index', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\butxo\\b', 'unspent transaction output', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\bvolatility\\b', 'volatile', post['clean'])\n",
    "            post['clean'] = re.sub(r'\\blamborghini\\b', 'lambo', post['clean'])\n",
    "            \n",
    "            # Create textblob object\n",
    "            post['TextBlob'] = TextBlob(post['clean'])\n",
    "\n",
    "            # Correct spelling (WARNING: SLOW)\n",
    "            #post['TextBlob'] = post['TextBlob'].correct()\n",
    "\n",
    "            #print(post['clean'])\n",
    "            #print(vader_polarity(post['clean']))\n",
    "            #print(Analyzer.polarity_scores(post['clean']))\n",
    "            posts.append(post)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e64185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVELOP MODELS\n",
    "def polarity_scores(doc):\n",
    "    return Analyzer.polarity_scores(doc.text)\n",
    "\n",
    "Doc.set_extension('polarity_scores', getter=polarity_scores, force=True)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "negResults = ''\n",
    "posResults = ''\n",
    "neuResults = ''\n",
    "results = ''\n",
    "for post in posts:\n",
    "    doc = nlp(post['clean'])\n",
    "    tokens = [token.text for token in doc if not token.is_stop]\n",
    "    score = doc._.polarity_scores\n",
    "    #results += post['clean']\n",
    "    post['compound'] = score['compound']\n",
    "    post['sentiment'] = vader_polarity(post['clean'])\n",
    "\n",
    "# spaCy count most common words from all posts    \n",
    "#docMain = nlp(results)\n",
    "#words = [token.text for token in docMain if not token.is_stop and token.is_punct != True\n",
    "#         and token.is_space != True]\n",
    "#word_freq = Counter(words)\n",
    "#common_words = word_freq.most_common(30)\n",
    "#print(common_words)\n",
    "\n",
    "posts_sorted = sorted(posts, key=lambda k: k['compound'])\n",
    "\n",
    "# Posts filtered into sentiment categories\n",
    "# Posts that have a compound Negative value\n",
    "negative_posts = [d for d in posts_sorted if d['sentiment'] == 'Negative']\n",
    "for post in negative_posts:\n",
    "    #print(post['id'], post['compound'], post['clean'])\n",
    "    negResults += (post['clean'])\n",
    "\n",
    "# Count most common words in Negative sentiment sentences\n",
    "negDoc = nlp(negResults)\n",
    "negWords = [token.text for token in negDoc if not token.is_stop and token.is_punct != True \n",
    "           and token.is_space != True]\n",
    "word_freq2 = Counter(negWords)\n",
    "common_words2 = word_freq2.most_common(30)\n",
    "#print(common_words2)\n",
    "\n",
    "# Posts that have a compound Positive value\n",
    "positive_posts = [d for d in posts_sorted if d['sentiment'] == 'Positive']\n",
    "for post in positive_posts:\n",
    "#     print(post['Id'], post['polarity'], post['clean'])\n",
    "    posResults += (post['clean'])\n",
    "\n",
    "# Count most common words in Positive sentiment sentences\n",
    "#posDoc = nlp(posResults)\n",
    "#posWords = [token.text for token in posDoc if not token.is_stop and token.is_punct != True\n",
    "#            and token.is_space != True]\n",
    "#word_freq3 = Counter(posWords)\n",
    "#common_words3 = word_freq3.most_common(30)\n",
    "#print(common_words3)\n",
    "\n",
    "# Posts that have a compound Neutral value.\n",
    "neutral_posts = [d for d in posts_sorted if d['sentiment'] == 'Neutral']\n",
    "for post in neutral_posts:\n",
    "#     print(post['Id'], post['polarity'], post['clean'])\n",
    "    neuResults += (post['clean'])\n",
    "\n",
    "#print(negResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f3db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTS\n",
    "# A histogram of the compound scores.\n",
    "x = [d['compound'] for d in posts_sorted]\n",
    "num_bins = 21\n",
    "n, bins, patches = plt.hist(x, num_bins, density=1, facecolor='green', alpha=0.5)\n",
    "plt.xlabel('Compound Scores')\n",
    "plt.ylabel('Probability')\n",
    "plt.title(r'Histogram of Compound Scores')\n",
    "plt.subplots_adjust(left=0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd32cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A pie chart showing the number of posts in each sentiment category\n",
    "pos = len(positive_posts)\n",
    "neu = len(negative_posts)\n",
    "neg = len(neutral_posts)\n",
    "labels = 'Positive', 'Neutral', 'Negative'\n",
    "sizes = [pos, neu, neg]\n",
    "colors = ['yellowgreen', 'gold', 'lightcoral']\n",
    "plt.pie(sizes, labels=labels, colors=colors,\n",
    "        autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac5dc8",
   "metadata": {},
   "source": [
    "## Negative Sentiment WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c97f62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(\n",
    "    background_color='white',\n",
    "    stopwords=stopwords,\n",
    "    width=1600,\n",
    "    height=800,\n",
    "    random_state=21,\n",
    "    colormap='jet',\n",
    "    max_words=50,\n",
    "    max_font_size=200).generate(negResults)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eaf0a6",
   "metadata": {},
   "source": [
    "## Positive Sentiment WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ea86bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(\n",
    "    background_color='white',\n",
    "    stopwords=stopwords,\n",
    "    width=1600,\n",
    "    height=800,\n",
    "    random_state=21,\n",
    "    colormap='jet',\n",
    "    max_words=50,\n",
    "    max_font_size=200).generate(posResults)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539e3626",
   "metadata": {},
   "source": [
    "## Neutral Sentiment WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4461a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(\n",
    "    background_color='white',\n",
    "    stopwords=stopwords,\n",
    "    width=1600,\n",
    "    height=800,\n",
    "    random_state=21,\n",
    "    colormap='jet',\n",
    "    max_words=50,\n",
    "    max_font_size=200).generate(neuResults)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5506884",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(inputCsvName)\n",
    "data['Time'] = pd.to_datetime(data.Time)\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6535fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assessments(str):\n",
    "    blob = TextBlob(str)\n",
    "    return blob.sentiment_assessments.assessments\n",
    "\n",
    "\n",
    "def translated(series):\n",
    "    blob = TextBlob(series)\n",
    "    try:\n",
    "        trans = blob.translate(to='en')\n",
    "        return trans.raw\n",
    "    except textblob.exceptions.NotTranslated:\n",
    "        return ''\n",
    "\n",
    "\n",
    "data['Blob'] = data.Body.apply(TextBlob)\n",
    "data['Polarity'] = data.Blob.apply(attrgetter('sentiment.polarity'))\n",
    "data['Subjectivity'] = data.Blob.apply(attrgetter('sentiment.subjectivity'))\n",
    "data['Assessment'] = data.Body.apply(assessments)\n",
    "# data['Detect_lang'] = data.Blob.apply(\n",
    "#     methodcaller('detect_language'))  # powered by google API\n",
    "# data['Translated'] = data.Body.apply(translated)\n",
    "data['Vader_sentiment'] = data.Body.apply(Analyzer.polarity_scores)\n",
    "data['Vader_compound'] = data['Vader_sentiment'].apply(lambda x: x['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc25843a",
   "metadata": {},
   "source": [
    "## Topic's Post Compared using VADER and Textblob Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b315bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['Body', 'Polarity', 'Subjectivity', 'Vader_compound', 'Vader_sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782b468a",
   "metadata": {},
   "source": [
    "# Is this Cryptocurrency a Scam?\n",
    "Based on reports leading to acknowledgement of a scam possibility, combined with the sentiment analysis results, a plausible determination can be shown. \n",
    "***NOTE***: These results should not be taken as fact, but rather as an aide to determine the possibility that a particular cryptocurrency could be showing signs of a scam attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260c6356",
   "metadata": {},
   "outputs": [],
   "source": [
    "isScamHeader = False\n",
    "count = 0\n",
    "scamCount = 0\n",
    "scam = 'Very Unlikely'\n",
    "ratio = (neg / (pos + neg + neu)) * 100\n",
    "\n",
    "print('Analysis for: ', row[12])\n",
    "for word in common_words2:\n",
    "    count+=1\n",
    "    if word[0] == 'scam':\n",
    "        print('Count of the word Scam: ', word[1])\n",
    "        print('Word use ranked at: ', count)\n",
    "        scamCount = count\n",
    "if row[14] == 'True':\n",
    "    isScamHeader = True\n",
    "if((20 > ratio >= 10) and isScamHeader == False):\n",
    "    scam = 'Unlikely'\n",
    "if((35 > ratio >= 20) and isScamHeader == False):\n",
    "    scam = 'Possible'\n",
    "if((ratio >= 35) and isScamHeader == False and (scamCount == 0)):\n",
    "    scam = 'Possible'\n",
    "if((ratio >= 35) and isScamHeader == False and (0 < scamCount < 10)):\n",
    "    scam = 'Likely'\n",
    "if((20 > ratio >= 10) and isScamHeader == True):\n",
    "    scam = 'Possible'\n",
    "if((33 > ratio >= 20) and isScamHeader == True and (0 < scamCount <= 30)):\n",
    "    scam = 'Likely'\n",
    "if((50 > ratio >= 33) and isScamHeader == True and (0 < scamCount <= 20)):\n",
    "    scam = 'Very Likely'\n",
    "if((ratio >= 50) and isScamHeader == True and (0 < scamCount < 10)):\n",
    "    scam = 'Almost Certain'\n",
    "\n",
    "print('Negative Sentiment in topic: ~', round(ratio), '%')\n",
    "print('Likelihood of scam: \\033[1m' + scam)\n",
    "if(isScamHeader == True):\n",
    "    print('This Cryptocurrency has been reported as a potential scam attempt')\n",
    "print('\\033[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be896ed4",
   "metadata": {},
   "source": [
    "## Explanation of Results\n",
    "The likelihood of the cryptocurrency being a scam is based on several characteristics sorted into categories:\n",
    "- **Very Unlikely**: Shows minimal negativity, not reported as scam, low use of the word *scam*\n",
    "- **Unlikely**: Shows low negativity, not reported as scam, low use of the word *scam*\n",
    "- **Possible**: Shows moderative negativity, moderate use of the word *scam* and may have been reported\n",
    "- **Likely**: Shows above normal negativity, above normal use of the word *scam* and may have been reported\n",
    "- **Very Likely**: Shows above normal negativity, above normal use of the word *scam* and was reported\n",
    "- **Almost Certain**: Shows high negativity, high usage of the word scam and was reported as *scam*\n",
    "\n",
    "The results of this analysis are shown above this explanation and mention the likelihood along with a note if the scam was reported on Bitcointalk.org as showing signs of a scam attempt. These results show the word count of *scam* and its ranking compared to other words in the forum topic. Based on the increasing negativity, the ranking of the usage of *scam* in comparison to other word choices and the potential reporting of a scam, this analysis places emphasis on a building negativity relative to other posts and ranks the likelihood accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8486fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.system('jupyter nbconvert --to html vaderSentAnalysis.ipynb') # Output without custom filename\n",
    "os.system('jupyter nbconvert --to html vaderSentAnalysis.ipynb --output-dir ./data/processed --output ' + csvName + '.html')\n",
    "#os.system('jupyter nbconvert --to markdown vaderSentAnalysis.ipynb --output-dir ./data/processed --output ' + csvName + '.md')\n",
    "#os.system('jupyter nbconvert --to pdf vaderSentAnalysis.ipynb --output-dir ./data/processed --output ' + csvName + '.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c5ed4",
   "metadata": {},
   "source": [
    "An output HTML file named after the input CSV will be converted and placed into the parent directory. This HTML file may be viewed in a web browser to review a complete run of this analysis notebook on that particular cryptocurrency topic. There are two other converts which are currently disabled in the cell above (markdown & PDF export). These can be enabled at your discretion, however the PDF export requires [texlive](https://www.tug.org/texlive/) to be installed and located in PATH. NOTE: There may be CSS issues which do not show the updated outputs, please clear cache and refresh browser to find update custom.CSS file. Also, ensure that these output conversions to HTML, MD and/or PDF happen after the notebook has run and is saved. You may need to run the cell above again to get the output cells to show correctly in the processed files."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
