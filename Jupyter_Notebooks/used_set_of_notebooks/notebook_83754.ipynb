{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "400801c0",
   "metadata": {},
   "source": [
    "# Plotting Decision Boundaries\n",
    "\n",
    "This supplementary notebook explains the logic behind the rather complicated-looking 'plot_decision_boundary' function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c85bb5",
   "metadata": {},
   "source": [
    "### When is it used\n",
    "\n",
    "Plotting decision boundaries is a useful visualization technique for the following tasks:\n",
    "\n",
    "- Classification: when classes are labelled (supervised learning), helps visualize where the classifier predicts the boundaries between the class\n",
    "\n",
    "- Clustering: when no classes are labelled (unsupervised learning), helps visualize where one possible cluster begins and another cluster ends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecd2f98",
   "metadata": {},
   "source": [
    "## Outline\n",
    "First we'll cover how the plot works in steps.\n",
    "\n",
    "Then we'll walk through an example dataset for each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98addf67",
   "metadata": {},
   "source": [
    "### How the plot works\n",
    "\n",
    "1. If the feature ranges are very different, we apply StandardScaler to scale the features. Otherwise, the boundary plot can take a long time to process, or the plot will look skewed.\n",
    "\n",
    "2. If the features are multi-dimensional, we first reduce the dimension of the features to 2D.  This is done using PCA (commonly) or t-SNE (if text-based and therefore sparse).\n",
    "  - What about 3D? Same technique applies, except the matplotlib axes are in 3D.\n",
    "  - What about 1D? Well, visualizing the boundary in 1D involves plotting a vertical line.  It's not as intuitive to visualize than 2D, and won't work for clustering, where there are no y-values (no classes).\n",
    "\n",
    "The rest of the discussion assumes we are doing 2D plots.\n",
    "\n",
    "3. For each feature (X_2d[:, 0] and X_2d[:, 1]), find the max and min values to create a bounding box: x_min, x_max, y_min, y_max\n",
    "  - x_min, x_max are the left and right limits of the plot\n",
    "  - y_min, y_max are the lower and upper limits of the plot\n",
    "\n",
    "\n",
    "4. Now comes the most complex part. Ideally, for boundaries we also want to colour the classes or clusters according to the class id or cluster id. Something like countries on a map. \n",
    "\n",
    "The way to accomplish this is to:\n",
    " a. Create a 2D matrix with the following specifications:\n",
    "  - np.arange(x_min, x_min, 0.05) on the horizontal axis\n",
    "  - np.arange(y_min, y_max, 0.05) on the vertical axis\n",
    "  - Every value in that 2D matrix is 0.05 step apart (units don't matter as much here, because we will be plotting PCA)\n",
    "  - We use [np.meshgrid](https://docs.scipy.org/doc/numpy/reference/generated/numpy.meshgrid.html), to generate the matrix.\n",
    "    - Depending on the data (i.e. X_2d), you may need to adjust the step size larger (if the data is spaced out), or smaller (if the data is close together).\n",
    "    - Note that a smaller step size can run very slowly. \n",
    "\n",
    "  b. For each value in the 2D matrix, run predict() using either the classifier or the clustering algorithm.\n",
    " \n",
    "  c. Plot the predicted values *as an image*, using the predicted values as the colours. \n",
    "    - This is done using [imshow](https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.imshow.html).\n",
    "    - In the \"empty\" spaces between the 0.05 steps, the colours are filled in using interpolation. This creates the solid colours characteristic of a boundary plot.\n",
    "\n",
    "\n",
    "5. Last step of the boundary plot is to scatter plot the values:\n",
    "  - The actual samples.\n",
    "  - For clustering using k-means the centroids are also plotted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0839cbd5",
   "metadata": {},
   "source": [
    "### Example dataset\n",
    "\n",
    "We'll use the COE dataset from the clustering lecture.\n",
    "\n",
    "It can be found at: https://data.gov.sg/dataset/coe-bidding-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7f3d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfc5f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/tmp/coe-bidding-results/coe-results.csv', # fix to your path\n",
    "                 usecols=['quota', 'premium'])\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f01d6a5",
   "metadata": {},
   "source": [
    "1. Scale the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25293820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df.loc[:, ['quota', 'premium']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4f1e2f",
   "metadata": {},
   "source": [
    "2. PCA is skipped because X (and therefore X_scaled) is already in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2a4be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf175db",
   "metadata": {},
   "source": [
    "3. Find the bounding box of our 2D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20eb41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# left and right plot limits\n",
    "# (x_ just denotes horizontal axis on the plot, unrelated to X)\n",
    "\n",
    "x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1\n",
    "\n",
    "print('plot left limit: %.3f, plot right limit: %.3f' %(x_min, x_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba0ae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upper and lower plot limits\n",
    "# (y_ just denotes vertical axis on the plot, unrelated to our labels)\n",
    "\n",
    "y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1\n",
    "\n",
    "print('plot lower limit: %.3f, plot upper limit: %.3f' %(y_min, y_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e419c305",
   "metadata": {},
   "source": [
    "4a. Create a 2D matrix with the following specifications:\n",
    " - np.arange(x_min, x_min, 0.25) on the horizontal axis\n",
    " - np.arange(y_min, y_max, 0.25) on the vertical axis\n",
    " - Every value in that 2D matrix is 0.25 step apart (units don't matter as much here, because we will be plotting PCA)\n",
    " - We use [np.meshgrid](https://docs.scipy.org/doc/numpy/reference/generated/numpy.meshgrid.html), to generate the matrix.\n",
    " - Depending on the data (i.e. X_2d), you may need to adjust the step size larger (if the data is spaced out), or smaller (if the data is close together).\n",
    " - Note that a smaller step size can run very slowly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb314d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = .05\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, step),\n",
    "                     np.arange(y_min, y_max, step))\n",
    "\n",
    "# a. flatten xx using ravel into [xx0, xx1, .., xxN]\n",
    "# b. flatten yy using ravel into [yy0, yy1, .., yyN]\n",
    "# c. column stack into a 2-D array\n",
    "#    (turn xx, yy into column vectors and then stack column-wise)\n",
    "#\n",
    "#   [[xx0, yy0],\n",
    "#    [xx1, yy1],\n",
    "#    ..\n",
    "#    [xxN, yyN]]\n",
    "#\n",
    "\n",
    "mesh = np.column_stack((xx.ravel(), yy.ravel()))\n",
    "mesh\n",
    "\n",
    "# this also works, but easy to forget what it does\n",
    "# mesh = np.c_[xx.ravel(), yy.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b37a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5ff8dd",
   "metadata": {},
   "source": [
    "4b. For each value in the 2D matrix, run predict() using either the classifier or the clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d31c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# first, fit on our dataset (note: not the mesh)\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# next, predict on our mesh\n",
    "clusters = kmeans.predict(mesh)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230bc3c2",
   "metadata": {},
   "source": [
    "4c. Plot the predicted values *as an image*, using the predicted values as the colours. \n",
    " - This is done using [imshow](https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.imshow.html).\n",
    " - In the \"empty\" spaces between the 0.05 steps, the colours are filled in using interpolation. This creates the solid colours characteristic of a boundary plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbe3065",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# convert to 2D array for image\n",
    "clusters_image = clusters.reshape(xx.shape)\n",
    "\n",
    "ax.imshow(clusters_image, interpolation='nearest',\n",
    "          extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "          cmap=plt.cm.Pastel2,\n",
    "          aspect='auto', # image aspect ratio\n",
    "          origin='lower') # origin of image is bottom left\n",
    "\n",
    "ax.set(title='COE dataset K-means boundary plot (k=%d)' % kmeans.n_clusters,\n",
    "       xlim=(x_min, x_max), ylim=(y_min, y_max),\n",
    "       xticks=(), yticks=())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cdeda7",
   "metadata": {},
   "source": [
    "5. Last step of the boundary plot is to scatter plot the values:\n",
    "  - The actual samples.\n",
    "  - For clustering using k-means the centroids are also plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9fae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.imshow(clusters_image, interpolation='nearest',\n",
    "          extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "          cmap=plt.cm.Pastel2,\n",
    "          aspect='auto',\n",
    "          origin='lower')\n",
    "\n",
    "ax.plot(X_scaled[:, 0], X_scaled[:, 1], 'k.', markersize=4)\n",
    "\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "ax.scatter(centroids[:, 0], centroids[:, 1],\n",
    "           marker='x', s=169, linewidths=3,\n",
    "           color='red', zorder=10, label='centroids')\n",
    "\n",
    "ax.set(title='COE dataset K-means boundary plot (k=%d)' % kmeans.n_clusters,\n",
    "       xlim=(x_min, x_max), ylim=(y_min, y_max),\n",
    "       xticks=(), yticks=())\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54c38bd",
   "metadata": {},
   "source": [
    "## Helper Function\n",
    "\n",
    "This is the helper function that combines steps 3, 4, and 5. The variables are named somewhat differently, but the steps are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1945dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_decision_boundaries(ax, title, kmeans_model, data):\n",
    "    \"\"\"Plots the decision boundaries for a fitted k-means model\n",
    "    Args:\n",
    "        ax: subplot axis\n",
    "        title: subplot title\n",
    "        kmeans_model: a fitted sklearn.cluster.KMeans model\n",
    "        data: 2-dimensional input data to cluster and plot\n",
    " \n",
    "    Based on: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html\n",
    "    \"\"\"\n",
    "    # Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "    h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n",
    "    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Obtain labels for each point in mesh using the trained model.\n",
    "    Z = kmeans_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    ax.imshow(Z, interpolation='nearest',\n",
    "              extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "              cmap=plt.cm.Pastel2,\n",
    "              aspect='auto', origin='lower')\n",
    "\n",
    "    ax.plot(data[:, 0], data[:, 1], 'k.', markersize=4)\n",
    "\n",
    "    # Plot the centroids as a red X\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    ax.scatter(centroids[:, 0], centroids[:, 1],\n",
    "               marker='x', s=169, linewidths=3,\n",
    "               color='red', zorder=10, label='centroids')\n",
    "    ax.set(title=title,\n",
    "           xlim=(x_min, x_max), ylim=(y_min, y_max),\n",
    "           xticks=(), yticks=())\n",
    "    ax.legend()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
