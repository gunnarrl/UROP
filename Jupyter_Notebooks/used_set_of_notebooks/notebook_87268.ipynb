{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5da9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "from matplotlib import gridspec\n",
    "from theano import tensor as tt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd6560b",
   "metadata": {},
   "source": [
    "# Chapter 12 - Psychophysical functions\n",
    "  \n",
    "A psychophysical function, showing the a sigmoid or S-shaped relationship between stimulus intensity and choice behavior. Important theoretical measures known as the point of subjective equality (PSE) and just noticeable difference (JND) are highlighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efa07f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def invlogit(x):\n",
    "    return np.log(x/(1-x))\n",
    "\n",
    "x = np.linspace(-3.75, 3.75, 100)\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "x1 = invlogit(.5)\n",
    "x2 = invlogit(.84)\n",
    "\n",
    "plt.plot(x, logit(x), 'k', linewidth=2)\n",
    "plt.plot([x1, x1], [0, .5], color='k', linestyle='--', linewidth=1)\n",
    "plt.plot([-3.75, x1], [.5, .5], color='k', linestyle='--', linewidth=1)\n",
    "plt.plot([x2, x2], [0, .84], color='k', linestyle='--', linewidth=1)\n",
    "plt.plot([-3.75, x2], [.84, .84], color='k', linestyle='--', linewidth=1)\n",
    "\n",
    "plt.scatter(x1, .5, c='k', s=75)\n",
    "plt.text(x1+.5, .5-.02, \"PSE\", horizontalalignment='center', fontsize=20)\n",
    "plt.plot([x1, x2], [.5/2+.06, .5/2+.06], color='k', linewidth=1)\n",
    "plt.text((x1+x2)/2, .5/2, \"JND\", horizontalalignment='center', fontsize=20)\n",
    "\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks((0, .5, .84, 1))\n",
    "ax.set_yticklabels(('0', '0.5', '0.84', '1'), fontsize=12)\n",
    "plt.xlim(-3.75, 3.75)\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('Stimulus Intensity', fontsize=15)\n",
    "plt.ylabel('Respone Probability', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626f64a6",
   "metadata": {},
   "source": [
    "Data for all 8 subjects, showing the proportion of “long” responses as a function of test interval duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23994b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv(\"./data/data_x.txt\", sep='\\t', header=None)\n",
    "n = pd.read_csv(\"./data/data_n.txt\", sep='\\t', header=None)\n",
    "r = pd.read_csv(\"./data/data_r.txt\", sep='\\t', header=None)\n",
    "rprop = pd.read_csv(\"./data/data_rprop.txt\", sep='\\t', header=None)\n",
    "\n",
    "xmean = np.array([318.888, 311.0417, 284.4444, 301.5909, \n",
    "                  296.2000, 305.7692, 294.6429, 280.3571])\n",
    "nstim = np.array([27, 24, 27, 22, 25, 26, 28, 28])\n",
    "nsubjs = 8\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "fig.text(0.5, -0.02, 'Test Interval (ms)', ha='center', fontsize=20)\n",
    "fig.text(-0.02, 0.5, 'Proportion of Long Responses', va='center', rotation='vertical', fontsize=20)\n",
    "gs = gridspec.GridSpec(2, 4)\n",
    "\n",
    "for ip in np.arange(nsubjs):\n",
    "    ax = plt.subplot(gs[ip])\n",
    "    xp = np.array(x.iloc[ip, :])\n",
    "    yp = np.array(rprop.iloc[ip, :])\n",
    "    ax.scatter(xp, yp, marker='s', alpha=.5)\n",
    "    plt.axis([190, 410, -.1, 1.1])\n",
    "    plt.yticks((0, .5, .84, 1))\n",
    "    plt.title('Subject %s'%(nsubjs))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfee3780",
   "metadata": {},
   "source": [
    "The psychometric function here is a logistic function with parameters $\\alpha_i$ and $\\beta_i$:  \n",
    "\n",
    "$$ \\theta_{ij} = \\frac{1}{1+\\text{exp}\\{-[\\alpha_{i}+\\beta_{i}(x_{ij}-\\bar x_{i})]\\}}$$  \n",
    "$$\\text{or}$$   \n",
    "$$ \\text{logit}(\\theta_{ij}) = \\alpha_{i}+\\beta_{i}(x_{ij}-\\bar x_{i})$$\n",
    "\n",
    "## 12.1 Psychophysical functions\n",
    "\n",
    "\n",
    "$$ r_{ij} \\sim \\text{Binomial}(\\theta_{ij},n_{ij})$$\n",
    "$$ \\text{logit}(\\theta_{ij}) = \\alpha_{i}+\\beta_{i}(x_{ij}-\\bar x_{i})$$\n",
    "$$ \\alpha_{i} \\sim \\text{Gaussian}(\\mu_{\\alpha},\\sigma_{\\alpha})$$\n",
    "$$ \\beta_{i} \\sim \\text{Gaussian}(\\mu_{\\beta},\\sigma_{\\beta})$$\n",
    "$$ \\mu_{\\alpha} \\sim \\text{Gaussian}(0,0.001)$$\n",
    "$$ \\mu_{\\beta} \\sim \\text{Gaussian}(0,0.001)$$\n",
    "$$ \\sigma_{\\alpha} \\sim \\text{Uniform}(0,1000)$$\n",
    "$$ \\sigma_{\\beta} \\sim \\text{Uniform}(0,1000)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1e365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xij_tmp = x.values\n",
    "nij_tmp = n.values\n",
    "rij_tmp = r.values\n",
    "tmp,nstim2 = np.shape(xij_tmp)\n",
    "\n",
    "xmeanvect = np.repeat(xmean, nstim2)\n",
    "sbjidx = np.repeat(np.arange(nsubjs), nstim2)\n",
    "\n",
    "# remove nans\n",
    "validmask = np.isnan(xij_tmp.flatten())==False\n",
    "xij2 = xij_tmp.flatten()\n",
    "nij2 = nij_tmp.flatten()\n",
    "rij2 = rij_tmp.flatten()\n",
    "\n",
    "xij = xij2[validmask]\n",
    "nij = nij2[validmask]\n",
    "rij = rij2[validmask]\n",
    "xvect = xmeanvect[validmask]\n",
    "sbjid = sbjidx[validmask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9904d27",
   "metadata": {},
   "source": [
    "Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c936a966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Phi(x):\n",
    "    # probit transform \n",
    "    return 0.5 + 0.5 * pm.math.erf(x/pm.math.sqrt(2))\n",
    "def tlogit(x):\n",
    "    return 1/(1+tt.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40929fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model1:\n",
    "    sigma_a = pm.Uniform('sigma_a', lower=0, upper=1000)\n",
    "    sigma_b = pm.Uniform('sigma_b', lower=0, upper=1000)\n",
    "    mu_a = pm.Normal('mu_a', mu=0, tau=.001)\n",
    "    mu_b = pm.Normal('mu_b', mu=0, tau=.001)\n",
    "    alpha = pm.Normal('alpha', mu=mu_a, sd=sigma_a, shape=nsubjs)\n",
    "    beta = pm.Normal('beta', mu=mu_b, sd=sigma_b, shape=nsubjs)\n",
    "    \n",
    "    linerpredi = alpha[sbjid] + beta[sbjid]*(xij-xvect)\n",
    "    thetaij = pm.Deterministic('thetaij', tlogit(linerpredi))\n",
    "    \n",
    "    rij_ = pm.Binomial('rij', p=thetaij, n=nij, observed=rij)\n",
    "    trace1 = pm.sample(1e3, njobs=2, init='advi+adapt_diag')\n",
    "    \n",
    "pm.traceplot(trace1, varnames=['alpha', 'beta']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7fb31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 8))\n",
    "fig.text(0.5, -0.02, 'Test Interval (ms)', ha='center', fontsize=20)\n",
    "fig.text(-0.02, 0.5, 'Proportion of Long Responses', va='center', rotation='vertical', fontsize=20)\n",
    "gs = gridspec.GridSpec(2, 4)\n",
    "\n",
    "burnin = 0\n",
    "# get MAP estimate\n",
    "tmp = pm.df_summary(trace1[burnin:], varnames=['alpha', 'beta'])\n",
    "alphaMAP = tmp['mean'][np.arange(nsubjs)]\n",
    "betaMAP = tmp['mean'][np.arange(nsubjs)+nsubjs]\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    idx = (np.abs(array-value)).argmin()\n",
    "    return idx\n",
    "\n",
    "for ip in np.arange(nsubjs):\n",
    "    ax = plt.subplot(gs[ip])\n",
    "    xp = np.array(x.iloc[ip, :])\n",
    "    yp = np.array(rprop.iloc[ip, :])\n",
    "    ax.scatter(xp, yp, marker='s', alpha=.5)\n",
    "    \n",
    "    xl = np.linspace(190, 410, 100)\n",
    "    yl = logit(alphaMAP[ip] + betaMAP[ip]*(xl-xmean[ip]))\n",
    "    x1 = xl[find_nearest(yl, .5)]\n",
    "    x2 = xl[find_nearest(yl, .84)]\n",
    "\n",
    "    plt.plot(xl, yl, 'k', linewidth=2)\n",
    "    plt.plot([x1, x1],[-.1, .5], color='k', linestyle='--', linewidth=1)\n",
    "    plt.plot([190, x1],[.5, .5], color='k', linestyle='--', linewidth=1)\n",
    "    plt.plot([x2, x2],[-.1, .84], color='k', linestyle='--', linewidth=1)\n",
    "    plt.plot([190, x2],[.84, .84], color='k', linestyle='--', linewidth=1)\n",
    "\n",
    "    plt.axis([190, 410, -.1, 1.1])\n",
    "    plt.yticks((0, .5, .84, 1))\n",
    "    plt.title('Subject %s'%(ip+1))\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e9dd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior sample\n",
    "from collections import defaultdict\n",
    "alphadist = model1.alpha\n",
    "betadist = model1.beta\n",
    "\n",
    "ppcsamples = 500\n",
    "ppcsize = 100\n",
    "ppc = defaultdict(list)\n",
    "for idx in np.random.randint(burnin, 1e3, ppcsamples):\n",
    "    param = trace1[idx]\n",
    "    ppc['alpha'].append(alphadist.distribution.random(point=param, size=ppcsize))\n",
    "    ppc['beta'].append(betadist.distribution.random(point=param, size=ppcsize))\n",
    "    \n",
    "# np.asarray(ppc['alpha']).shape\n",
    "alphaPPC = np.asarray(ppc['alpha']).mean(axis=1)\n",
    "betaPPC = np.asarray(ppc['beta']).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9709afd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT FOR EXERCISE 12.1.2 \n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "fig.text(0.5, -0.02, 'Test Interval (ms)', ha='center', fontsize=20)\n",
    "fig.text(-0.02, 0.5, 'Proportion of Long Responses', va='center', rotation='vertical', fontsize=20)\n",
    "gs = gridspec.GridSpec(2, 4)\n",
    "\n",
    "ppcsamples=100\n",
    "\n",
    "for ip in np.arange(nsubjs):\n",
    "    ax = plt.subplot(gs[ip])\n",
    "    xp = np.array(x.iloc[ip, :])\n",
    "    yp = np.array(rprop.iloc[ip, :])\n",
    "    ax.scatter(xp, yp, marker='s', alpha=.5)\n",
    "    \n",
    "    xl = np.linspace(190, 410, 100)\n",
    "    yl = logit(alphaMAP[ip] + betaMAP[ip]*(xl-xmean[ip]))\n",
    "\n",
    "    # Posterior sample from the trace\n",
    "    for ips in np.random.randint(burnin, 1e3, ppcsamples):\n",
    "        param = trace1[ips]\n",
    "        yl2 = logit(param['alpha'][ip] + param['beta'][ip]*(xl-xmean[ip]))\n",
    "        plt.plot(xl, yl2, 'k', linewidth=2, alpha=.05)\n",
    "    \n",
    "    plt.plot(xl, yl, 'r', linewidth=2)\n",
    "    \n",
    "    plt.axis([190, 410, -.1, 1.1])\n",
    "    plt.yticks((0, .5, .84, 1))\n",
    "    plt.title('Subject %s'%(ip+1))\n",
    "    \n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49434ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT FOR EXERCISE 12.1.4 \n",
    "from scipy.stats.kde import gaussian_kde # for plotting: to calculate a continuous \n",
    "                                         # approximation of the posterior and prior densities. \n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "fig.text(0.5, -0.02, 'JND (ms)', ha='center', fontsize=20)\n",
    "fig.text(-0.02, 0.5, 'Posterior Density', va='center', rotation='vertical', fontsize=20)\n",
    "gs = gridspec.GridSpec(2, 4)\n",
    "\n",
    "ppcsamples=500\n",
    "\n",
    "for ip in np.arange(nsubjs):\n",
    "    ax = plt.subplot(gs[ip])\n",
    "    \n",
    "    xl = np.linspace(190, 410, 200)\n",
    "    yl = logit(alphaMAP[ip] + betaMAP[ip]*(xl-xmean[ip]))\n",
    "    x1 = xl[find_nearest(yl, .5)]\n",
    "    x2 = xl[find_nearest(yl, .84)]\n",
    "    jnd1 = x2-x1\n",
    "    \n",
    "    # Posterior sample\n",
    "    jndps=[]\n",
    "    for ips in np.random.randint(burnin, 1e3, ppcsamples):\n",
    "        param = trace1[ips]\n",
    "        yl2 = logit(param['alpha'][ip] + param['beta'][ip]*(xl-xmean[ip]))\n",
    "        x1 = xl[find_nearest(yl2, .5)]\n",
    "        x2 = xl[find_nearest(yl2, .84)]\n",
    "        jndps.append(x2-x1)\n",
    "        \n",
    "    pdfpc = gaussian_kde(jndps)\n",
    "    x2=np.linspace(10, 109, 100)\n",
    "    plt.plot(x2, pdfpc(x2), 'k', alpha=.5) \n",
    "    plt.fill_between(x2, pdfpc(x2), 0, alpha=.5, color='k')\n",
    "    plt.axvline(jnd1, color='r', ls='--', lw=2)\n",
    "    \n",
    "    plt.axis([10, 105, -.01, .125])\n",
    "    plt.title('Subject %s'%(ip+1))\n",
    "    \n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b310dc7e",
   "metadata": {},
   "source": [
    "## 12.2 Psychophysical functions under contamination\n",
    "\n",
    "Latent-mixture model approach  \n",
    "$$ r_{ij} \\sim \\text{Binomial}(\\theta_{ij},n_{ij})$$\n",
    "\n",
    "$$   \\theta_{ij} \\sim\n",
    "\\begin{cases}\n",
    "\\frac{1}{1+\\text{exp}\\{-[\\alpha_{i}+\\beta_{i}(x_{ij}-\\bar x_{i})]\\}}  & \\text{if $z_{ij} = 0$} \\\\\n",
    "\\pi_{ij}  & \\text{if $z_{ij} = 1$}\n",
    "\\end{cases}  $$\n",
    "\n",
    "$$ \\Phi^{-1}(\\phi_{i}) \\sim \\text{Gaussian}(\\mu_{\\phi},\\sigma_{\\phi})$$\n",
    "$$ z_{ij} \\sim \\text{Bernoulli}(\\phi_{i})$$\n",
    "$$ \\pi_{ij} \\sim \\text{Uniform}(0,1)$$\n",
    "$$ \\alpha_{i} \\sim \\text{Gaussian}(\\mu_{\\alpha},\\sigma_{\\alpha})$$\n",
    "$$ \\beta_{i} \\sim \\text{Gaussian}(\\mu_{\\beta},\\sigma_{\\beta})$$\n",
    "$$ \\mu_{\\alpha},\\mu_{\\beta},\\mu_{\\phi} \\sim \\text{Gaussian}(0,0.001)$$\n",
    "$$ \\sigma_{\\alpha},\\sigma_{\\beta} \\sim \\text{Uniform}(0,1000)$$\n",
    "$$ \\sigma_{\\phi} \\sim \\text{Uniform}(0,3)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae37897",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model2b:\n",
    "    sigma_a = pm.Uniform('sigma_a', lower=0, upper=1000)\n",
    "    sigma_b = pm.Uniform('sigma_b', lower=0, upper=1000)\n",
    "    mu_a = pm.Normal('mu_a', mu=0, tau=.001)\n",
    "    mu_b = pm.Normal('mu_b', mu=0, tau=.001)\n",
    "    alpha = pm.Normal('alpha', mu=mu_a, sd=sigma_a, shape=nsubjs)\n",
    "    beta = pm.Normal('beta', mu=mu_b, sd=sigma_b, shape=nsubjs)\n",
    "    \n",
    "    linerpredi = alpha[sbjid] + beta[sbjid]*(xij-xvect)\n",
    "    \n",
    "    # latent model for contamination\n",
    "    sigma_p = pm.Uniform('sigma_p', lower=0, upper=3)\n",
    "    mu_p = pm.Normal('mu_p', mu=0, tau=.001)\n",
    "    \n",
    "    probitphi = pm.Normal('probitphi', mu=mu_p, sd=sigma_p, shape=nsubjs, testval=np.ones(nsubjs))\n",
    "    phii = pm.Deterministic('phii', Phi(probitphi))\n",
    "    \n",
    "    pi_ij = pm.Uniform('pi_ij', lower=0, upper=1, shape=xij.shape)\n",
    "    \n",
    "    # reparameterized so we can use ADVI initialization\n",
    "    # zij_ = pm.Uniform('zij_',lower=0, upper=1, shape=xij.shape)\n",
    "    # zij = pm.Deterministic('zij', tt.lt(zij_, phii[sbjid]))\n",
    "    \n",
    "    # rng = tt.shared_randomstreams.RandomStreams()\n",
    "    # zij_ = rng.binomial(n=1, p=phii[sbjid], size=xij.shape)\n",
    "    zij_ = pm.theanof.tt_rng().uniform(size=xij.shape)\n",
    "    zij = pm.Deterministic('zij', tt.lt(zij_, phii[sbjid]))\n",
    "    # zij = pm.Deterministic('zij', tt.eq(zij_, 0))\n",
    "    \n",
    "    thetaij = pm.Deterministic('thetaij', tt.switch(zij, tlogit(linerpredi), pi_ij))\n",
    "    \n",
    "    rij_ = pm.Binomial('rij', p=thetaij, n=nij, observed=rij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9551cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano\n",
    "with model2b:\n",
    "    s = theano.shared(pm.floatX(1))\n",
    "    inference = pm.ADVI(cost_part_grad_scale=s)\n",
    "    # ADVI has nearly converged\n",
    "    inference.fit(n=20000)\n",
    "    # It is time to set `s` to zero\n",
    "    s.set_value(0)\n",
    "    approx = inference.fit(n=10000)\n",
    "    trace2b = approx.sample(3000, include_transformed=True) \n",
    "    elbos1 = -inference.hist\n",
    "    \n",
    "pm.traceplot(trace2b, varnames=['alpha', 'beta']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e21b45a",
   "metadata": {},
   "source": [
    "Now used the ADVI result from above to initialize our original model without reparameterization of $z_{ij}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a3753",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model2b_:\n",
    "    sigma_a = pm.Uniform('sigma_a', lower=0, upper=1000)\n",
    "    sigma_b = pm.Uniform('sigma_b', lower=0, upper=1000)\n",
    "    mu_a = pm.Normal('mu_a', mu=0, tau=.001)\n",
    "    mu_b = pm.Normal('mu_b', mu=0, tau=.001)\n",
    "    alpha = pm.Normal('alpha', mu=mu_a, sd=sigma_a, shape=nsubjs)\n",
    "    beta = pm.Normal('beta', mu=mu_b, sd=sigma_b, shape=nsubjs)\n",
    "    \n",
    "    linerpredi = alpha[sbjid] + beta[sbjid]*(xij-xvect)\n",
    "    \n",
    "    # latent model for contamination\n",
    "    sigma_p = pm.Uniform('sigma_p', lower=0, upper=3)\n",
    "    mu_p = pm.Normal('mu_p', mu=0, tau=.001)\n",
    "    \n",
    "    probitphi = pm.Normal('probitphi', mu=mu_p, sd=sigma_p, shape=nsubjs, testval=np.ones(nsubjs))\n",
    "    phii = pm.Deterministic('phii', Phi(probitphi))\n",
    "    \n",
    "    pi_ij = pm.Uniform('pi_ij', lower=0, upper=1, shape=xij.shape)\n",
    "    \n",
    "    # place holder if zij_ is in the graph in the previous model\n",
    "    #zij_ = pm.Uniform('zij_',lower=0, upper=1, shape=xij.shape)\n",
    "    \n",
    "    zij = pm.Bernoulli('zij', p=phii[sbjid], shape=xij.shape)\n",
    "    thetaij = pm.Deterministic('thetaij', tt.switch(tt.eq(zij, 0), tlogit(linerpredi), pi_ij))\n",
    "    \n",
    "    rij_ = pm.Binomial('rij', p=thetaij, n=nij, observed=rij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ff95f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2b.free_RVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dae7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2b_.free_RVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4419c73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3.step_methods.hmc import quadpotential\n",
    "\n",
    "nchains = 2\n",
    "cov = np.atleast_1d(pm.trace_cov(trace2b, model=model2b))\n",
    "start = list(np.random.choice(trace2b, nchains))\n",
    "for ic in range(nchains):\n",
    "    start[ic]['zij'] = start[ic]['zij'].astype(int)\n",
    "potential = quadpotential.QuadPotentialFull(cov)\n",
    "step = pm.NUTS(potential=potential, model=model2b_, vars=model2b_.free_RVs[:-1])\n",
    "\n",
    "with model2b_:\n",
    "    trace2b_ = pm.sample(1000, tune=1000,\n",
    "                         step=step, start=start, chains=nchains, cores=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5d245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace2b_, varnames=['alpha', 'beta', 'zij']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7a8f33",
   "metadata": {},
   "source": [
    "Psychophysical functions corresponding to expected posterior parameter values, using the model including a contaminant process, for each of the 8 subjects. Square markers representing data are colored to represent how certain they are to be generated by the psychophysical process (lighter) or the contaminant process (darker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877db58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproduce figure 12.6\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "fig.text(0.5, -0.02, 'Test Interval (ms)', ha='center', fontsize=20)\n",
    "fig.text(-0.02, 0.5, 'Proportion of Long Responses', va='center', rotation='vertical', fontsize=20)\n",
    "gs = gridspec.GridSpec(2, 4)\n",
    "\n",
    "trace2 = trace2b_\n",
    "burnin = 0\n",
    "# get MAP estimate\n",
    "tmp = pm.df_summary(trace2[burnin:], varnames=['alpha', 'beta'])\n",
    "tmp2 = pm.df_summary(trace2[burnin:], varnames=['zij'])\n",
    "\n",
    "alphaMAP2 = tmp['mean'][np.arange(nsubjs)]\n",
    "betaMAP2 = tmp['mean'][np.arange(nsubjs)+nsubjs]\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    idx = (np.abs(array-value)).argmin()\n",
    "    return idx\n",
    "\n",
    "for ip in np.arange(nsubjs):\n",
    "    ax = plt.subplot(gs[ip])\n",
    "    xp = np.array(x.iloc[ip, :])\n",
    "    yp = np.array(rprop.iloc[ip, :])\n",
    "    v1 = np.asarray(tmp2['mean'][sbjid==ip])\n",
    "    rgbclr = np.zeros((len(v1),4))\n",
    "    rgbclr[:,2] = 1\n",
    "    # scale the alpha here - lighter color indicates less weight in the model (i.e., outliner)\n",
    "    rgbclr[:,3] = 1 - v1/v1.max()*.75\n",
    "    ax.scatter(xp, yp, marker='s', color=rgbclr)\n",
    "    \n",
    "    xl = np.linspace(190, 410, 100)\n",
    "    yl = logit(alphaMAP2[ip] + betaMAP2[ip]*(xl-xmean[ip]))\n",
    "    yl2= logit(alphaMAP[ip] + betaMAP[ip]*(xl-xmean[ip]))\n",
    "    x1 = xl[find_nearest(yl, .5)]\n",
    "    x2 = xl[find_nearest(yl, .84)]\n",
    "\n",
    "    plt.plot(xl, yl, 'k', linewidth=2)\n",
    "    plt.plot(xl, yl2, 'k', linestyle='--', linewidth=2)\n",
    "    plt.plot([x1, x1], [-.1, .5], color='k', linestyle='--', linewidth=1)\n",
    "    plt.plot([190, x1], [.5, .5], color='k', linestyle='--', linewidth=1)\n",
    "    plt.plot([x2, x2], [-.1, .84], color='k', linestyle='--', linewidth=1)\n",
    "    plt.plot([190, x2], [.84, .84], color='k', linestyle='--', linewidth=1)\n",
    "\n",
    "    plt.axis([190, 410, -.1, 1.1])\n",
    "    plt.yticks((0, .5, .84, 1))\n",
    "    plt.title('Subject %s'%(ip+1))\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210cfecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproduce figure 12.7\n",
    "from scipy.stats.kde import gaussian_kde # for plotting: to calculate a continuous \n",
    "                                         # approximation of the posterior and prior densities. \n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "fig.text(0.5, -0.02, 'JND (ms)', ha='center', fontsize=20)\n",
    "fig.text(-0.02, 0.5, 'Posterior Density', va='center', rotation='vertical', fontsize=20)\n",
    "gs = gridspec.GridSpec(2, 4)\n",
    "\n",
    "ppcsamples=200\n",
    "\n",
    "for ip in np.arange(nsubjs):\n",
    "    ax = plt.subplot(gs[ip])\n",
    "    \n",
    "    xl = np.linspace(190, 410, 200)\n",
    "    \n",
    "    yl = logit(alphaMAP[ip] + betaMAP[ip]*(xl-xmean[ip]))\n",
    "    x1 = xl[find_nearest(yl, .5)]\n",
    "    x2 = xl[find_nearest(yl, .84)]\n",
    "    jnd1 = x2-x1\n",
    "    \n",
    "    yl2 = logit(alphaMAP2[ip] + betaMAP2[ip]*(xl-xmean[ip]))\n",
    "    x12 = xl[find_nearest(yl2, .5)]\n",
    "    x22 = xl[find_nearest(yl2, .84)]\n",
    "    jnd2 = x22-x12\n",
    "    \n",
    "    # Posterior sample\n",
    "    jndps=[]\n",
    "    jndps2=[]\n",
    "    for ips in np.random.randint(0, 1e3, ppcsamples):\n",
    "        param = trace1[ips]\n",
    "        yl2 = logit(param['alpha'][ip] + param['beta'][ip]*(xl-xmean[ip]))\n",
    "        x1 = xl[find_nearest(yl2, .5)]\n",
    "        x2 = xl[find_nearest(yl2, .84)]\n",
    "        jndps.append(x2-x1)\n",
    "        \n",
    "        param = trace2[ips]\n",
    "        yl2 = logit(param['alpha'][ip] + param['beta'][ip]*(xl-xmean[ip]))\n",
    "        x1 = xl[find_nearest(yl2, .5)]\n",
    "        x2 = xl[find_nearest(yl2, .84)]\n",
    "        jndps2.append(x2-x1)\n",
    "        \n",
    "    x2=np.linspace(10, 109, 100)\n",
    "    \n",
    "    pdfpc = gaussian_kde(jndps)\n",
    "    plt.plot(x2, pdfpc(x2), 'k', alpha=.5) \n",
    "    plt.fill_between(x2, pdfpc(x2), 0, alpha=.5, color='k')\n",
    "    plt.axvline(jnd1, color='r', ls='--', lw=2)\n",
    "    \n",
    "    pdfpc2 = gaussian_kde(jndps2)\n",
    "    plt.plot(x2, pdfpc2(x2), 'k', alpha=.25) \n",
    "    plt.fill_between(x2, pdfpc2(x2), 0, alpha=.25, color='k')\n",
    "    plt.axvline(jnd2, color='g', ls='-', lw=2)\n",
    "    \n",
    "    plt.axis([10, 105, -.01, .125])\n",
    "    plt.title('Subject %s'%(ip+1))\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
