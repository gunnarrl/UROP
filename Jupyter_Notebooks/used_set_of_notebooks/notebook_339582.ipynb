{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1158c6e1",
   "metadata": {},
   "source": [
    "<h1 align = 'center'> Neural Networks Demystified </h1>\n",
    "<h2 align = 'center'> Part 6: Training </h2>\n",
    "\n",
    "\n",
    "<h4 align = 'center' > @stephencwelch </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d4185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('9KM9Td6RVgQ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226979b6",
   "metadata": {},
   "source": [
    "So far we’ve built a neural network in python, computed a cost function to let us know how well our network is performing, computed the gradient of our cost function so we can train our network, and last time we numerically validated our gradient computations. After all that work, it’s finally time to train our neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd9f247",
   "metadata": {},
   "source": [
    "Back in part 3, we decided to train our network using gradient descent. While gradient descent is conceptually pretty straight forward, its implementation can actually be quite complex- especially as we increase the size and number of layers in our neural network. If we just march downhill with consistent step sizes, we may get stuck in a local minimum or flat spot, we may move too slowly and never reach our minimum, or we may move to quickly and bounce out of our minimum. And remember, all this must happen in high-dimensional space, making things significantly more complex. Gradient descent is a wonderfully clever method, but provides no guarantees that we will converge to a good solution, that we will converge to a solution in a certain amount of time, or that we will converge to a solution at all.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3411d0aa",
   "metadata": {},
   "source": [
    "The good and bad news here is that this problem is not unique to Neural Networks - there’s an entire field dedicated to finding the best combination of inputs to minimize the output of an objective function: the field of Mathematical Optimization. The bad news is that optimization can be a bit overwhelming; there are many different techniques we could apply to our problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea430d89",
   "metadata": {},
   "source": [
    "Part of what makes the optimization challenging is the broad range of approaches covered - from very rigorous, theoretical methods to hands-on, more heuristics-driven methods. Yann Lecun’s 1998 publication Efficient BackProp presents an excellent review of various optimization techniques as applied to neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f173d7",
   "metadata": {},
   "source": [
    "Here, we’re going to use a more sophisticated variant on gradient descent, the popular Broyden-Fletcher-Goldfarb-Shanno numerical optimization algorithm. The BFGS algorithm overcomes some of the limitations of plain gradient descent by estimating the second derivative, or curvature, of the cost function surface, and using this information to make more informed movements downhill. BFGS will allow us to find solutions more often and more quickly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d58704e",
   "metadata": {},
   "source": [
    "We’ll use the BFGS implementation built into the scipy optimize package, specifically within the minimize function. To use BFGS, the minimize function requires us to pass in an objective function that accepts a vector of parameters, input data, and output data, and returns both the cost and gradients. Our neural network implementation doesn’t quite follow these semantics, so we’ll use a wrapper function to give it this behavior. We’ll also pass in initial parameters, set the jacobian parameter to true since we’re computing the gradient within our neural network class, set the method to BFGS, pass in our input and output data, and some options. Finally, we’ll implement a callback function that allows us to track the cost function value as we train the network. Once the network is trained, we’ll replace the original, random parameters, with the trained parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8254b17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "#Import code from previous videos:\n",
    "from partFive import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad883933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a3a938",
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainer(object):\n",
    "    def __init__(self, N):\n",
    "        #Make Local reference to network:\n",
    "        self.N = N\n",
    "        \n",
    "    def callbackF(self, params):\n",
    "        self.N.setParams(params)\n",
    "        self.J.append(self.N.costFunction(self.X, self.y))   \n",
    "        \n",
    "    def costFunctionWrapper(self, params, X, y):\n",
    "        self.N.setParams(params)\n",
    "        cost = self.N.costFunction(X, y)\n",
    "        grad = self.N.computeGradients(X,y)\n",
    "        \n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        #Make an internal variable for the callback function:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        #Make empty list to store costs:\n",
    "        self.J = []\n",
    "        \n",
    "        params0 = self.N.getParams()\n",
    "\n",
    "        options = {'maxiter': 200, 'disp' : True}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='L-BFGS-B', \\\n",
    "                                 args=(X, y), options=options, callback=self.callbackF)\n",
    "\n",
    "        self.N.setParams(_res.x)\n",
    "        self.optimizationResults = _res\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffe845b",
   "metadata": {},
   "source": [
    "If we plot the cost against the number of iterations through training, we should see a nice, monotonically decreasing function. Further, we see that the number of function evaluations required to find the solution is less than 100, and far less than the 10^27 function evaluation that would have been required to find a solution by brute force, as shown in part 3. Finally, we can evaluate our gradient at our solution and see very small values – which make sense, as our minimum should be quite flat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd62b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = Neural_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1301473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = trainer(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c54e63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008420fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(T.J)\n",
    "grid(1)\n",
    "xlabel('Iterations')\n",
    "ylabel('Cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7282dd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.costFunctionPrime(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def43db2",
   "metadata": {},
   "source": [
    "The more exciting thing here is that we finally have a trained network that can predict your score on a test based on how many hours you sleep and how many hours you study the night before. If we run our training data through our forward method now, we see that our predictions are excellent. We can go one step further and explore the input space for various combinations of hours sleeping and hours studying, and maybe we can find an optimal combination of the two for your next test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e135af",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7716571",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3940acac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test network for various combinations of sleep/study:\n",
    "hoursSleep = linspace(0, 10, 100)\n",
    "hoursStudy = linspace(0, 5, 100)\n",
    "\n",
    "#Normalize data (same way training data way normalized)\n",
    "hoursSleepNorm = hoursSleep/10.\n",
    "hoursStudyNorm = hoursStudy/5.\n",
    "\n",
    "#Create 2-d versions of input for plotting\n",
    "a, b  = meshgrid(hoursSleepNorm, hoursStudyNorm)\n",
    "\n",
    "#Join into a single input matrix:\n",
    "allInputs = np.zeros((a.size, 2))\n",
    "allInputs[:, 0] = a.ravel()\n",
    "allInputs[:, 1] = b.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d62354",
   "metadata": {},
   "outputs": [],
   "source": [
    "allOutputs = NN.forward(allInputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9b1b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contour Plot:\n",
    "yy = np.dot(hoursStudy.reshape(100,1), np.ones((1,100)))\n",
    "xx = np.dot(hoursSleep.reshape(100,1), np.ones((1,100))).T\n",
    "\n",
    "CS = contour(xx,yy,100*allOutputs.reshape(100, 100))\n",
    "clabel(CS, inline=1, fontsize=10)\n",
    "xlabel('Hours Sleep')\n",
    "ylabel('Hours Study')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046ae0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D plot:\n",
    "\n",
    "##Uncomment to plot out-of-notebook (you'll be able to rotate)\n",
    "#%matplotlib qt\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "surf = ax.plot_surface(xx, yy, 100*allOutputs.reshape(100, 100), \\\n",
    "                       cmap=cm.jet)\n",
    "\n",
    "ax.set_xlabel('Hours Sleep')\n",
    "ax.set_ylabel('Hours Study')\n",
    "ax.set_zlabel('Test Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254d21d0",
   "metadata": {},
   "source": [
    "Our results look pretty reasonable, and we see that for our model, sleep actually has a bigger impact on your grade than studying – something I wish I had realized when I was in school. So we’re done, right? \n",
    "\tNope. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655e27f0",
   "metadata": {},
   "source": [
    "We’ve made possibly the most dangerous and tempting error in machine learning – overfitting. Although our network is performing incredibly well (maybe too well) on our training data – that doesn’t mean that our model is a good fit for the real world, and that’s what we’ll work on next time. "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
