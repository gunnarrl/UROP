{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6a0a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt') #uncomment if running on new machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b438777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gensim\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "# Helpful variables\n",
    "EXT_DATA_FOLDER = \"C:\\\\Users\\\\Admin\\\\Projects\\\\thesis\\\\data\\\\\"\n",
    "EXT_DATA_FOLDER2 = \"B:\\\\Datasets\\\\\"\n",
    "\n",
    "ANALYSIS_SAMPLES = os.path.join(EXT_DATA_FOLDER, \"Credibility_Analysis_Samples\\\\September_25\\\\\")\n",
    "dataset_columns = ['Identifier', 'Type', 'Category', 'URL', 'Cat1', 'Cat2', 'Cat3', 'Cat4', 'Cat5',\n",
    " 'Cat6', 'Cat7', 'Score', 'First date_time', 'Tweets', 'Likes', 'Retweets',\n",
    " 'Potential exposure', 'HTML', 'TEXT']\n",
    "criterias = [\"Cat1\", \"Cat2\", \"Cat3\", \"Cat4\", \"Cat5\", \"Cat6\", \"Cat7\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89a06d",
   "metadata": {},
   "source": [
    "### Low Credibility Score Detector\n",
    "\n",
    "1) Create a dataset that separates he articles as having low credibility and not low credibility where low credibility is defined to be articles with a credibility score of less than 3\n",
    "\n",
    "2.a) Apply the ensemble of models to classify an article for each category and then use the resulting score to determine whether an article's credibility is low or not\n",
    "\n",
    "2.b) Apply a new model that has been trained to solely just determine whether an article's credibility is low or not\n",
    "\n",
    "3) ???\n",
    "\n",
    "4) Profit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbad23dc",
   "metadata": {},
   "source": [
    "### Retrieving the manually labelled articles from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdde7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "labelled_articles = pd.read_excel(\"dataset5.xlsx\")\n",
    "labelled_articles = labelled_articles.dropna(subset=['TEXT'])\n",
    "labelled_articles = labelled_articles[pd.to_numeric(labelled_articles['Cat1'], errors='coerce').notnull()]\n",
    "for criteria in criterias:\n",
    "    labelled_articles = labelled_articles.dropna(subset=[criteria])\n",
    "print(labelled_articles.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce59f554",
   "metadata": {},
   "source": [
    "### Making Training and testing sets for 2 classification tasks\n",
    "\n",
    "1) Make a training and testing set for classfying whether an article has low credibility or not\n",
    "\n",
    "2) Make a training and testing set for classifying whether an article satisfies each criteria and then using these outputs do determine whether the article has low crediibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8a94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "crit1_labels = labelled_articles[\"Cat1\"]\n",
    "crit2_labels = labelled_articles[\"Cat2\"]\n",
    "crit3_labels = labelled_articles[\"Cat3\"]\n",
    "crit4_labels = labelled_articles[\"Cat4\"]\n",
    "crit5_labels = labelled_articles[\"Cat5\"]\n",
    "crit6_labels = labelled_articles[\"Cat6\"]\n",
    "crit7_labels = labelled_articles[\"Cat7\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd4905",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = [article.split(\"TITLE: \")[1].replace(\"TEXT: \",\"\").strip(\" \") for article in labelled_articles[\"TEXT\"]]\n",
    "\n",
    "#If score < cred_thresh than article isn't credible\n",
    "cred_thresh = 3\n",
    "labelled_articles['Credible'] = np.where(labelled_articles['Score'] < cred_thresh, 0, 1)\n",
    "low_cred_labels = list(labelled_articles[\"Credible\"])\n",
    "num_runs = 10\n",
    "seeds = [random.randint(1,100000) for i in range(num_runs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e6b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labelled_articles[labelled_articles['Credible'] == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3226111d",
   "metadata": {},
   "source": [
    "### Single Model Approach\n",
    "SVM + TF-IDF with stopwords removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489ac37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_build_train(article_text, labels, seed=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(list(article_text), [int(label) for label in labels], random_state=seed, test_size=0.10)\n",
    "    svm_clf = Pipeline([('vect', CountVectorizer(stop_words='english', analyzer='word', max_features=20000)),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                               alpha=1e-3, random_state=69,\n",
    "                                               max_iter=5, tol=None)),\n",
    "    ])\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "    model = {\"model\": svm_clf,\n",
    "             \"X_train\": X_train, \n",
    "             \"X_test\": X_test,\n",
    "             \"y_train\": y_train,\n",
    "             \"y_test\": y_test}\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16931481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "models = defaultdict(str)\n",
    "seed = random.randint(1,10000)\n",
    "#Train a classifier for all 7 criteria\n",
    "for criteria in criterias:\n",
    "    models[criteria] = svm_build_train(article_text, labelled_articles[criteria], seed)\n",
    "\n",
    "#Replicate the X and y test sets to get the actual credibility score\n",
    "X2_train, X2_test, y2_train, credibility_scores = train_test_split(article_text, list(labelled_articles[\"Score\"]), random_state=seed, test_size=0.10)\n",
    "X2_train, X2_test, y2_train, low_credibility_labels = train_test_split(article_text, list(labelled_articles[\"Credible\"]), random_state=seed, test_size=0.10)\n",
    "\n",
    "#for each criteria, I get the use the model that I trained specifically for it and get it to predict on the articles stored in the X_test set\n",
    "#The results are then stored in 'label_predictions' where it is a 7xN matrix where N is the number of articles in the test sets\n",
    "label_predictions = []\n",
    "for criteria in criterias:\n",
    "    predictions = list(models[criteria]['model'].predict(models[criteria]['X_test']))\n",
    "    label_predictions.append(predictions)\n",
    "\n",
    "#The predictions are then tallied up and compared with the actual credibility score of the article\n",
    "#And the score difference when it correctly and incorrectly identifies\n",
    "predicted_scores = [0 for i in range(len(credibility_scores))]\n",
    "for label in label_predictions:\n",
    "    for article in range(len(label)):\n",
    "        predicted_scores[article] = predicted_scores[article] + label[article]\n",
    "\n",
    "print(\"Predicted Scores vs Actual Scores:\\n{}\\n{}\".format(predicted_scores, credibility_scores))\n",
    "\n",
    "identified = []\n",
    "correct_diff = []\n",
    "incorrect_diff = []\n",
    "for i in range(len(credibility_scores)):\n",
    "    if((credibility_scores[i] < cred_thresh and predicted_scores[i] < cred_thresh) or (credibility_scores[i] >= cred_thresh and predicted_scores[i] >= cred_thresh)) :\n",
    "        identified.append(1)\n",
    "        correct_diff.append(abs(credibility_scores[i] - predicted_scores[i]))\n",
    "    else:\n",
    "        incorrect_diff.append(abs(credibility_scores[i] - predicted_scores[i]))\n",
    "\n",
    "identified_avg = len(identified)/len(credibility_scores)\n",
    "print(\"% Correct: {}\".format(identified_avg))\n",
    "correct_diff_avg = np.array(correct_diff).mean()\n",
    "incorrect_diff_avg = np.array(incorrect_diff).mean()\n",
    "print(\"Average difference when correct: {}\\nAverage difference when incorrect: {}\".format(correct_diff_avg, incorrect_diff_avg))\n",
    "\n",
    "#Calculating the micro averaged f1 score\n",
    "\n",
    "#Converting the predicted scores into a credibility label\n",
    "low_credibility_preds = []\n",
    "for i in range(len(credibility_scores)):\n",
    "        if((credibility_scores[i] < cred_thresh and predicted_scores[i] < cred_thresh)):\n",
    "            low_credibility_preds.append(0)\n",
    "        else:\n",
    "            low_credibility_preds.append(1)\n",
    "        \n",
    "print(\"Micro averaged f1-scores: {}\".format(f1_score(low_credibility_labels, low_credibility_preds, labels=[0,1], average='micro')))\n",
    "print(\"Micro averaged precision scores: {}\".format(precision_score(low_credibility_labels, low_credibility_preds, labels=[0,1], average='micro')))\n",
    "print(\"Micro averaged recall scores: {}\".format(recall_score(low_credibility_labels, low_credibility_preds, labels=[0,1], average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d14ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "svm_predicted = list(svm_clf.predict(X_test))\n",
    "svm_cv_scores = cross_val_score(svm_clf, X_train, y_train, cv=10, scoring='f1_micro')\n",
    "\n",
    "print(\"SVM Average micro f1-score: %0.2f (+/- %0.2f)\" % (svm_cv_scores.mean(), svm_cv_scores.std()))\n",
    "joblib.dump(svm_clf, 'models/svm_clf_tfidf_stop_' + criteria + '.joblib')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b440da",
   "metadata": {},
   "source": [
    "### Multiple runs of test to get idea of average accuracy and average difference between scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc70436",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_identified = []\n",
    "total_correct_diff = []\n",
    "total_incorrect_diff = []\n",
    "total_single_low_cred_preds = []\n",
    "total_low_cred_labels = []\n",
    "\n",
    "for seed in seeds:\n",
    "    models = defaultdict(str)\n",
    "    #Train a classifier for all 7 criteria\n",
    "    for criteria in criterias:\n",
    "        models[criteria] = svm_build_train(article_text, labelled_articles[criteria], seed)\n",
    "\n",
    "    #Replicate the X and y test sets to get the actual credibility score\n",
    "    X2_train, X2_test, y2_train, credibility_scores = train_test_split(article_text, list(labelled_articles[\"Score\"]), random_state=seed, test_size=0.10)\n",
    "    X2_train, X2_test, y2_train, low_credibility_labels = train_test_split(article_text, list(labelled_articles[\"Credible\"]), random_state=seed, test_size=0.10)\n",
    "\n",
    "    #for each criteria, I get the use the model that I trained specifically for it and get it to predict on the articles stored in the X_test set\n",
    "    #The results are then stored in 'label_predictions' where it is a 7xN matrix where N is the number of articles in the test sets\n",
    "    label_predictions = []\n",
    "    for criteria in criterias:\n",
    "        predictions = list(models[criteria]['model'].predict(models[criteria]['X_test']))\n",
    "        label_predictions.append(predictions)\n",
    "\n",
    "    #The predictions are then tallied up and compared with the actual credibility score of the article\n",
    "    #And the score difference when it correctly and incorrectly identifies\n",
    "    predicted_scores = [0 for i in range(len(credibility_scores))]\n",
    "    for label in label_predictions:\n",
    "        for article in range(len(label)):\n",
    "            predicted_scores[article] = predicted_scores[article] + label[article]\n",
    "    identified = []\n",
    "    correct_diff = []\n",
    "    incorrect_diff = []\n",
    "    for i in range(len(credibility_scores)):\n",
    "        if((credibility_scores[i] < cred_thresh and predicted_scores[i] < cred_thresh) or (credibility_scores[i] >= cred_thresh and predicted_scores[i] >= cred_thresh)) :\n",
    "            identified.append(1)\n",
    "            correct_diff.append(abs(credibility_scores[i] - predicted_scores[i]))\n",
    "        else:\n",
    "            incorrect_diff.append(abs(credibility_scores[i] - predicted_scores[i]))\n",
    "            \n",
    "    if(np.array(correct_diff).size == 0):\n",
    "        correct_diff = [0]\n",
    "    \n",
    "    if(np.array(incorrect_diff).size == 0):\n",
    "        incorrect_diff = [0]\n",
    "\n",
    "    identified_avg = len(identified)/len(credibility_scores)\n",
    "    correct_diff_avg = np.array(correct_diff).mean()\n",
    "    incorrect_diff_avg = np.array(incorrect_diff).mean()\n",
    "    \n",
    "    total_identified.append(identified_avg)\n",
    "    total_correct_diff.append(correct_diff_avg)\n",
    "    total_incorrect_diff.append(incorrect_diff_avg)\n",
    "    \n",
    "    #Calculating the micro averaged f1 score\n",
    "\n",
    "    #Converting the predicted scores into a credibility label\n",
    "    low_credibility_preds = []\n",
    "    for i in range(len(credibility_scores)):\n",
    "            if((credibility_scores[i] < cred_thresh and predicted_scores[i] < cred_thresh)):\n",
    "                low_credibility_preds.append(0)\n",
    "            else:\n",
    "                low_credibility_preds.append(1)\n",
    "    total_single_low_cred_preds.extend(low_credibility_preds)\n",
    "    total_low_cred_labels.extend(low_credibility_labels)\n",
    "    \n",
    "    \n",
    "if(np.array(total_incorrect_diff).size == 0):\n",
    "    total_incorrect_diff = [0]\n",
    "    \n",
    "    \n",
    "print(\"\"\"\n",
    "For {} runs:\\n\n",
    "Average correctly identified %: {}\\n\n",
    "Average score difference when correctly classified: {}\\n\n",
    "Average score difference when incorrectly classified: {}\n",
    "\"\"\".format(num_runs, np.array(total_identified).mean(), np.array(total_correct_diff).mean(), np.array(total_incorrect_diff).mean()))\n",
    "\n",
    "\n",
    "print(\"Accuracy: {}\".format(accuracy_score(total_low_cred_labels, total_single_low_cred_preds)))\n",
    "\n",
    "print(\"Micro averaged f1-scores: {}\".format(f1_score(total_low_cred_labels, total_single_low_cred_preds, labels=[0,1], average='micro')))\n",
    "print(\"Macro averaged f1-scores: {}\".format(f1_score(total_low_cred_labels, total_single_low_cred_preds, labels=[0,1], average='macro')))\n",
    "print(\"Binary averaged f1-scores: {}\".format(f1_score(total_low_cred_labels, total_single_low_cred_preds, labels=[0,1], average='binary')))\n",
    "print(\"Micro averaged precision scores: {}\".format(precision_score(total_low_cred_labels, total_single_low_cred_preds, labels=[0,1], average='micro')))\n",
    "print(\"Micro averaged recall scores: {}\".format(recall_score(total_low_cred_labels, total_single_low_cred_preds, labels=[0,1], average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a32d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "IMAGE_FOLDER = \"C:\\\\Users\\\\Admin\\\\OneDrive - Macquarie University\\\\Uni\\\\2018 Semester 2\\\\ENGG411\\\\Honours-Thesis\\\\Thesis\\\\images\"\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, filename,\n",
    "                          normalize=False,\n",
    "                          title='',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(os.path.join(IMAGE_FOLDER, filename), bbox_inches='tight')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "def plot_roc(y_true, y_score, filename):\n",
    "    probs = total_y_scores\n",
    "    preds = total_low_cred_labels\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_true, y_score)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    # method I: plt\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.title('ROC of Binary Classifier')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.savefig(os.path.join(IMAGE_FOLDER, filename), bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdf72d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(total_low_cred_labels, total_single_low_cred_preds)\n",
    "tn, fp, fn, tp = confusion_matrix(total_low_cred_labels, total_single_low_cred_preds).ravel()\n",
    "plot_confusion_matrix(confusion, classes=[\"Low Credibility\", \"High Credibility\"], filename='single-model-performance.png', title=\"Single Model Approach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed37ac",
   "metadata": {},
   "source": [
    "### Ensemble Approach\n",
    "Criteria 1: SVM + TF-IDF (stopwords removed)\n",
    "\n",
    "Criteria 2: SVM + TF-IDF (stopwords removed)\n",
    "\n",
    "Criteria 3: SVM + BoW (stopwords removed)\n",
    "\n",
    "Criteria 4: SVM + BoW (stopwords removed)\n",
    "\n",
    "Criteria 5: SVM + TF-IDF (stopwords removed)\n",
    "\n",
    "Criteria 6: SVM + TF-IDF (stopwords removed)\n",
    "\n",
    "Criteria 7: SVM + BoW (all words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5110a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_build_train(labelled_articles, seed=42):\n",
    "    \n",
    "    c1_clf = Pipeline([('vect', CountVectorizer(stop_words='english', analyzer='word', max_features=20000)),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                               alpha=1e-3, random_state=69,\n",
    "                                               max_iter=5, tol=None)),])\n",
    "    c2_clf = Pipeline([('vect', CountVectorizer(stop_words='english', analyzer='word', max_features=20000)),\n",
    "                        ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                               alpha=1e-3, random_state=69,\n",
    "                                               max_iter=5, tol=None)),])\n",
    "    c3_clf = Pipeline([('vect', CountVectorizer(stop_words='english', analyzer='word', max_features=20000)),\n",
    "                         ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                               alpha=1e-3, random_state=69,\n",
    "                                               max_iter=5, tol=None)),])\n",
    "    c4_clf = Pipeline([('vect', CountVectorizer(stop_words='english', analyzer='word', max_features=20000)),\n",
    "                         ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                               alpha=1e-3, random_state=69,\n",
    "                                               max_iter=5, tol=None)),])\n",
    "    c5_clf = Pipeline([('vect', CountVectorizer(stop_words='english', analyzer='word', max_features=20000)),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                               alpha=1e-3, random_state=69,\n",
    "                                               max_iter=5, tol=None)),])\n",
    "    c6_clf = Pipeline([('vect', CountVectorizer(stop_words='english', analyzer='word', max_features=20000)),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                               alpha=1e-3, random_state=69,\n",
    "                                               max_iter=5, tol=None)),])\n",
    "    c7_clf = Pipeline([('vect', CountVectorizer(max_features=20000)),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                               alpha=1e-3, random_state=69,\n",
    "                                               max_iter=5, tol=None)),])\n",
    "    \n",
    "    article_text = list(labelled_articles[\"TEXT\"])\n",
    "    crit1_labels = [int(label) for label in list(labelled_articles[\"Cat1\"])]\n",
    "    crit2_labels = [int(label) for label in list(labelled_articles[\"Cat2\"])]\n",
    "    crit3_labels = [int(label) for label in list(labelled_articles[\"Cat3\"])]\n",
    "    crit4_labels = [int(label) for label in list(labelled_articles[\"Cat4\"])]\n",
    "    crit5_labels = [int(label) for label in list(labelled_articles[\"Cat5\"])]\n",
    "    crit6_labels = [int(label) for label in list(labelled_articles[\"Cat6\"])]\n",
    "    crit7_labels = [int(label) for label in list(labelled_articles[\"Cat7\"])]\n",
    "    \n",
    "    X1_train, X1_test, y1_train, y1_test = train_test_split(article_text, crit1_labels, random_state=seed, test_size=0.10)\n",
    "    X2_train, X2_test, y2_train, y2_test = train_test_split(article_text, crit2_labels, random_state=seed, test_size=0.10)\n",
    "    X3_train, X3_test, y3_train, y3_test = train_test_split(article_text, crit3_labels, random_state=seed, test_size=0.10)\n",
    "    X4_train, X4_test, y4_train, y4_test = train_test_split(article_text, crit4_labels, random_state=seed, test_size=0.10)\n",
    "    X5_train, X5_test, y5_train, y5_test = train_test_split(article_text, crit5_labels, random_state=seed, test_size=0.10)\n",
    "    X6_train, X6_test, y6_train, y6_test = train_test_split(article_text, crit6_labels, random_state=seed, test_size=0.10)\n",
    "    X7_train, X7_test, y7_train, y7_test = train_test_split(article_text, crit7_labels, random_state=seed, test_size=0.10)\n",
    "    \n",
    "    c1_clf.fit(X1_train,y1_train)\n",
    "    c2_clf.fit(X2_train,y2_train)\n",
    "    c3_clf.fit(X3_train,y3_train)\n",
    "    c4_clf.fit(X4_train,y4_train)\n",
    "    c5_clf.fit(X5_train,y5_train)\n",
    "    c6_clf.fit(X6_train,y6_train)\n",
    "    c7_clf.fit(X7_train,y7_train)\n",
    "    \n",
    "    ensemble = {\"Cat1\": {\"model\": c1_clf, \"X_train\": X1_train, \"X_test\": X1_test, \"y_train\": y1_train, \"y_test\": y1_test},\n",
    "                \"Cat2\": {\"model\": c2_clf, \"X_train\": X2_train, \"X_test\": X2_test, \"y_train\": y2_train, \"y_test\": y2_test},\n",
    "                \"Cat3\": {\"model\": c3_clf, \"X_train\": X3_train, \"X_test\": X3_test, \"y_train\": y3_train, \"y_test\": y3_test},\n",
    "                \"Cat4\": {\"model\": c4_clf, \"X_train\": X4_train, \"X_test\": X4_test, \"y_train\": y4_train, \"y_test\": y4_test},\n",
    "                \"Cat5\": {\"model\": c5_clf, \"X_train\": X5_train, \"X_test\": X5_test, \"y_train\": y5_train, \"y_test\": y5_test},\n",
    "                \"Cat6\": {\"model\": c6_clf, \"X_train\": X6_train, \"X_test\": X6_test, \"y_train\": y6_train, \"y_test\": y6_test},\n",
    "                \"Cat7\": {\"model\": c7_clf, \"X_train\": X7_train, \"X_test\": X7_test, \"y_train\": y7_train, \"y_test\": y7_test}\n",
    "               }\n",
    "\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b8d203",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_identified = []\n",
    "total_correct_diff = []\n",
    "total_incorrect_diff = []\n",
    "total_ensemble_low_cred_preds = []\n",
    "total_low_cred_labels = []\n",
    "\n",
    "num_runs = 10\n",
    "for seed in seeds:\n",
    "    #Train a ensemble classifier for all 7 criteria\n",
    "    ensemble = ensemble_build_train(labelled_articles, seed)\n",
    "\n",
    "    #Replicate the X and y test sets to get the actual credibility score\n",
    "    __train, __test, __train, credibility_scores = train_test_split(article_text, list(labelled_articles[\"Score\"]), random_state=seed, test_size=0.10)\n",
    "    __train, __test, __train, low_credibility_labels = train_test_split(article_text, list(labelled_articles[\"Credible\"]), random_state=seed, test_size=0.10)\n",
    "\n",
    "    #for each criteria, I get the use the model that I trained specifically for it and get it to predict on the articles stored in the X_test set\n",
    "    #The results are then stored in 'label_predictions' where it is a 7xN matrix where N is the number of articles in the test sets\n",
    "    label_predictions = []\n",
    "    for criteria in criterias:\n",
    "        predictions = list(ensemble[criteria]['model'].predict(ensemble[criteria]['X_test']))\n",
    "        label_predictions.append(predictions)\n",
    "\n",
    "    #The predictions are then tallied up and compared with the actual credibility score of the article\n",
    "    #And the score difference when it correctly and incorrectly identifies\n",
    "    predicted_scores = [0 for i in range(len(credibility_scores))]\n",
    "    for label in label_predictions:\n",
    "        for article in range(len(label)):\n",
    "            predicted_scores[article] = predicted_scores[article] + label[article]\n",
    "    identified = []\n",
    "    correct_diff = []\n",
    "    incorrect_diff = []\n",
    "    for i in range(len(credibility_scores)):\n",
    "        if((credibility_scores[i] < cred_thresh and predicted_scores[i] < cred_thresh) or (credibility_scores[i] >= cred_thresh and predicted_scores[i] >= cred_thresh)) :\n",
    "            identified.append(1)\n",
    "            correct_diff.append(abs(credibility_scores[i] - predicted_scores[i]))\n",
    "        else:\n",
    "            incorrect_diff.append(abs(credibility_scores[i] - predicted_scores[i]))\n",
    "\n",
    "    identified_avg = len(identified)/len(credibility_scores)\n",
    "    correct_diff_avg = np.array(correct_diff).mean()\n",
    "    incorrect_diff_avg = np.array(incorrect_diff).mean()\n",
    "    \n",
    "    total_identified.append(identified_avg)\n",
    "    total_correct_diff.append(correct_diff_avg)\n",
    "    total_incorrect_diff.append(incorrect_diff_avg)\n",
    "    \n",
    "    #Calculating the micro averaged f1 score\n",
    "\n",
    "    #Converting the predicted scores into a credibility label\n",
    "    low_credibility_preds = []\n",
    "    for i in range(len(credibility_scores)):\n",
    "            if((credibility_scores[i] < cred_thresh and predicted_scores[i] < cred_thresh)):\n",
    "                low_credibility_preds.append(0)\n",
    "            else:\n",
    "                low_credibility_preds.append(1)\n",
    "    total_ensemble_low_cred_preds.extend(low_credibility_preds)\n",
    "    total_low_cred_labels.extend(low_credibility_labels)\n",
    "    \n",
    "    \n",
    "if(np.isnan(np.array(total_incorrect_diff).mean())):\n",
    "    total_incorrect_df = [0]\n",
    "    \n",
    "    \n",
    "print(\"\"\"\n",
    "For {} runs:\\n\n",
    "Average correctly identified %: {}\\n\n",
    "Average score difference when correctly classified: {}\\n\n",
    "Average score difference when incorrectly classified: {}\n",
    "\"\"\".format(num_runs, np.array(total_identified).mean(), np.array(total_correct_diff).mean(), np.array(total_incorrect_diff).mean()))\n",
    "\n",
    "\n",
    "print(\"Accuracy: {}\".format(accuracy_score(total_low_cred_labels, total_single_low_cred_preds)))\n",
    "print(\"Micro averaged f1-scores: {}\".format(f1_score(total_low_cred_labels, total_ensemble_low_cred_preds, labels=[0,1], average='micro')))\n",
    "print(\"Micro averaged precision scores: {}\".format(precision_score(total_low_cred_labels, total_ensemble_low_cred_preds, labels=[0,1], average='micro')))\n",
    "print(\"Micro averaged recall scores: {}\".format(recall_score(total_low_cred_labels, total_ensemble_low_cred_preds, labels=[0,1], average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbac87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(total_low_cred_labels, total_ensemble_low_cred_preds)\n",
    "tn, fp, fn, tp = confusion_matrix(total_low_cred_labels, total_ensemble_low_cred_preds).ravel()\n",
    "plot_confusion_matrix(confusion, classes=[\"Low Credibility\", \"High Credibility\"], filename='ensemble-performance', title=\"Ensemble Approach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3b4464",
   "metadata": {},
   "source": [
    "### Binary classification for Credible Article Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedec5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_identified = []\n",
    "total_correct_diff = []\n",
    "total_incorrect_diff = []\n",
    "total_bin_low_cred_preds = []\n",
    "total_low_cred_labels = []\n",
    "total_y_scores = []\n",
    "total_y_test = []\n",
    "\n",
    "total_bin_preds = []\n",
    "total_bin_labels = []\n",
    "\n",
    "\n",
    "for seed in seeds:\n",
    "    models = defaultdict(str)\n",
    "    seed = random.randint(1,10000)\n",
    "    #Train a classifier for all 7 criteria\n",
    "    bin_clf = svm_build_train(article_text, labelled_articles['Credible'], seed)\n",
    "\n",
    "    #Replicate the X and y test sets to get the actual credibility score\n",
    "    __train, __test, __train, credibility_scores = train_test_split(article_text, list(labelled_articles[\"Score\"]), random_state=seed, test_size=0.10)\n",
    "    __train, __test, __train, low_credibility_labels = train_test_split(article_text, list(labelled_articles[\"Credible\"]), random_state=seed, test_size=0.10)\n",
    "\n",
    "    #I train a SVM using\n",
    "    predictions = list(bin_clf['model'].predict(bin_clf['X_test']))\n",
    "    \n",
    "    total_bin_preds.extend(predictions)\n",
    "    total_bin_labels.extend(list(bin_clf['y_test']))\n",
    "    total_y_scores.extend((bin_clf['model'].decision_function(bin_clf['X_test'])))\n",
    "    \n",
    "print(\"Accuracy: {}\".format(accuracy_score(total_bin_labels, total_bin_preds)))\n",
    "print(\"Micro averaged f1-scores: {}\".format(f1_score(total_bin_labels, total_bin_preds, labels=[0,1], average='micro')))\n",
    "print(\"Micro averaged precision scores: {}\".format(precision_score(total_bin_labels, total_bin_preds, labels=[0,1], average='micro')))\n",
    "print(\"Micro averaged recall scores: {}\".format(recall_score(total_bin_labels, total_bin_preds, labels=[0,1], average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1806fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(total_bin_preds, total_bin_labels)\n",
    "plot_confusion_matrix(confusion, classes=[\"Low Credibility\", \"High Credibility\"], filename='binary-performance.png', title=\"Binary Approach\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d81bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(total_bin_labels, total_y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fc1471",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(total_bin_labels , total_y_scores, \"binary-roc-curve.png\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
