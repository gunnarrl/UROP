{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "131a7871",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b832b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tarfile, sys\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba15060",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac0eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('float_format', '{:,.2f}'.format)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bc66bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGES = ['en', 'es']\n",
    "language_dict = dict(zip(LANGUAGES, ['English', 'Spanish']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ab511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(t):\n",
    "    m, s = divmod(t, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return '{:02.0f}:{:02.0f}:{:02.0f}'.format(h, m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38dab1d",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d9d92c",
   "metadata": {},
   "source": [
    "### TED 2013 English & Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9f6de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = 'TED'\n",
    "FILE_NAME = 'TED2013'\n",
    "DATA_DIR = Path('..', 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef38bd93",
   "metadata": {},
   "source": [
    "Data source: http://opus.nlpl.eu/TED2013.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21965394",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = DATA_DIR / 'TED' / 'TED2013.en'\n",
    "print(filename.read_text()[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e70b07a",
   "metadata": {},
   "source": [
    "### Tokenize & Clean Sentences\n",
    "\n",
    "Models expect data provided as a single sentence per line. We'll remove punctuation after using `spaCy`'s parser to tokenize the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c709c2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences(path, min_sent_length=3):\n",
    "    stats = pd.DataFrame()\n",
    "    sentences = []\n",
    "    skipped, word_count = 0, 0\n",
    "    \n",
    "    with path.open() as source:\n",
    "        for sentence in source:\n",
    "            # remove short sentences and urls (for TED data)\n",
    "            n_words = len(sentence.split())\n",
    "            if n_words < min_sent_length or sentence.startswith('http:///'):\n",
    "                skipped += 1\n",
    "            else:\n",
    "                word_count += n_words\n",
    "                sentences.append(sentence.strip())\n",
    "                \n",
    "    stats = pd.Series({'Sentences': len(sentences),\n",
    "                       '# Words': word_count,\n",
    "                       'Skipped': skipped})\n",
    "    return sentences, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99910309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences(sents, nlp, path, lang):\n",
    "    exclude = ['PUNCT', 'SYM', 'X']\n",
    "    start = time()\n",
    "    vocab = Counter()\n",
    "    sents = nlp.pipe(sents)\n",
    "    d = []\n",
    "    with open(path / 'ngrams_1.txt'.format(language), 'a') as f:\n",
    "        for i, sent in enumerate(sents):\n",
    "            if i % 20000 == 0 and i > 0:\n",
    "                print(i, end=' ')\n",
    "            d.extend([[i, w.text, w.pos_] for w in sent])\n",
    "            clean_sentence = [w.text.lower() for w in sent if w.pos_ not in exclude]\n",
    "            vocab.update(clean_sentence)\n",
    "            f.write(' '.join(clean_sentence) + '\\n')\n",
    "\n",
    "    vocab = pd.Series(vocab).sort_values(ascending=False).to_frame('count')\n",
    "    with pd.HDFStore(path.parent / 'vocab.h5') as store:\n",
    "        store.put('/'.join([lang, 'vocab']), vocab)\n",
    "        store.put('/'.join([lang, 'tokens']), pd.DataFrame(d, columns=['sent_id', 'token', 'pos']))\n",
    "    duration = time() - start\n",
    "    print('\\n\\tDuration: ', format_time(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33606788",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, stats = {}, pd.DataFrame()\n",
    "\n",
    "for language in LANGUAGES:\n",
    "    source_path =  DATA_DIR / SOURCE / '{}.{}'.format(FILE_NAME, language)\n",
    "    sentences[language], stats[language_dict[language]] = read_sentences(source_path)\n",
    "    \n",
    "    print(language, end=': ')\n",
    "    target_path = Path('vocab', SOURCE, language)\n",
    "    if not target_path.exists():\n",
    "        target_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    clean_sentences(sentences[language], spacy.load(language), target_path, language)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c154f054",
   "metadata": {},
   "source": [
    "### Corpus Summary Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39335b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.applymap(lambda x: '{:,d}'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf6f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.HDFStore(Path('vocab', SOURCE, 'vocab.h5')) as store:\n",
    "    store.put('stats', stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e620d3f8",
   "metadata": {},
   "source": [
    "### Inspect Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e04b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences['en'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9977c9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences['es'][:3] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8125be",
   "metadata": {},
   "source": [
    "### Create n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b6eb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(language, max_length=3):\n",
    "    \"\"\"Using gensim to create ngrams\"\"\"\n",
    "    \n",
    "    path = Path('vocab', SOURCE, language)\n",
    "    n_grams = pd.DataFrame()\n",
    "    start = time()\n",
    "    for n in range(2, max_length + 1):\n",
    "        print(n, end=' ')\n",
    "        \n",
    "        sentences = LineSentence(str(path / 'ngrams_{}.txt'.format(n-1)))\n",
    "        phrases = Phrases(sentences, threshold=100, min_count=10)\n",
    "\n",
    "        s = pd.Series({k.decode('utf-8'): v for k,\n",
    "                       v in phrases.export_phrases(sentences)}) \n",
    "        s = s.to_frame('score').reset_index().rename(\n",
    "            columns={'index': 'phrase'}).assign(length=n)\n",
    "        \n",
    "        n_grams = pd.concat([n_grams, s])\n",
    "        grams = Phraser(phrases)\n",
    "        sentences = grams[sentences]\n",
    "        \n",
    "        with open(path / 'ngrams_{}.txt'.format(n), 'w') as f:\n",
    "            for sentence in sentences:\n",
    "                f.write(' '.join(sentence) + '\\n')\n",
    "                \n",
    "    n_grams = n_grams.sort_values('score', ascending=False)\n",
    "    n_grams.phrase = n_grams.phrase.str.replace('_', ' ')\n",
    "    n_grams['ngram'] = n_grams.phrase.str.replace(' ', '_')\n",
    "    \n",
    "    with pd.HDFStore(Path(path.parent / 'vocab.h5')) as store:\n",
    "        store.put('/'.join([language, 'ngrams']), n_grams)\n",
    "        \n",
    "    print('\\n\\tDuration: ', format_time(time() - start))\n",
    "    print('\\tngrams: {:,d}\\n'.format(len(n_grams)))\n",
    "    print(n_grams.groupby('length').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e87b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in LANGUAGES:\n",
    "    print('\\n', language, end=' ')\n",
    "    create_ngrams(language)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
