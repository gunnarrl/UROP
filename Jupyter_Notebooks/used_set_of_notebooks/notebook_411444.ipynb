{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37fb39d2",
   "metadata": {},
   "source": [
    "# Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "## Домашнее задание №2 - Дерево решений\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d29d6fc",
   "metadata": {},
   "source": [
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** до 27 марта 2018, 06:00   \n",
    "**Штраф за опоздание:** -2 балла после 06:00 27 марта, -4 балла после 06:00 3 апреля, -6 баллов после 06:00 10 апреля\n",
    "\n",
    "При отправлении ДЗ указывайте фамилию в названии файла   \n",
    "\n",
    "\n",
    "Присылать ДЗ необходимо в виде ссылки на свой github репозиторий в slack @alkhamush\n",
    "Необходимо в slack создать таск в приватный чат:   \n",
    "/todo Фамилия Имя *ссылка на гитхаб* @alkhamush   \n",
    "Пример:   \n",
    "/todo Ксения Стройкова https://github.com/stroykova/spheremailru/stroykova_hw2.ipynb @alkhamush   \n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb09bf51",
   "metadata": {},
   "source": [
    "###### Задание 1 (2 баллов)\n",
    "Разберитесь в коде MyDecisionTreeClassifier, который уже частично реализован. В комментариях, где написано \"Что делает этот блок кода?\", ответьте на этот вопрос. Допишите код там, где написано \"Ваш код\". Ваша реализация дерева должна работать по точности не хуже DecisionTreeClassifier из sklearn. Точность проверяется на wine и Speed Dating Data.\n",
    "\n",
    "###### Задание 2 (2 балла)\n",
    "Добиться скорости работы на fit сравнимой со sklearn wine и Speed Dating Data. \n",
    "Для этого используем numpy. \n",
    "\n",
    "###### Задание 3 (2 балла)\n",
    "Продемонстрируйте умение работать с Pipeline на данных Speed Dating Data и DecisionTreeClassifier. Нужно в pipeline произвести все необходимые преобразования данных и в конце обучить модель. Задание реализуйте под пунктом Задание 3 (уже написано ниже)\n",
    "\n",
    "###### Задание 4 (2 балла)\n",
    "Добавьте функционал, который определяет значения feature importance. Выведите 10 главных фичей под пунктом Задание 4 (уже написано ниже) для MyDecisionTreeClassifier и DecisionTreeClassifier так, чтобы сразу были видны выводы и по MyDecisionTreeClassifier, и по DecisionTreeClassifier. Используем данные Speed Dating Data.\n",
    "\n",
    "###### Задание 5 (2 балла)\n",
    "С помощью GridSearchCV или RandomSearchCV подберите наиболее оптимальные параметры для случайного леса (Выберете 2-3 параметра). Используем данные Speed Dating Data. Задание реализуйте под пунктом Задание 5 (уже написано ниже)\n",
    "\n",
    "\n",
    "**Штрафные баллы:**\n",
    "\n",
    "1. Невыполнение PEP8 -1 балл\n",
    "2. Отсутствие фамилии в имени скрипта (скрипт должен называться по аналогии со stroykova_hw2.ipynb) -1 балл\n",
    "3. Все строчки должны быть выполнены. Нужно, чтобы output команды можно было увидеть уже в git'е. В противном случае -1 балл\n",
    "4. При оформлении ДЗ нужно пользоваться данным файлом в качестве шаблона. Не нужно удалять и видоизменять написанный код и текст. В противном случае -1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3318469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c53b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##%%pycodestyle\n",
    "\n",
    "\n",
    "class MyDecisionTreeClassifier:\n",
    "    NON_LEAF_TYPE = 0\n",
    "    LEAF_TYPE = 1\n",
    "\n",
    "    def __init__(self, min_samples_split=2, max_depth=None, sufficient_share=1.0, criterion='gini', max_features=None):\n",
    "        self.tree = dict()\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.sufficient_share = sufficient_share\n",
    "        self.num_class = -1\n",
    "        self.feature_importances_ = None\n",
    "        if criterion == 'gini':\n",
    "            self.G_function = self.__gini\n",
    "        elif criterion == 'entropy':\n",
    "            self.G_function = self.__entropy\n",
    "        elif criterion == 'misclass':\n",
    "            self.G_function = self.__misclass\n",
    "        else:\n",
    "            print('invalid criterion name')\n",
    "            raise\n",
    "\n",
    "        if max_features == 'sqrt':\n",
    "            self.get_feature_ids = self.__get_feature_ids_sqrt\n",
    "        elif max_features == 'log2':\n",
    "            self.get_feature_ids = self.__get_feature_ids_log2\n",
    "        elif max_features == None:\n",
    "            self.get_feature_ids = self.__get_feature_ids_N\n",
    "        else:\n",
    "            print('invalid max_features name')\n",
    "            raise\n",
    "\n",
    "    def __gini(self, l_c, l_s, r_c, r_s):\n",
    "        l_s = l_s.astype('float')\n",
    "        r_s = r_s.astype('float')\n",
    "        l_w = l_s / (l_s + r_s)\n",
    "        return l_w * (1 - ((l_c / l_s)**2).sum(axis=1, keepdims=True)) + (1 - l_w) * (1 - ((r_c / r_s)**2).sum(axis=1, keepdims=True))\n",
    "    \n",
    "    def __entropy(self, l_c, l_s, r_c, r_s):\n",
    "        p_l = l_c / l_s\n",
    "        p_r = r_c / r_s\n",
    "        h_l = -(p_l * np.nan_to_num(np.log2(p_l))).sum(axis=1, keepdims=True)\n",
    "        h_r = -(p_r * np.nan_to_num(np.log2(p_r))).sum(axis=1, keepdims=True)\n",
    "        l_w = l_s / (l_s + r_s)\n",
    "        return l_w * h_l + (1 - l_w) * h_r\n",
    "\n",
    "    def __misclass(self, l_c, l_s, r_c, r_s):\n",
    "        l_w = l_s / (l_s + r_s)\n",
    "        \n",
    "        return l_w * (1 - np.max((l_c / l_s), axis=1, keepdims=True)) + (1 - l_w) * (1 - np.max((r_c / r_s), axis=1, keepdims=True))\n",
    "\n",
    "    def __get_feature_ids_sqrt(self, n_feature):\n",
    "        feature_ids = np.arange(n_feature)\n",
    "        np.random.shuffle(feature_ids)\n",
    "        return feature_ids[:int(np.sqrt(n_feature))]\n",
    "        \n",
    "    def __get_feature_ids_log2(self, n_feature):\n",
    "        feature_ids = np.arange(n_feature)\n",
    "        np.random.shuffle(feature_ids)\n",
    "        return feature_ids[:int(np.log2(n_feature))]\n",
    "\n",
    "    def __get_feature_ids_N(self, n_feature):\n",
    "        return np.arange(n_feature)\n",
    "    \n",
    "    def __sort_samples(self, x, y):\n",
    "        sorted_idx = x.argsort()\n",
    "        return x[sorted_idx], y[sorted_idx]\n",
    "\n",
    "    def __div_samples(self, x, y, feature_id, threshold):\n",
    "        left_mask = x[:, feature_id] > threshold\n",
    "        right_mask = ~left_mask\n",
    "        return x[left_mask], x[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "    def __find_threshold(self, x, y):\n",
    "        # Что делает этот блок кода?\n",
    "        # Сортируем элементы по x, для того, чтобы лучше находить разбиение по непрерывному признаку\n",
    "        # class_number - количество уникальных классов\n",
    "        sorted_x, sorted_y = self.__sort_samples(x, y)\n",
    "        #class_number = np.unique(y).shape[0]\n",
    "        class_number = self.num_class\n",
    "        # Slack feature!\n",
    "        cut_size = int(self.min_samples_split / 2) - 1\n",
    "        # Что делает этот блок кода?\n",
    "        # Вырезаем из y середину по минимальному количеству элементов разбиения(min_samples_split\\cut_size)\n",
    "        splitted_sorted_y = sorted_y[cut_size:-cut_size] if cut_size != 0 else sorted_y \n",
    "        # Получаем индексы, по которым происходит переход между классами, т.е находим индексы порогов \n",
    "        r_border_ids = np.where(splitted_sorted_y[:-1] != splitted_sorted_y[1:])[0] + (cut_size + 1)\n",
    "        \n",
    "        if len(r_border_ids) == 0:\n",
    "            return float('+inf'), None\n",
    "        \n",
    "        # Что делает этот блок кода?\n",
    "        # eq_el_count - расстояния между порогами\n",
    "        eq_el_count = r_border_ids - np.append([cut_size], r_border_ids[:-1])\n",
    "        # Кодируем категориальные переменные(например для класса 2 будет 0, 0, 1) для каждой левой части разбиений\n",
    "        one_hot_code = np.zeros((r_border_ids.shape[0], class_number))\n",
    "        one_hot_code[np.arange(r_border_ids.shape[0]), sorted_y[r_border_ids - 1]] = 1\n",
    "        # Считаем количество классов в каждой левой части разбиений\n",
    "        class_increments = one_hot_code * eq_el_count.reshape(-1, 1)\n",
    "        # И добавляем оставшуюся, отрезанную крайнюю левую часть\n",
    "        class_increments[0] = class_increments[0] + np.bincount(sorted_y[:cut_size], minlength=class_number)\n",
    "        \n",
    "        # Что делает этот блок кода?\n",
    "        # Считаем сумму количеств классов для каждой левой части\n",
    "        l_class_count = np.cumsum(class_increments, axis=0)\n",
    "        # Сумма количеств классов для правой части. \n",
    "        # По сути вычитаем из суммы всех классов суммы количеств левой частей разбиений\n",
    "        r_class_count = np.bincount(sorted_y, minlength=class_number) - l_class_count\n",
    "        # Количество элементов левой и правой частей разбиений\n",
    "        l_sizes = r_border_ids.reshape(l_class_count.shape[0], 1)\n",
    "        r_sizes = sorted_y.shape[0] - l_sizes\n",
    "\n",
    "        # Что делает этот блок кода?\n",
    "        # Вычисление неопределенности по заданному критерию для каждой из частей разбиений\n",
    "        gs = self.G_function(l_class_count, l_sizes, r_class_count, r_sizes)\n",
    "        # Ищем индекс наименьшую неопределенность\n",
    "        idx = np.argmin(gs)\n",
    "    \n",
    "        # Что делает этот блок кода?\n",
    "        # Получает индекс наилучшего разбиения\n",
    "        left_el_id = l_sizes[idx][0]\n",
    "        # Возвращает из функции значение наименьшей неопределенности и медиану разбиения к этой неопределенности\n",
    "        return gs[idx], (sorted_x[left_el_id-1] + sorted_x[left_el_id]) / 2.0\n",
    "    \n",
    "    def __create_leaf(self, y):\n",
    "        classes_count = np.bincount(y, minlength=self.num_class).astype('float')\n",
    "        probs = classes_count / np.sum(classes_count)\n",
    "        return (self.LEAF_TYPE, classes_count.argmax(), probs)\n",
    "    \n",
    "    def __fit_node(self, x, y, node_id, depth, pred_f=-1):\n",
    "        # Ваш код\n",
    "        # Необходимо использовать следующее:\n",
    "        # self.LEAF_TYPE\n",
    "        # self.NON_LEAF_TYPE\n",
    "        \n",
    "        # self.tree\n",
    "        # self.max_depth\n",
    "        # self.sufficient_share\n",
    "        # self.min_samples_split\n",
    "\n",
    "        # self.get_feature_ids\n",
    "        # self.__find_threshold\n",
    "        # self.__div_samples\n",
    "        # self.__fit_node\n",
    "        if depth == self.max_depth or np.unique(y).size == 1 or y.size <= self.min_samples_split:\n",
    "            print(\"Stopping criteria 1\")\n",
    "            print(\"Y: \", y)\n",
    "            self.tree[node_id] = self.__create_leaf(y)\n",
    "            return\n",
    "        feature_ids = self.get_feature_ids(x.shape[1])\n",
    "        thresholds = np.empty(x.shape[1])\n",
    "        gss = np.empty(x.shape[1])\n",
    "        for feature_id in feature_ids:\n",
    "            gss[feature_id], thresholds[feature_id] = self.__find_threshold(x[:, feature_id], y)\n",
    "            #print(\"GSS: \", gss[feature_id])\n",
    "            #print(\"THREASHOLD: \", thresholds[feature_id])\n",
    "        best_feature_id = gss.argmin()\n",
    "        best_threashold = thresholds[best_feature_id]\n",
    "        print(\"BEST GS: \", gss.min())\n",
    "        print(\"BEST threashold: \", best_threashold)\n",
    "        print(\"BEST FEATURE_ID: \", best_feature_id)\n",
    "        l_x, r_x, l_y, r_y = self.__div_samples(x, y, best_feature_id, best_threashold)\n",
    "\n",
    "        print(\"SPLIT\")\n",
    "        print(\"l_y: \", l_y)\n",
    "        print(\"r_y: \", r_y)\n",
    "        if l_y.size == 0 or r_y.size == 0:\n",
    "            #if np.unique(l_y).size != 1 or np.unique(r_y).size != 1:\n",
    "                #print(\"Fucking bullshit X_L\", l_x)\n",
    "                #print(\"FUCKING BULLSHIT X_R\", r_x)\n",
    "            print(\"Stopping criteria 2\")\n",
    "            self.tree[node_id] = self.__create_leaf(y)\n",
    "            return\n",
    "        print(self.info_gain(y, l_y, r_y))\n",
    "        self.feature_importances_[best_feature_id] = self.info_gain(y, l_y, r_y).max()\n",
    "        self.tree[node_id] = (self.NON_LEAF_TYPE, best_feature_id, best_threashold)\n",
    "        # Left subtree\n",
    "        self.__fit_node(l_x, l_y, node_id * 2 + 1, depth + 1)\n",
    "        # Right subtree\n",
    "        self.__fit_node(r_x, r_y, node_id * 2 + 2, depth + 1)\n",
    "    \n",
    "    def info_gain(self, y, l_y, r_y):\n",
    "        y_c = np.bincount(y, minlength=self.num_class)\n",
    "        l_y_c = np.bincount(l_y, minlength=self.num_class)\n",
    "        r_y_c = np.bincount(r_y, minlength=self.num_class)\n",
    "        p = l_y.size / (l_y.size + r_y.size)\n",
    "        return (1 - ((y_c / y.size)**2).sum()) - p * (1 - ((l_y_c / l_y.size)**2).sum()) - (1 - p) * (1 - ((r_y_c / r_y.size)**2).sum())\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.num_class = np.unique(y).size\n",
    "        self.feature_importances_ = np.empty(x.shape[1])\n",
    "        self.__fit_node(x, y, 0, 0) \n",
    "\n",
    "    def __predict_class(self, x, node_id):\n",
    "        node = self.tree[node_id]\n",
    "        if node[0] == self.__class__.NON_LEAF_TYPE:\n",
    "            _, feature_id, threshold = node\n",
    "            if x[feature_id] > threshold:\n",
    "                return self.__predict_class(x, 2 * node_id + 1)\n",
    "            else:\n",
    "                return self.__predict_class(x, 2 * node_id + 2)\n",
    "        else:\n",
    "            return node[1]\n",
    "\n",
    "    def __predict_probs(self, x, node_id):\n",
    "        node = self.tree[node_id]\n",
    "        if node[0] == self.__class__.NON_LEAF_TYPE:\n",
    "            _, feature_id, threshold = node\n",
    "            if x[feature_id] > threshold:\n",
    "                return self.__predict_probs(x, 2 * node_id + 1)\n",
    "            else:\n",
    "                return self.__predict_probs(x, 2 * node_id + 2)\n",
    "        else:\n",
    "            return node[2]\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.array([self.__predict_class(x, 0) for x in X])\n",
    "    \n",
    "    def predict_probs(self, X):\n",
    "        return np.array([self.__predict_probs(x, 0) for x in X])\n",
    "\n",
    "    def fit_predict(self, x_train, y_train, predicted_x):\n",
    "        self.fit(x_train, y_train)\n",
    "        return self.predict(predicted_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e09963",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = MyDecisionTreeClassifier(min_samples_split=2)\n",
    "clf = DecisionTreeClassifier(min_samples_split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4f1e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, random_state=42, test_size=0.2, stratify=wine.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb888683",
   "metadata": {},
   "source": [
    "## Проверка скорости работы на wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36008d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89e1dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time my_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33eac65",
   "metadata": {},
   "source": [
    "## Проверка качества работы на wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff4e6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1_score(y_pred=clf.predict(X_test), y_true=y_test, average='macro')\n",
    "print(\"Sklearn tree f1 on train: \", f1_score(y_pred=clf.predict(X_train), y_true=y_train, average='macro'))\n",
    "print(\"Sklearn tree f1 on test: \", f1_score(y_pred=clf.predict(X_test), y_true=y_test, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b125f1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"My tree f1 on train: \", f1_score(y_pred=my_clf.predict(X_train), y_true=y_train, average='macro'))\n",
    "print(\"My tree f1 on test: \", f1_score(y_pred=my_clf.predict(X_test), y_true=y_test, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1891d8a5",
   "metadata": {},
   "source": [
    "## Подготовка данных Speed Dating Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279675e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# тут делаете то же самое, что и на семинаре https://github.com/stroykova/spheremailru/blob/master/2018-02/lecture_04_trees/pract-speed-dating-trees-proc.ipynb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class Transform_Speed_Dating(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.df = X # It's pandas dataset\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        df = self.df\n",
    "        df = df.iloc[:, :97]\n",
    "        df.iid.nunique()\n",
    "\n",
    "        df = df.drop(['id'], axis=1)\n",
    "        df = df.drop(['idg'], axis=1)\n",
    "        df.drop_duplicates(subset=['iid']).gender.value_counts()\n",
    "        df.drop_duplicates(subset=['iid']).condtn.value_counts()\n",
    "        df = df.drop(['condtn'], axis=1)\n",
    "\n",
    "        df = df.drop(['round'], axis=1)\n",
    "        df = df.drop(['position', 'positin1'], axis=1)\n",
    "        df = df.drop(['order'], axis=1)\n",
    "        df = df.drop(['partner'], axis=1)\n",
    "        df = df.drop(['age_o', 'race_o', 'pf_o_att',\n",
    "                      'pf_o_sin', 'pf_o_int',\n",
    "                      'pf_o_fun', 'pf_o_amb', 'pf_o_sha',\n",
    "                      'dec_o', 'attr_o', 'sinc_o', 'intel_o', 'fun_o',\n",
    "                      'amb_o', 'shar_o', 'like_o', 'prob_o', 'met_o'],\n",
    "                     axis=1)\n",
    "        df.drop_duplicates(subset=['iid']).age\n",
    "        df.drop_duplicates('iid').age.isnull().sum()\n",
    "        df = df.dropna(subset=['age'])\n",
    "\n",
    "        df.field_cd.isnull().sum()\n",
    "\n",
    "        df.loc[:, 'field_cd'] = df.loc[:, 'field_cd'].fillna(19)\n",
    "\n",
    "        df = df.drop(['field'], axis=1)\n",
    "        df = df.drop(['undergra'], axis=1)\n",
    "\n",
    "        df.loc[:, 'mn_sat'] = df.loc[:, 'mn_sat'].str.replace(',', '').astype(np.float)\n",
    "\n",
    "        df.drop_duplicates('iid').mn_sat\n",
    "        df.drop_duplicates('iid').mn_sat.isnull().sum()\n",
    "        df.loc[:, 'mn_sat'] = df.mn_sat.fillna(-999)\n",
    "\n",
    "        df.loc[:, 'tuition'] = df.loc[:, 'tuition'].str.replace(',', '').astype(np.float)\n",
    "        df.drop_duplicates('iid').tuition\n",
    "        df.drop_duplicates('iid').tuition.isnull().sum()\n",
    "        df.loc[:, 'tuition'] = df.tuition.fillna(-999)\n",
    "\n",
    "        df.drop_duplicates('iid').race.value_counts()\n",
    "        df.drop_duplicates('iid').age.isnull().sum()\n",
    "        df.drop_duplicates('iid').race\n",
    "        df.drop_duplicates('iid').imprace.isnull().sum()\n",
    "        df.drop_duplicates('iid').imprelig.isnull().sum()\n",
    "\n",
    "        df = df.dropna(subset=['imprelig', 'imprace'])\n",
    "\n",
    "        df = df.drop(['from', 'zipcode'], axis=1)\n",
    "        df.loc[:, 'income'] = df.loc[:, 'income'].str.replace(',', '').astype(np.float)\n",
    "\n",
    "        df.drop_duplicates('iid').loc[:, 'income']\n",
    "\n",
    "        df.loc[:, 'income'] = df.loc[:, 'income'].fillna(-999)\n",
    "\n",
    "        df = df.dropna(subset=['date'])\n",
    "\n",
    "        df.loc[:, 'career_c'] = df.loc[:, 'career_c'].fillna(18)\n",
    "\n",
    "        df = df.drop(['career'], axis=1)\n",
    "        df.loc[:, ['sports', 'tvsports', 'exercise', 'dining', 'museums', 'art', 'hiking', 'gaming',\n",
    "                   'clubbing', 'reading', 'tv', 'theater', 'movies', 'concerts', 'music', 'shopping', 'yoga']\n",
    "        ].isnull().sum()\n",
    "\n",
    "        df = df.drop(['sports', 'tvsports', 'exercise', 'dining', 'museums', 'art', 'hiking', 'gaming',\n",
    "                      'clubbing', 'reading', 'tv', 'theater', 'movies', 'concerts', 'music', 'shopping', 'yoga'], axis=1)\n",
    "        df.drop_duplicates('iid').exphappy.isnull().sum()\n",
    "        df.drop_duplicates('iid').expnum.isnull().sum()\n",
    "        df = df.drop(['expnum'], axis=1)\n",
    "\n",
    "        feat = ['iid', 'wave', 'attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']\n",
    "        temp = df.drop_duplicates(subset=['iid', 'wave']).loc[:, feat]\n",
    "        temp.loc[:, 'totalsum'] = temp.iloc[:, 2:].sum(axis=1)\n",
    "        idx = ((temp.wave < 6) | (temp.wave > 9)) & (temp.totalsum < 99)\n",
    "        temp.loc[idx,]\n",
    "        idx = ((temp.wave >= 6) & (temp.wave <= 9))\n",
    "\n",
    "        temp.loc[idx,]\n",
    "        df.loc[:, 'temp_totalsum'] = df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']].sum(axis=1)\n",
    "        df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']] = \\\n",
    "            (df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']].T / df.loc[:,\n",
    "                                                                                              'temp_totalsum'].T).T * 100\n",
    "        feat = ['iid', 'wave', 'attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']\n",
    "\n",
    "        temp = df.drop_duplicates(subset=['iid', 'wave']).loc[:, feat]\n",
    "        temp.loc[:, 'totalsum'] = temp.iloc[:, 2:].sum(axis=1)\n",
    "\n",
    "        idx = ((temp.wave < 6) | (temp.wave > 9)) & (temp.totalsum < 90) & (temp.totalsum != 0)\n",
    "        temp.loc[idx,]\n",
    "\n",
    "        idx = ((temp.wave >= 6) & (temp.wave <= 9))\n",
    "        temp.loc[idx,]\n",
    "\n",
    "        df.loc[:, 'temp_totalsum'] = df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']].sum(axis=1)\n",
    "        df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']] = \\\n",
    "            (df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']].T / df.loc[:,\n",
    "                                                                                              'temp_totalsum'].T).T * 100\n",
    "        df = df.drop(['temp_totalsum'], axis=1)\n",
    "        for i in [4, 5]:\n",
    "            feat = ['attr{}_1'.format(i), 'sinc{}_1'.format(i),\n",
    "                    'intel{}_1'.format(i), 'fun{}_1'.format(i),\n",
    "                    'amb{}_1'.format(i), 'shar{}_1'.format(i)]\n",
    "\n",
    "            if i != 4:\n",
    "                feat.remove('shar{}_1'.format(i))\n",
    "\n",
    "            df = df.drop(feat, axis=1)\n",
    "\n",
    "        df = df.drop(['wave'], axis=1)\n",
    "        df_male = df.query('gender == 1').drop_duplicates(subset=['iid', 'pid']) \\\n",
    "            .drop(['gender'], axis=1) \\\n",
    "            .dropna()\n",
    "        df_female = df.query('gender == 0').drop_duplicates(subset=['iid']) \\\n",
    "            .drop(['gender', 'match', 'int_corr', 'samerace'], axis=1) \\\n",
    "            .dropna()\n",
    "        df_female.columns = df_female.columns + '_f'\n",
    "\n",
    "        df_pair = df_male.join(df_female.set_index('iid_f'), on='pid', how='inner')\n",
    "        df_pair = df_pair.drop(['iid', 'pid'], axis=1)\n",
    "        return df_pair\n",
    "\n",
    "\n",
    "# Data prep with pipeline\n",
    "speed_dating_data = pd.read_csv('speed-dating-experiment/sdd.csv', encoding='cp1251')\n",
    "pipeline = Pipeline([('transform_speed_dating', Transform_Speed_Dating())])\n",
    "data = pipeline.fit_transform(speed_dating_data)\n",
    "\n",
    "X = data.iloc[:, 1:].values\n",
    "y = data.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ce61ca",
   "metadata": {},
   "source": [
    "## Проверка скорости работы на Speed Dating Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e534592",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13137175",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time my_clf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64443590",
   "metadata": {},
   "source": [
    "## Проверка качества работы на Speed Dating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4bb8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(y_pred=clf.predict(X_test), y_true=y_test, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abc2106",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(y_pred=my_clf.predict(X_test), y_true=y_test, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbd8686",
   "metadata": {},
   "source": [
    "## Задание 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec929ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier pipeline\n",
    "pipeline = Pipeline([('clf', MyDecisionTreeClassifier())])\n",
    "X = pipeline.fit(X_train, y_train)\n",
    "print(f1_score(y_pred=pipeline.predict(X_test), y_true=y_test, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b01ad42",
   "metadata": {},
   "source": [
    "## Задание 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0dfbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "important_feature_ids_sklearn = clf.feature_importances_.argsort()[:10]\n",
    "print(\"Sklearn Feature importance\")\n",
    "print(data.columns[1:][important_feature_ids_sklearn].values.reshape(-1, 1))\n",
    "important_feature_ids_my_clf = my_clf.feature_importances_.argsort()[:10]\n",
    "print(\"My clf Feature importance\")\n",
    "print(data.columns[1:][important_feature_ids_my_clf].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a5770e",
   "metadata": {},
   "source": [
    "## Задание 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf83b8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search\n",
    "clf = RandomForestClassifier()\n",
    "params = {\"max_depth\": list(range(1, 10)) + [None], \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "grid = GridSearchCV(clf, params, cv=2, n_jobs=-1, verbose=True)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best estimator\")\n",
    "print(grid.best_estimator_)\n",
    "print(\"Best score\")\n",
    "print(grid.best_score_)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
