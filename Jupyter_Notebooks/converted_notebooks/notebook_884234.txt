import matplotlib.pyplot as plt
import numpy as np

train_data = np.genfromtxt('ex1data1.txt', delimiter=',')
print("Dimentions of training samples: [%d" % train_data.shape[0], ", %d]" % train_data.shape[1])

X = train_data[:, 0]
y = train_data[:, 1]
m = y.shape[0]
print("Size of training samples: %d" % m)

X = X.reshape(m, 1)
y = y.reshape(m, 1)

# Scatter plot
plt.style.use('seaborn-whitegrid')
plt.xlabel("Population of city in 10,000s")
plt.ylabel("Profit in $10,000s")
plt.title("Scatter plot of training data")
plt.plot(X, y, 'rx');

### Method to compute Cost Function
def compute_cost(X, y, theta):
    diff = X.dot(theta) - y
    ssq = np.sum(diff**2)
    return ssq / (2 * m)

# Add one additional column of all ones to X representing x0
x0 = np.ones((m, 1))
original_X = X
X = np.append(x0, X, axis=1)
print(X.shape)
print(y.shape)

# Initialize thetas to zeros
# Iteration count to 1500
# Alpha to 0.01
theta = np.zeros((2, 1))
iterations = 1500
alpha = 0.01

# Tests on compute_cost
J = compute_cost(X, y, theta)
print("Expected cost value 32.07; Calculated cost value %f" % J)

theta_temp = np.array([[-1], [2]])
J = compute_cost(X, y, theta_temp)
print("Expected cost value 54.24; Calculated cost value %f" % J)

# Method to calculate gradient descent
def gradient_descent(X, y, theta, alpha, num_iters):
    J_history = np.zeros(num_iters)
    for step in range(num_iters):
        if step >= 1:
            theta_0 = theta[0, 0] - alpha * np.sum(X.dot(theta) - y) / m
            theta_1 = theta[1, 0] - alpha * np.sum((X.dot(theta) - y) * X) / m
            theta[0, 0] = theta_0
            theta[1, 0] = theta_1
        J_curr = compute_cost(X, y, theta)
        J_history[step - 1] = J_curr
        print("Current Cost Value %f" % J_curr)
    return theta
        
# Calculate theta
theta = gradient_descent(X, y, theta, alpha, iterations)
print("\nObtained theta values: ")
print(theta)

print(X.shape)
### Plot the resultant linear regression 
plt.style.use('seaborn-whitegrid')
plt.xlabel("Population of city in 10,000s")
plt.ylabel("Profit in $10,000s")
plt.title("Scatter plot of training data")
plt.plot(original_X, y, 'rx', X[:, 1], X.dot(theta), 'b-', lw=2)

### Predictions
test1 = np.array([1, 3.5]).reshape(1, 2)
test2 = np.array([1, 7]).reshape(1, 2)
predict1 = test1.dot(theta)
print('For population = 35,000, we predict a profit of %f\n' % (predict1 * 10000))
predict2 = test2.dot(theta)
print('For population = 70,000, we predict a profit of %f\n' % (predict2 * 10000))

import matplotlib.pyplot as plt
import numpy as np

data_multi = np.genfromtxt('ex1data2.txt', delimiter=',')
print("Dimentions of training samples: [%d" % data_multi.shape[0], ", %d]" % data_multi.shape[1])

X_multi = data_multi[:, 0:2]
print(X_multi.shape)
y = data_multi[:, 2]
multi = y.shape[0]
print("Size of training samples: %d" % multi)

y = y.reshape(multi, 1)

print('First 10 examples from the dataset: \n')
print(X_multi[0:10,:])
print(y[0:10,:])

def featureNormalize(X):
    X_norm = X
    mu = np.mean(X, axis = 0)
    sigma = np.std(X, axis = 0)
    # Tile rows together for matrix operations
    mu_matrix = np.tile(mu, (X.shape[0], 1))
    sigma_matrix = np.tile(sigma, (X.shape[0], 1))
    X_norm = (X_norm - mu_matrix) / sigma_matrix
    mu = mu.reshape(1, X.shape[1])
    sigma = sigma.reshape(1, X.shape[1])
    return X_norm, mu, sigma

X_norm, mu, sigma = featureNormalize(X_multi)
print(mu)
print(sigma)

### Add x0 column into training dataset
x0 = np.ones((multi, 1))
original_X_multi = X_norm
X_norm = np.append(x0, X_norm, axis=1)

### Method to calculate cost value for multi-variant
def compute_cost_multi(X, y, theta):
    T = X.dot(theta) - y
    return np.transpose(T).dot(T) / (2 * multi)
    
### Method to calculate gradient descent for multi-variant
def gradient_descent_multi(X, y, theta, alpha, num_iters):
    J_history = np.zeros(num_iters)
    for step in range(num_iters):
        delta = (1 / multi) * np.sum(X * np.tile((X.dot(theta) - y), (1, X.shape[1])))
        theta = np.transpose(np.transpose(theta) - alpha * delta)
        J_curr = compute_cost_multi(X, y, theta)
        J_history[step - 1] = J_curr
        print("Current Cost Value %f" % J_curr)
    return theta

theta = np.zeros((X_norm.shape[1], 1))
iterations = 1500
alpha = 0.01

theta = gradient_descent_multi(X_norm, y, theta, alpha, iterations)
print(theta)
