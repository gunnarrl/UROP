{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d483c75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard python helper libraries.\n",
    "import os, sys, re, json, time, wget, csv, string, time, random\n",
    "import itertools, collections\n",
    "\n",
    "# NumPy\n",
    "import numpy as np\n",
    "\n",
    "# NLTK for NLP utils\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GaussianNoise, Dropout, Dense, Embedding, MaxPool1D, GlobalMaxPool1D, Conv1D, LSTM, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "from pymagnitude import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6febde2c",
   "metadata": {},
   "source": [
    "# Categorical Classifier for Phenotypes using a Convolutional Neural Network\n",
    "\n",
    "In this notebook, we will be building a model for classifying a sentence from an EHR note for the presence of phenotypes. The model will be trained using a convoultional neural network (CNN). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0614214",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976ed4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ehr_notes = []\n",
    "with open('data/ehr_samples.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:     \n",
    "        if int(row['SpecialtyID']) in [39, 6, 16, 37, 11, 12, 29, 26, 7, 21, 19, 10, 2, 18]:\n",
    "            continue\n",
    "        else:\n",
    "            ehr_notes.append([row['Specialty'], row['Note']])\n",
    "\n",
    "print(ehr_notes[0])\n",
    "print(ehr_notes[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc985a3",
   "metadata": {},
   "source": [
    "## Natural Langauge Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7335aa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ehr_notes_clean = []\n",
    "for record in ehr_notes:\n",
    "    sentences = []\n",
    "    \n",
    "    sent_text = nltk.sent_tokenize(record[1])\n",
    "    for sent in sent_text:\n",
    "        tokens = word_tokenize(sent)\n",
    "\n",
    "        # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "\n",
    "        # remove punctuation from each word\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "\n",
    "        # filter out stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "\n",
    "#         # stem words\n",
    "#         porter = PorterStemmer()\n",
    "#         tokens = [porter.stem(word) for word in tokens]\n",
    "\n",
    "        # remove blanks\n",
    "        tokens = [w for w in tokens if w != '']\n",
    "        \n",
    "        clean_sentence = '<s> <s> ' + ' '.join(tokens) + ' </s> </s>'\n",
    "        sentences.append(clean_sentence)\n",
    "        \n",
    "    ehr_notes_clean.append([record[0], ' '.join(sentences)])\n",
    "\n",
    "random.Random(4).shuffle(ehr_notes_clean)\n",
    "\n",
    "print(ehr_notes_clean[0])\n",
    "print(ehr_notes_clean[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301f1088",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "\n",
    "The first step is to load in our word embedding, which is trained on text from Wikipedia, Pubmed, and Pubmed Central. We load our word embedding through a tool called Magnitude. A full exploration of the word embedding and Magnitude can be found in the Introduction notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a44486",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = list(np.array(ehr_notes_clean)[:,1])\n",
    "lengths = [len(note.split(' ')) for note in notes]\n",
    "MAX_WORDS = max(lengths) # The maximum number of words the sequence model will consider\n",
    "med_vectors = Magnitude(\"data/wikipedia-pubmed-and-PMC-w2v.magnitude\", pad_to_length=MAX_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3191eda7",
   "metadata": {},
   "source": [
    "## Defining our Training and Test Data\n",
    "\n",
    "Before we can start building our neural networks, we first have to define our datasets. Specifically, we have to break up our EHR data so that we have records that we can train on and records that are exclusively used to test on. Maintaining a separate set for testing ensures we avoid overfitting our data.\n",
    "\n",
    "We will use some built-in functions provided by Magnitude that helps encode our classes/categories. We then partition our data into our train and test sets. For each set we have both data and labels. Initially, we will be making these partitions small to make iterating through model development much quicker. However, once the models are developed, we will expand our datasets to include all of our data. To ensure we defined our data correctly, we can print a few lines from the two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc75ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ehr_notes_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c38bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_intent, intent_to_int, int_to_intent = MagnitudeUtils.class_encoding()\n",
    "\n",
    "x_train = [ehr_note[1].split(' ') for ehr_note in ehr_notes_clean[:2200]]\n",
    "x_test = [ehr_note[1].split(' ') for ehr_note in ehr_notes_clean[2201:]]\n",
    "\n",
    "y_train = [add_intent(ehr_note[0]) for ehr_note in ehr_notes_clean[:2200]]\n",
    "y_test = [add_intent(ehr_note[0]) for ehr_note in ehr_notes_clean[2201:]]\n",
    "\n",
    "y_train = list(np.array(y_train).reshape(len(y_train)))\n",
    "y_test = list(np.array(y_test).reshape(len(y_test)))\n",
    "\n",
    "num_training = len(x_train)\n",
    "num_test = len(x_test)\n",
    "num_outputs = int(max(max(y_train), max(y_test))) + 1\n",
    "\n",
    "print(int_to_intent(0))\n",
    "\n",
    "print(\"First line of train/test data:\")\n",
    "print(\"\\t\", x_train[0])\n",
    "print(\"\\t\", y_train[0], int_to_intent(y_train[0]))\n",
    "print(\"\\t\", x_test[0])\n",
    "print(\"\\t\", y_test[0], int_to_intent(y_test[0]))\n",
    "print(\"Second line of train/test data:\")\n",
    "print(\"\\t\", x_train[1])\n",
    "print(\"\\t\", y_train[1], int_to_intent(y_train[1]))\n",
    "print(\"\\t\", x_test[1])\n",
    "print(\"\\t\", y_test[1], int_to_intent(y_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba69e053",
   "metadata": {},
   "source": [
    "## Defining Custom Callback Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3625b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    \n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2df0fae",
   "metadata": {},
   "source": [
    "## Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d695d240",
   "metadata": {},
   "outputs": [],
   "source": [
    "STD_DEV = 0.01 # Deviation of noise for Gaussian Noise applied to the embeddings\n",
    "DROPOUT_RATIO = .5 # The ratio to dropout\n",
    "BATCH_SIZE = 100 # The number of examples per train/validation step\n",
    "EPOCHS = 100 # The number of times to repeat through all of the training data\n",
    "LEARNING_RATE = .01 # The learning rate for the optimizer\n",
    "HIDDEN_UNITS = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GaussianNoise(STD_DEV, input_shape=(MAX_WORDS, med_vectors.dim)))\n",
    "model.add(Bidirectional(LSTM(HIDDEN_UNITS, activation='tanh'), merge_mode='concat'))\n",
    "model.add(Dropout(DROPOUT_RATIO))\n",
    "model.add(Dense(num_outputs, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', f1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5926f9d",
   "metadata": {},
   "source": [
    "## Training Batches and Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a97ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batches = MagnitudeUtils.batchify(x_train, y_train, BATCH_SIZE) # Split the training data into batches\n",
    "num_batches_per_epoch_train = int(np.ceil(num_training/float(BATCH_SIZE)))\n",
    "test_batches = MagnitudeUtils.batchify(x_test, y_test, BATCH_SIZE)  # Split the test data into batches\n",
    "num_batches_per_epoch_test = int(np.ceil(num_test/float(BATCH_SIZE)))\n",
    "\n",
    "\n",
    "# Generates batches of the transformed training data\n",
    "train_batch_generator = (\n",
    "  (\n",
    "    med_vectors.query(x_train_batch), # Magnitude will handle converting the 2D array of text into the 3D word vector representations!\n",
    "    MagnitudeUtils.to_categorical(y_train_batch, num_outputs) # Magnitude will handle converting the class labels into one-hot encodings!\n",
    "  ) for x_train_batch, y_train_batch in training_batches\n",
    ")\n",
    "\n",
    "# Generates batches of the transformed test data\n",
    "test_batch_generator = (\n",
    "  (\n",
    "    med_vectors.query(x_test_batch), # Magnitude will handle converting the 2D array of text into the 3D word vector representations!\n",
    "    MagnitudeUtils.to_categorical(y_test_batch, num_outputs) # Magnitude will handle converting the class labels into one-hot encodings!\n",
    "  ) for x_test_batch, y_test_batch in test_batches\n",
    ")\n",
    "\n",
    "# Start training\n",
    "model.fit_generator(\n",
    "    generator = train_batch_generator,\n",
    "    steps_per_epoch = num_batches_per_epoch_train,\n",
    "    validation_data = test_batch_generator,\n",
    "    validation_steps = num_batches_per_epoch_test,\n",
    "    epochs = EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a38fc32",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d581329",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results after training for %d epochs:\" % (EPOCHS,))\n",
    "\n",
    "train_metrics = model.evaluate_generator(\n",
    "    generator = train_batch_generator,\n",
    "    steps = num_batches_per_epoch_train,\n",
    ")\n",
    "\n",
    "print(\"loss: %.4f - categorical_accuracy: %.4f - f1: %.4f\" % tuple(train_metrics))\n",
    "\n",
    "val_metrics = model.evaluate_generator(\n",
    "    generator = test_batch_generator,\n",
    "    steps = num_batches_per_epoch_test,\n",
    ")\n",
    "\n",
    "print(\"val_loss: %.4f - val_categorical_accuracy: %.4f - f1: %.4f\" % tuple(val_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c7fe36",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2294a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(model.history.history['acc'])\n",
    "plt.plot(model.history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(model.history.history['loss'])\n",
    "plt.plot(model.history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1ef969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_pred = np.argmax(model.predict(med_vectors.query(x_test)), axis=1)\n",
    "class_labels = [int_to_intent(y) for y in set(y_test)] \n",
    "report = classification_report(y_test, y_pred, target_names=class_labels)\n",
    "print(class_labels)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14303629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = [int_to_intent(y_class) for y_class in unique_labels(y_test, y_pred)]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_figheight(15)\n",
    "    fig.set_figwidth(15)\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_labels\n",
    "                      , title='Confusion Matrix for Phenotype LSTM Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4154e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i, y in enumerate(y_pred):\n",
    "    if y == y_test[i]:\n",
    "        correct += 1\n",
    "\n",
    "correct / len(y_pred)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
