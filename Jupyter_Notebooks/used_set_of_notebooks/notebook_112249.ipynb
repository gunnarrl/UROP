{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4e5971d",
   "metadata": {},
   "source": [
    "# Neural Machine Translation with Attention Model\n",
    "\n",
    "\n",
    "In this notebook, we  will build a Neural Machine Translation (NMT) model to translate human readable dates (\"28th of September, 2009\") into machine readable dates (\"2009-09-28\"). We will do this with an attention model, which is used to learn sophisticated mappings from one sequence to another.\n",
    "\n",
    "Let's load all the packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eae90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from nmt_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571c157b",
   "metadata": {},
   "source": [
    "## Translating human readable dates into machine readable dates\n",
    "\n",
    "The attention model can be used to translate from one language to another but that task will require large training dataset and a lot of time training on GPUs. Hence in order to explore the functionality of this model, we will instead use it to translate dates written in a variety of formats to machine readable dates.\n",
    "For example, \n",
    "- $\"the 29th of August 1958\"$ to $\"1958-08-29\"$\n",
    "\n",
    "- $\"03/30/1968\"$ to $\"1968-03-30\"$\n",
    "\n",
    "- $\"24 JUNE 1987\"$ to  $\"1987-06-24\"$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434d9813",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "For trainig, we will use a dataset of 10000 dates in different formats and their equivalent machine readable dates.\n",
    "\n",
    "Let's print some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2a9ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e161922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets load some sample dates and their corresponding machine readable dates\n",
    "\n",
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7150e9f",
   "metadata": {},
   "source": [
    "The data we have loaded has,\n",
    "- `dataset`: a list of tuples of (human readable date, machine readable date)\n",
    "- `human_vocab`: a python dictionary mapping all characters used in the human readable dates to an integer-valued index \n",
    "- `machine_vocab`: a python dictionary mapping all characters used in machine readable dates to an integer-valued index.  \n",
    "- `inv_machine_vocab`: the inverse dictionary of `machine_vocab`, mapping from indices back to characters. \n",
    "\n",
    "Let's preprocess the data and map the raw text data into the index values. We will also use Tx=30 (which we assume is the maximum length of the human readable date; if we get a longer input, we would have to truncate it) and Ty=10 (since \"YYYY-MM-DD\" is 10 characters long). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2b2d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = 30\n",
    "Ty = 10\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433b5d21",
   "metadata": {},
   "source": [
    "We now have:\n",
    "- `X`: where each character in the human readable dates is replaced by an index mapped to the character via `human_vocab`. Each date is further padded to $T_x = 30$ values with a special character (< pad >). `X.shape = (m, Tx)`\n",
    "- `Y`: where each character in machine readable dates is replaced by the index it is mapped to in `machine_vocab`. `Y.shape = (m, Ty)`. \n",
    "- `Xoh`: one-hot version of `X`,\n",
    "- `Yoh`: one-hot version of `Y`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f5f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 9\n",
    "print(\"Source date:\", dataset[index][0])\n",
    "print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X[index])\n",
    "print(\"Target after preprocessing (indices):\", Y[index])\n",
    "print()\n",
    "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
    "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f0983b",
   "metadata": {},
   "source": [
    "## Neural machine translation with attention\n",
    "\n",
    "Let's take a look at out Model.\n",
    "\n",
    "The attention mechanism tells a Neural Machine Translation model where it should pay attention to at any step. If we are to translate a sentence from one language to another, especially if its a long sentence or a paragraph, we don't read the whole paragraph and translate it, but we pay attention to a few words in a paragraph like negation or conjugation and then translate the paragraph in pieces. That's what attention model does.\n",
    "\n",
    "\n",
    "###  Attention mechanism\n",
    "\n",
    "Here is a figure of how the model works. The diagram on the left shows the attention model. The diagram on the right shows what one \"Attention\" step does to calculate the attention variables $\\alpha^{\\langle t, t' \\rangle}$, which are used to compute the context variable $context^{\\langle t \\rangle}$ for each timestep in the output ($t=1, \\ldots, T_y$). \n",
    "\n",
    "<table>\n",
    "<td> \n",
    "<img src=\"images/attn_model.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "<td> \n",
    "<img src=\"images/attn_mechanism.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "</table>\n",
    "<caption><center> **Source**: Coursera.org, Deep Learning Specialization.</center></caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539618b0",
   "metadata": {},
   "source": [
    "\n",
    "Here are some properties of the model that you may notice: \n",
    "\n",
    "- The LSTM at the bottom is a Bi-directional LSTM and comes before the attention model and the one at the top a one-directional LSTM. The pre-attention LSTM goes through $T_x$ time steps while the post attantion LSTM goes though $T_y$ time steps. We use two LSTMs here because $T_x$ is not necessarily equal to $T_y$.\n",
    "\n",
    "- In the figure on the right, where we implement the one step of the attention model, for a given time step ($t$), we take as input all the Hidden states of the Bi_LSTM and the previous hiddent state ($t-1$) of the post-attention LSTM. The output gives us a weighted sum of all the hidden states. The weights here decide how each character in the sentence ( or a date in this case) influences the character at time step $t$. The model will learn this influence and will try to replicate it when we test it on a new data. \n",
    "\n",
    "\n",
    "**1) `one_step_attention()`**: At step $t$, given all the hidden states of the Bi-LSTM ($[a^{<1>},a^{<2>}, ..., a^{<T_x>}]$) and the previous hidden state of the second LSTM ($s^{<t-1>}$), `one_step_attention()` will compute the attention weights ($[\\alpha^{<t,1>},\\alpha^{<t,2>}, ..., \\alpha^{<t,T_x>}]$) and output the context vector (see Figure  1 (right) for details):\n",
    "$$context^{<t>} = \\sum_{t' = 0}^{T_x} \\alpha^{<t,t'>}a^{<t'>}$$\n",
    "\n",
    "  \n",
    "**2) `model()`**: Implements the entire model. It first runs the input through a Bi-LSTM to get back $[a^{<1>},a^{<2>}, ..., a^{<T_x>}]$. Then, it calls `one_step_attention()` $T_y$ times (`for` loop). At each iteration of this loop, it gives the computed context vector $c^{<t>}$ to the second LSTM, and runs the output of the LSTM through a dense layer with softmax activation to generate a prediction $\\hat{y}^{<t>}$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b676c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor1 = Dense(10, activation = \"tanh\")\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') \n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de838c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) to\n",
    "    #concatenate it with all hidden states \"a\" \n",
    "    s_prev = repeator(s_prev)\n",
    "   # print(s_prev.shape)\n",
    "    # Use concatenator to concatenate a and s_prev on the last axis \n",
    "    concat = concatenator([a , s_prev])\n",
    "    #print(concat.shape)\n",
    "    \n",
    "    # Use densor1 to propagate concat through a small fully-connected \n",
    "    # neural network to compute the \"intermediate energies\" variable e. \n",
    "    e = densor1(concat)\n",
    "    #print(e.shape)\n",
    "    # Use densor2 to propagate e through a small fully-connected neural \n",
    "    # network to compute the \"energies\" variable energies. \n",
    "    energies = densor2(e)\n",
    "    #print(energies.shape)\n",
    "    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\" \n",
    "    alphas = activator(energies)\n",
    "    #print(alphas.shape)\n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector\n",
    "    # to be given to the next (post-attention) LSTM-cell \n",
    "    context = dotor([alphas,a])\n",
    "    #print(context.shape)\n",
    "    #print(context.shape)\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efcdffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global layers that will share weights to be used in 'model()'\n",
    "\n",
    "n_a = 32\n",
    "n_s = 64\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "output_layer = Dense(len(machine_vocab), activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f57aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the inputs of the model with a shape (Tx,)\n",
    "    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
    "    X = Input(shape=(Tx, human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    \n",
    "    # Step 1: Define  pre-attention Bi-LSTM.. \n",
    "    a = Bidirectional( LSTM(n_a, return_sequences=True ))(X)\n",
    "    #print(a.shape)\n",
    "    \n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t\n",
    "        context = one_step_attention(a , s)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        #  initial_state = [hidden state, cell state] \n",
    "        s, _, c = post_activation_LSTM_cell(context, initial_state = [s,c])\n",
    "        \n",
    "        # Step 2.C: Applying Dense layer to the hidden state output of the post-attention LSTM \n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Step 2.D: Append \"out\" to the \"outputs\" list \n",
    "        outputs.append(out)\n",
    "    \n",
    "    # Step 3: Create model instance taking three inputs and returning the list of outputs. \n",
    "    model = Model(inputs = [X,s0,c0], outputs = outputs)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1c35c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelb = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))\n",
    "#model_use = modela\n",
    "opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999) # decay=0.01)\n",
    "modelb.compile(opt, loss='categorical_crossentropy', metrics=['accuracy'] )\n",
    "#modelb.summary()\n",
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))\n",
    "modelb.fit([Xoh, s0, c0], outputs, epochs=10, batch_size=100, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08729e0",
   "metadata": {},
   "source": [
    "## Test on few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8f91ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import newaxis\n",
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "\n",
    "\n",
    "for example in EXAMPLES:\n",
    "    \n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "    #print(source)\n",
    "    #print(human_vocab)\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))#.swapaxes(0,1)\n",
    "    #print(source.shape)\n",
    "    source = source[newaxis,:,:]\n",
    "    #print(source.shape)\n",
    "    prediction = modelb.predict([source,s0, c0])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
    "    \n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d00689",
   "metadata": {},
   "source": [
    "### Getting the activations from the network\n",
    "\n",
    "Lets now visualize the attention values in your network. We'll propagate an example through the network, then visualize the values of $\\alpha^{\\langle t, t' \\rangle}$. \n",
    "\n",
    "To figure out where the attention values are located, let's start by printing a summary of the model ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2144ca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed66497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attn_map(model, input_vocabulary, inv_output_vocabulary, text, n_s = 64, num = 7 , Tx = 30, Ty = 10):\n",
    "    \"\"\"\n",
    "    Plot the attention map.\n",
    "  \n",
    "    \"\"\"\n",
    "    attention_map = np.zeros((10, 30))\n",
    "    Ty, Tx = attention_map.shape\n",
    "    \n",
    "    s0 = np.zeros((1, n_s))\n",
    "    c0 = np.zeros((1, n_s))\n",
    "    layer = model.layers[num]\n",
    "\n",
    "    encoded = np.array(string_to_int(text, Tx, input_vocabulary)).reshape((1, 30))\n",
    "    encoded = np.array(list(map(lambda x: to_categorical(x, num_classes=len(input_vocabulary)), encoded)))\n",
    "\n",
    "    f = K.function(model.inputs, [layer.get_output_at(t) for t in range(Ty)])\n",
    "    r = f([encoded, s0, c0])\n",
    "    \n",
    "    for t in range(Ty):\n",
    "        for t_prime in range(Tx):\n",
    "            attention_map[t][t_prime] = r[t][0,t_prime,0]\n",
    "\n",
    "    # Normalize attention map\n",
    "#     row_max = attention_map.max(axis=1)\n",
    "#     attention_map = attention_map / row_max[:, None]\n",
    "\n",
    "    prediction = model.predict([encoded, s0, c0])\n",
    "    \n",
    "    predicted_text = []\n",
    "    for i in range(len(prediction)):\n",
    "        predicted_text.append(int(np.argmax(prediction[i], axis=1)))\n",
    "        \n",
    "    predicted_text = list(predicted_text)\n",
    "    predicted_text = int_to_string(predicted_text, inv_output_vocabulary)\n",
    "    text_ = list(text)\n",
    "    \n",
    "    # get the lengths of the string\n",
    "    input_length = len(text)\n",
    "    output_length = Ty\n",
    "    \n",
    "    # Plot the attention_map\n",
    "    plt.clf()\n",
    "    f = plt.figure()\n",
    "    f.set_figwidth(18)\n",
    "    f.set_figheight(8.5)\n",
    "    ax = f.add_subplot(1, 1, 1)\n",
    "\n",
    "    # add image\n",
    "    i = ax.imshow(attention_map, interpolation='nearest', cmap='gray')\n",
    "\n",
    "    # add colorbar\n",
    "    cbaxes = f.add_axes([0.2, 0.0, 0.6, 0.03])\n",
    "    cbar = f.colorbar(i, cax=cbaxes, orientation='horizontal')\n",
    "    cbar.ax.set_xlabel('Alpha value (Probability output of the \"softmax\")', labelpad=2)\n",
    "\n",
    "    # add labels\n",
    "    ax.set_yticks(range(output_length))\n",
    "    ax.set_yticklabels(predicted_text[:output_length])\n",
    "\n",
    "    ax.set_xticks(range(input_length))\n",
    "    ax.set_xticklabels(text_[:input_length], rotation=45)\n",
    "\n",
    "    ax.set_xlabel('Input Sequence')\n",
    "    ax.set_ylabel('Output Sequence')\n",
    "\n",
    "    # add grid and legend\n",
    "    ax.grid()\n",
    "\n",
    "    #f.show()\n",
    "    \n",
    "    return attention_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25acf78f",
   "metadata": {},
   "source": [
    "On the generated plot we can observe the values of the attention weights for each character of the predicted output. \n",
    "\n",
    "In the date translation application, we will observe that most of the time attention helps predict the date/month, and hasn't much impact on predicting the year ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d5aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_map = plot_attn_map(modelb, human_vocab, inv_machine_vocab, \"Friday Oct 19 2018\", num = 7, n_s = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51105171",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_map = plot_attn_map(modelb, human_vocab, inv_machine_vocab, \"Saturday May 9 2018\", num = 7, n_s = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890b0eb3",
   "metadata": {},
   "source": [
    "The ideas in this notebook are based on Andrew Ng's Deep Learning Specialization on Coursera.org."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
