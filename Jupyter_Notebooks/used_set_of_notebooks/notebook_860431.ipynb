{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed96fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author Vishwanath Akuthota\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52349ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "#df.head()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f930e0",
   "metadata": {},
   "source": [
    "checking target class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b895067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_classes = pd.value_counts(df['Class'], sort = True).sort_index()\n",
    "count_classes.plot(kind = 'bar')\n",
    "plt.title(\"Fraud class histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cadf35",
   "metadata": {},
   "source": [
    "The data is unblanced\n",
    "   Resampling approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da17bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting our input and target variables + resampling.\n",
    "#Normalising the amount column. The amount column is not in line with the anonimised features.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df['normAmount'] = StandardScaler().fit_transform(df['Amount'].reshape(-1, 1))\n",
    "df = df.drop(['Time','Amount'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438de83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning X and Y. No resampling.\n",
    "\n",
    "X = df.ix[:, df.columns != 'Class']\n",
    "y = df.ix[:, df.columns == 'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c7b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets's do Resampling.\n",
    "\n",
    "# Number of data points in the minority class\n",
    "number_records_fraud = len(df[df.Class == 1])\n",
    "fraud_indices = np.array(df[df.Class == 1].index)\n",
    "\n",
    "\n",
    "# Picking the indices of the normal classes\n",
    "normal_indices = df[df.Class == 0].index\n",
    "\n",
    "# Out of the indices we picked, randomly select \"x\" number (number_records_fraud)\n",
    "random_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace = False)\n",
    "random_normal_indices = np.array(random_normal_indices)\n",
    "\n",
    "# Appending the 2 indices\n",
    "under_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\n",
    "\n",
    "# Under sample dataset\n",
    "under_sample_df = df.iloc[under_sample_indices,:]\n",
    "\n",
    "X_undersample = under_sample_df.ix[:, under_sample_df.columns != 'Class']\n",
    "y_undersample = under_sample_df.ix[:, under_sample_df.columns == 'Class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a10c3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing ratio\n",
    "\n",
    "print(\"Percentage of normal transactions: \", len(under_sample_df[under_sample_df.Class == 0])/len(under_sample_df))\n",
    "print(\"Percentage of fraud transactions: \", len(under_sample_df[under_sample_df.Class == 1])/len(under_sample_df))\n",
    "print(\"Total number of transactions in resampled data: \", len(under_sample_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f16f9e",
   "metadata": {},
   "source": [
    "# Splitting data into two sets train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df63089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# splitting up the Whole dataset \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 0)\n",
    "\n",
    "print(\"Number transactions train dataset: \", len(X_train))\n",
    "print(\"Number transactions test dataset: \", len(X_test))\n",
    "print(\"Total number of transactions: \", len(X_train)+len(X_test))\n",
    "\n",
    "# Undersampled dataset\n",
    "X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample\n",
    "                                                                                                   ,y_undersample\n",
    "                                                                                                   ,test_size = 0.3\n",
    "                                                                                                   ,random_state = 0)\n",
    "                                                                                                   \n",
    "print(\"\")\n",
    "print(\"Number transactions train dataset: \", len(X_train_undersample))\n",
    "print(\"Number transactions test dataset: \", len(X_test_undersample))\n",
    "print(\"Total number of transactions: \", len(X_train_undersample)+len(X_test_undersample))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55700893",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recall function could be more used because of 50 - 50 % \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import KFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d6bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printing_Kfold_scores(x_train_df,y_train_df):\n",
    "    fold = KFold(len(y_train_df),5,shuffle=False) \n",
    "\n",
    "    # Different C parameters\n",
    "    c_param_range = [0.01,0.1,1,10,100]\n",
    "\n",
    "    results_table = pd.DataFrame(index = range(len(c_param_range),2), columns = ['C_parameter','Mean recall score'])\n",
    "    results_table['C_parameter'] = c_param_range\n",
    "\n",
    "    # the k-fold will give 2 lists: train_indices = indices[0], test_indices = indices[1]\n",
    "    j = 0\n",
    "    for c_param in c_param_range:\n",
    "        print('-------------------------------------------')\n",
    "        print('C parameter: ', c_param)\n",
    "        print('-------------------------------------------')\n",
    "        print('')\n",
    "\n",
    "        recall_accs = []\n",
    "        for iteration, indices in enumerate(fold,start=1):\n",
    "\n",
    "            # Call the logistic regression model with a certain C parameter\n",
    "            lr = LogisticRegression(C = c_param, penalty = 'l1')\n",
    "\n",
    "            # Use the training data to fit the model. In this case, we use the portion of the fold to train the model\n",
    "            # with indices[0]. We then predict on the portion assigned as the 'test cross validation' with indices[1]\n",
    "            lr.fit(x_train_df.iloc[indices[0],:],y_train_df.iloc[indices[0],:].values.ravel())\n",
    "\n",
    "            # Predict values using the test indices in the training data\n",
    "            y_pred_undersample = lr.predict(x_train_df.iloc[indices[1],:].values)\n",
    "\n",
    "            # Calculate the recall score and append it to a list for recall scores representing the current c_parameter\n",
    "            recall_acc = recall_score(y_train_df.iloc[indices[1],:].values,y_pred_undersample)\n",
    "            recall_accs.append(recall_acc)\n",
    "            print('Iteration ', iteration,': recall score = ', recall_acc)\n",
    "\n",
    "        # The mean value of those recall scores is the metric we want to save and get hold of.\n",
    "        results_table.ix[j,'Mean recall score'] = np.mean(recall_accs)\n",
    "        j += 1\n",
    "        print('')\n",
    "        print('Mean recall score ', np.mean(recall_accs))\n",
    "        print('')\n",
    "\n",
    "    best_c = results_table.loc[results_table['Mean recall score'].idxmax()]['C_parameter']\n",
    "    \n",
    "    # Finally, we can check which C parameter is the best amongst the chosen.\n",
    "    print('*********************************************************************************')\n",
    "    print('Best model to choose from cross validation is with C parameter = ', best_c)\n",
    "    print('*********************************************************************************')\n",
    "    \n",
    "    return best_c\n",
    "\n",
    "best_c = printing_Kfold_scores(X_train_undersample,y_train_undersample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9042f6da",
   "metadata": {},
   "source": [
    "# Create a function to plot a fancy confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790a1979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=0)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        1#print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"green\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6c6ca9",
   "metadata": {},
   "source": [
    "# writing a function for confusion matrix recall(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b0c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this C_parameter to build the final model with the whole training dataset and predict the classes in the test\n",
    "# dataset\n",
    "lr = LogisticRegression(C = best_c, penalty = 'l1')\n",
    "lr.fit(X_train_undersample,y_train_undersample.values.ravel())\n",
    "y_pred_undersample = lr.predict(X_test_undersample.values)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test_undersample,y_pred_undersample)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "class_names = [0,1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix\n",
    "                      , classes=class_names\n",
    "                      , title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e07d91b",
   "metadata": {},
   "source": [
    "# Now lets's apply for the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78316adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this C_parameter to build the final model with the whole training dataset and predict the classes in the test\n",
    "# dataset\n",
    "lr = LogisticRegression(C = best_c, penalty = 'l1')\n",
    "lr.fit(X_train_undersample,y_train_undersample.values.ravel())\n",
    "y_pred = lr.predict(X_test.values)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test,y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "class_names = [0,1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix\n",
    "                      , classes=class_names\n",
    "                      , title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14170d77",
   "metadata": {},
   "source": [
    "We can start to be happy with how initial approach is workin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40e5f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC CURVE\n",
    "lr = LogisticRegression(C = best_c, penalty = 'l1')\n",
    "y_pred_undersample_score = lr.fit(X_train_undersample,y_train_undersample.values.ravel()).decision_function(X_test_undersample.values)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test_undersample.values.ravel(),y_pred_undersample_score)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "\n",
    "# Plot ROC\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b',label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.0])\n",
    "plt.ylim([-0.1,1.01])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1215536",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_c = printing_Kfold_scores(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ef5870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this C_parameter to build the final model with the whole training dataset and predict the classes in the test\n",
    "# dataset\n",
    "lr = LogisticRegression(C = best_c, penalty = 'l1')\n",
    "lr.fit(X_train,y_train.values.ravel())\n",
    "y_pred_undersample = lr.predict(X_test.values)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test,y_pred_undersample)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "class_names = [0,1]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix\n",
    "                      , classes=class_names\n",
    "                      , title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caf7660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets make predictions at didfferent level o f thersold \n",
    "\n",
    "lr = LogisticRegression(C = 0.01, penalty = 'l1')\n",
    "lr.fit(X_train_undersample,y_train_undersample.values.ravel())\n",
    "y_pred_undersample_proba = lr.predict_proba(X_test_undersample.values)\n",
    "\n",
    "thresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "j = 1\n",
    "for i in thresholds:\n",
    "    y_test_predictions_high_recall = y_pred_undersample_proba[:,1] > i\n",
    "    \n",
    "    plt.subplot(3,3,j)\n",
    "    j += 1\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_test_undersample,y_test_predictions_high_recall)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    print(\"Recall metric in the testing dataset: \", cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1]))\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    class_names = [0,1]\n",
    "    plot_confusion_matrix(cnf_matrix\n",
    "                          , classes=class_names\n",
    "                          , title='Threshold >= %s'%i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e116503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make a percision with observations.\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "lr = LogisticRegression(C = 0.01, penalty = 'l1')\n",
    "lr.fit(X_train_undersample,y_train_undersample.values.ravel())\n",
    "y_pred_undersample_proba = lr.predict_proba(X_test_undersample.values)\n",
    "\n",
    "thresholds = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal', 'red', 'yellow', 'green', 'blue','black'])\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "j = 1\n",
    "for i,color in zip(thresholds,colors):\n",
    "    y_test_predictions_prob = y_pred_undersample_proba[:,1] > i\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(y_test_undersample,y_test_predictions_prob)\n",
    "    \n",
    "    # Plot Precision-Recall curve\n",
    "    plt.plot(recall, precision, color=color,\n",
    "                 label='Threshold: %s'%i)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall example')\n",
    "    plt.legend(loc=\"lower left\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
