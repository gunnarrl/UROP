{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24766b2c",
   "metadata": {},
   "source": [
    "# Topic modelling on news data 25 topics\n",
    "\n",
    "- Data is taken from kaggle.\n",
    "- Applying the topic modeling the see the results for 25 topics\n",
    "- And choose the results for 2^nd level topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afb6392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv('abcnews-date-text.csv',error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963cab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e894b5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17284ae6",
   "metadata": {},
   "source": [
    "# Data preprocessing \n",
    "## Different process we do here like as Tokenization ,lemetization and stemming the data\n",
    "- we wanted to convert the data to their normal form for example 'stolen' would converted to steal\n",
    "### Here nltk is used for removing different language rather than english (Hindi and urdu like that word would be removed)\n",
    "- In preprocessing actually we remove all the punctuation marks , exclamatory marks and commas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be7b317",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "               \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2ecc4b",
   "metadata": {},
   "source": [
    "# Lemmatization \n",
    "## Lemmatization is used for gouping of word that's contains the same meaning(synonyms,antonyms)\n",
    "# Tokenization\n",
    "## Tokenization is used for keeps the word having meaningfull meaning\n",
    "- This is used for removal of word like if,the ,a,an that word doesn't make any sense in Topic\n",
    "# Stemming\n",
    "## Stemming is used for convert the word into their root form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff40f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110ba95a",
   "metadata": {},
   "source": [
    "# preview data after preprocessing \n",
    "- How the data will look like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbb1f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a document to preview after preprocessing\n",
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6412ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a0dd4f",
   "metadata": {},
   "source": [
    "### Dictionary is formed for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ce448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1799fe3",
   "metadata": {},
   "source": [
    "## Filtering the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561aede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7a4c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d510231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(bow_corpus, open('bow_corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbcaa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f5b75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d15ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=25, id2word=dictionary, passes=2, workers=2)\n",
    "lda_model.save('model25.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c15a6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d94400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_document = 'How a Pentagon de to help him identity crisis for Google'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62355574",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "corpus = pickle.load(open('bow_corpus.pkl', 'rb'))\n",
    "lda = gensim.models.ldamodel.LdaModel.load('model25.gensim')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee17e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f076c7c",
   "metadata": {},
   "source": [
    "## As topic related to health is syncronised well so we increase the number of topic stop here.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
