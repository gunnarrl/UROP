{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b781b8c",
   "metadata": {},
   "source": [
    "# Actividad pr치ctica: Predicci칩n de series de tiempo\n",
    "\n",
    "En esta tarea se pide entrenar y evaluar un predictor para la serie de tiempo Mackey-Glass. Esta serie de tiempo se obtiene de la soluci칩n de la siguiente ecuaci칩n diferencial\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dt} = 0.2 \\frac{ y(t-\\tau)}{1 + y(t-\\tau)^{10}} - 0.1 y(t),\n",
    "$$\n",
    "\n",
    "donde el par치metro $\\tau$ controla el comportamiento din치mico de la serie de tiempo \n",
    "\n",
    "- Siga las instrucciones en este notebook para resolver el problema de predicci칩n\n",
    "- Conteste las preguntas que se encuentran en este enunciado\n",
    "- Finalmente env칤e su notebook con los resultados y respuestas a phuijse@inf.uach.cl\n",
    "- No olvide cambiar el t칤tulo para reflejar los integrantes de su grupo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb2ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib notebook\n",
    "from IPython.display import display, Audio, HTML\n",
    "import matplotlib.pylab as plt\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import scipy.signal\n",
    "%matplotlib notebook\n",
    "from matplotlib import animation, patches\n",
    "import soundfile as sf\n",
    "from style import *\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import *\n",
    "\n",
    "NMSE = lambda y, yhat : np.sum((y - yhat)**2)/np.var(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b61eeef",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Use el c칩digo que se muestra a continuaci칩n para generar la serie de tiempo Mackey Glass\n",
    "\n",
    "- Considere una raz칩n se침al a ruido (SNR) de 2.\n",
    "- Considere $\\tau=17$ (comportamiento debilmente ca칩tico)\n",
    "\n",
    "Se generaran 1000 muestras de la serie de tiempo. Use los primeros 500 puntos para entrenar y los siguientes 500 puntos para hacer predicci칩n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576db2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# n칰mero de observaciones (no modificar)\n",
    "N = 1000\n",
    "# Raz칩n se침al a ruido (2., 0.5)\n",
    "SNR = 2.\n",
    "# constantes de la ecuaci칩n diferencial (no modificar)\n",
    "a, b = 0.1, 0.2\n",
    "# comportamiento din치mico de Mackey-Glass (17, 30)\n",
    "tau = 17.\n",
    "print(tau)\n",
    "# paso de integraci칩n (no modificar)\n",
    "dt = 0.05\n",
    "# condici칩n incial (no modificar)\n",
    "y0 = 0.9\n",
    "# largo temporal (no modificar)\n",
    "tt = 5.\n",
    "t = np.linspace(0, tt, num=N)\n",
    "\n",
    "N_full, tau_full = int(N*tt/dt), int(tau/dt)\n",
    "ymg = y0*np.ones(shape=(N_full, ))\n",
    "# Runge-Kutta integration\n",
    "for n in range(tau_full, N_full-1):\n",
    "    byd = b*ymg[n-tau_full]/(1.0 + ymg[n-tau_full]**10.0)\n",
    "    yk1 = dt*(-a*ymg[n] + byd)\n",
    "    yk2 = dt*(-a*(ymg[n]+yk1/2) + byd)\n",
    "    yk3 = dt*(-a*(ymg[n]+yk2/2) + byd)\n",
    "    yk4 = dt*(-a*(ymg[n]+yk3) + byd)\n",
    "    ymg[n+1] = ymg[n] + yk1/6 + yk2/3 +yk3/3 +yk4/6;\n",
    "ymg = ymg[::int(tt/dt)]\n",
    "#ymg = ymg - np.mean(ymg) \n",
    "# Contaminaci칩n con ruido blanco aditivo\n",
    "s_noise = np.sqrt(np.var(ymg)/SNR) \n",
    "np.random.seed(0)\n",
    "y_obs = ymg + s_noise*np.random.randn(len(ymg))\n",
    "\n",
    "# Gr치fico\n",
    "fig, ax = plt.subplots(1, figsize=(9, 3), tight_layout=True)\n",
    "ax.plot(t[:500], y_obs[:500])\n",
    "ax.set_title('Serie de tiempo Mackey-Glass (entrenamiento)');\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3d13e0",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "1. Describa en detalle el algoritmo LMS indicando sus semejanzas y diferencias con el filtro de Wiener \n",
    "1. Partiendo del error instantaneo $J_n^s(\\textbf{w}) = e_n^2$ derive la regla de actualizaci칩n de pesos\n",
    "1. La siguiente clase de *Python* predice y entrena un filtro LMS. Complete la l칤nea que dice \n",
    "\n",
    "` self.w = ? `\n",
    "\n",
    "con el valor correcto de actualizaci칩n de peso del filtro LMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4887315",
   "metadata": {},
   "source": [
    " 1) EL algoritmo LMS es un tipo de filtro adaptativo, usa el descenso del gradiente para estimar una se침al variable en el tiempo. Este algoritmo es uno de los m치s utilizados usado debido a su baja complejidad computacional y estabilidad.\n",
    "\n",
    "   El filtro consiste en dos componentes, la primera es el calculo de la salida del filtro FIR convulucionado la entrada con los coeficientes. Ademas de la estimaci칩n del error.  y la segunda componente es el coeficiente de actualizacion de pesos.\n",
    "   \n",
    "Semejanzas: \n",
    "            - LMS converge a la solucion de Wiener\n",
    "            - LMS es derivado de Wiener\n",
    "            - Son predictores/estimadores\n",
    "            - Ambos puede ser usados para identificar la respuesta al impulso de sistema desconocido.\n",
    "            - Se entrena de manera recursiva y online\n",
    "            \n",
    "Diferencias: \n",
    "             - LMS: No se requiere conocimiento estad칤stico del proceso\n",
    "             - LMS: No es necesario calcular e invertir la matriz de correlaci칩n\n",
    "             - LMS se actualiza online y tiene costo 洧.\n",
    "             - LMS: El aprendizaje es estad칤stico\n",
    "             - Wiener: se entrena offline y tiene costo 洧2\n",
    "             - Wiener: El aprendizaje es determinista.\n",
    "\n",
    "\n",
    "2) Derivada $J_n^s(\\textbf{w}) = e_n^2$\n",
    "\n",
    "Sabiendo que $e_n = (d_n - y_n)$,  tambien $y_n = \\sum_{k=0}^{L} w_{n, k} u_{n-k}$\n",
    "               \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J^s_n(\\textbf{w}) &= e_n^2  \\\\\n",
    "&= (d_n - y_n)^2 \\\\\n",
    "&= (d_n - \\sum_{k=0}^{L} w_{n, k} u_{n-k} )^2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dJ(w)}{dw} &=   2 (d_n - \\sum_{k=0}^{L} w_{n, k} u_{n-k})  -u_{n-k} \\\\\n",
    "\\frac{dJ(w)}{dw} &=   -2 e_n u_{n-k} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Usando la regla SGD llegamos a:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{w}_{n+1} = \\textbf{w}_{n} + 2 \\mu e_n \\textbf{u}_{n}\\nonumber \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db8b0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMS_filter(object):\n",
    "    \n",
    "    def __init__(self, L=1, mu=0.5, normalized=True):\n",
    "        self.L = L\n",
    "        self.mu = mu\n",
    "        self.w = np.zeros(shape=(L, ))\n",
    "        self.normalized = normalized\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.L\n",
    "    \n",
    "    def predict(self, u):\n",
    "        return np.dot(self.w, u)\n",
    "    \n",
    "    def update(self, u, d):\n",
    "        d_pred = self.predict(u)\n",
    "        norm = 1.\n",
    "        if self.normalized:\n",
    "            norm = np.sum(u**2) + 1e-6\n",
    "        self.w = self.w + 2 * self.mu * (d - d_pred) * u / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ca3cc0",
   "metadata": {},
   "source": [
    "***\n",
    "## Predicci칩n con algoritmo LMS\n",
    "\n",
    "1. Entrene el predictor con el algoritmo normalized LMS usando el siguiente bloque de c칩digo\n",
    "1. Construya una tabla con los NMSE de entrenamiento y prueba para distintos valores de $\\mu$ y $L$\n",
    "    - Se recomienda hacer un barrido logar칤tmico en $\\mu$ (por ejemplo `mu=np.logspace(-2, 0, num=20)`)\n",
    "    - Use al menos los siguientes valores de $L$: [5, 10, 20, 30]\n",
    "1. Describa cada experimento analizando sus resultados de forma cuantitativa y cualitativa\n",
    "    - 쯉e sobreajuste el filtro a los datos de entrenamiento? \n",
    "    - 쯉e desestabiliza el filtro?\n",
    "1. Indique que combinaci칩n obtiene menor MSE de prueba \n",
    "***\n",
    "\n",
    "1. Repita el experimento para $\\tau = 30$ (comportamiento fuertemente ca칩tico) \n",
    "1. Compare los resultados obtenidos con cada serie de tiempo. 쯈u칠 casos son m치s sencillos y cuales m치s complicados?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf69f21b",
   "metadata": {},
   "source": [
    "3) Dada la cantidad de posibiladades que existen entre L y mu, revisaremos algunos casos mas relevantes:\n",
    "    \n",
    "    \n",
    "Para 洧랦=17 (puede probar los valores, abajo se encuentra el gr치fico con un slider)\n",
    "\n",
    "a) L=10 y  mu=0.784760 Se puede apreciar en el grafico que esta sobreajustado, es decir, anda muy bien \n",
    "  para el entrenamiento, pero en el test se ve terriblemente afectado. Se desastebiliza bastante.\n",
    "  Siendo MSE entrenamiento 1032.4427, prueba 5186.2362\n",
    "\n",
    "b) L=20 y mu=0.29 Este es uno de los mejores casos, no esta sobreajustado, para el entrenamiento como con el test anda bastante bien,\n",
    "ya que el error cuadratico es bastante peque침o. El filtro no se desastebiliza en general.\n",
    "Siendo MSE entrenamiento 303.7687, prueba 311.9917.\n",
    "\n",
    "c) L=30 y mu=0.61 Este es un caso medio, tiene un sobreajuste medio, posee un error cuadratico moderado,\n",
    "                 Siendo MSE entrenamiento 377.1947, prueba 502.9953\n",
    "\n",
    "\n",
    "4)\n",
    " Valores Optimos L = 30, mu = 0.088587, test = 187.915610\n",
    "\n",
    "\n",
    "\n",
    "Para 洧랦=30\n",
    "\n",
    "a) L=10 y  mu=0.784760 Se puede ver en el gr치fico que se encuentra sobreajustado, tiene un error cuadratico tremendo, por lo que se desastebiliza bastante.\n",
    "Siendo MSE entrenamiento 1519.2776, prueba 7629.6103\n",
    "\n",
    "b) L=20 y mu=0.29 En este caso, se puede apreciar que esta mediamante sobreajustado, el error cuadratico es medio. Se desastebiliza medianamente. Siendo MSE entrenamiento 413.9651, prueba 869.1495\n",
    "\n",
    "c) L=30 Y mu=0.61 Esta bastante sobreajustado, el error cuadratico es muy grande. Se desastebiliza bastante. Siendo MSE entrenamiento 395.9658, prueba 2181.1657.\n",
    "\n",
    "4) Valores Optimos L = 20, mu = 0.069519, test = 316.133279\n",
    "\n",
    "----------------------------------------------------------------------------------\n",
    "Compare los resultados obtenidos con cada serie de tiempo. 쯈u칠 casos son m치s sencillos y cuales m치s complicados?\n",
    "\n",
    "Se puede apreciar que para el 洧랦=30, en los casos anteriormentes estudiados, el resultado es mucho m치s caotico, por ejemplo para 洧랦=17 L=20 y mu=0.29, resulta ser uno de lo mejores casos, pero para el 洧랦=30, para ser un caso sin ning칰n atractivo. Tambien sucede para la mayoria de los otros casos, en vez de mejorar se empeoran m치s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef52aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_NMSE_train = []\n",
    "data_NMSE_test = []\n",
    "L = [5,10,20,30]\n",
    "mu = np.logspace(-2,0,num=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386703c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lms_update(L,mu,print_plot):\n",
    "    lms = LMS_filter(L, mu, normalized=True)\n",
    "\n",
    "    # Entrenamiento\n",
    "    y_pred = np.zeros(shape=(len(y_obs), ))\n",
    "    for k in range(lms.__len__(), 500):\n",
    "        y_window = y_obs[k-lms.__len__():k]\n",
    "        y_pred[k] = lms.predict(y_window)\n",
    "        lms.update(d=y_obs[k], u=y_window)\n",
    "    # Prueba\n",
    "    for k in range(500, len(y_obs)):\n",
    "        y_window = y_obs[k-lms.__len__():k]\n",
    "        y_pred[k] = lms.predict(y_window)\n",
    "\n",
    "    \n",
    "    data_NMSE_train.append((NMSE(ymg[lms.__len__():500], y_pred[lms.__len__():500])))\n",
    "    data_NMSE_test.append(NMSE(ymg[500:], y_pred[500:]))\n",
    "                           \n",
    "    if print_plot:\n",
    "        print(\"MSE entrenamiento %0.4f, prueba %0.4f\" %(NMSE(ymg[lms.__len__():500], y_pred[lms.__len__():500]), \n",
    "                                                NMSE(ymg[500:], y_pred[500:])))\n",
    "    \n",
    "        fig, ax = plt.subplots(3, figsize=(9, 6), tight_layout=True)\n",
    "        ax[0].plot(t, y_obs, 'k.', alpha=0.5, label='Observado'); ax[0].legend();\n",
    "        ax[1].plot(t, ymg, 'g-', alpha=0.5, lw=2, label='Intr칤nseco'); \n",
    "        ax[1].plot(t[:500], y_pred[:500], alpha=0.75, lw=2, label='Predicho train'); \n",
    "        ax[1].plot(t[500:], y_pred[500:], alpha=0.75, lw=2, label='Predicho test'); ax[1].legend();\n",
    "\n",
    "        ax[2].plot(t[:500], (ymg[:500] - y_pred[:500])**2, label='Error cuadr치tico train'); \n",
    "        ax[2].plot(t[500:], (ymg[500:] - y_pred[500:])**2, label='Error cuadr치tico test'); ax[2].legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11382737",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [5,10,20,30]\n",
    "mu = np.logspace(-2,0,num=20)\n",
    "\n",
    "l_opt = mu_opt = test_opt = 100000000\n",
    "\n",
    "for l_val in L:\n",
    "    print(\"L = %i\"%l_val)\n",
    "    data_NMSE_train = []\n",
    "    data_NMSE_test = []\n",
    "    for mu_val in mu:\n",
    "        lms_update(l_val,mu_val,print_plot=False)\n",
    "    df = DataFrame({\"MU\":mu, \"MSE TRAIN\": data_NMSE_train,\"MSE TEST\": data_NMSE_test})\n",
    "    print(df)\n",
    "    \n",
    "    for opt in data_NMSE_test:\n",
    "        if opt < test_opt:\n",
    "            test_opt = opt\n",
    "            l_opt = l_val\n",
    "            mu_opt = mu[data_NMSE_test.index(opt)]\n",
    "    \n",
    "    if l_val < 30 : print(\"--------------------------------------------------------------------------\")\n",
    "    \n",
    "print(\"\\n Valores Optimos L = %i, mu = %f, test = %f\"%(l_opt,mu_opt,test_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a594c594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lms_slider(L,mu):\n",
    "    lms_update(L,mu,print_plot=True)\n",
    "\n",
    "print(\"Plot for TAU=%f\"%tau)\n",
    "interact(lms_slider, L=widgets.SelectionSlider(options=L, value=l_opt),mu=np.logspace(-2,0,num=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2acf408",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# n칰mero de observaciones (no modificar)\n",
    "N = 1000\n",
    "# Raz칩n se침al a ruido (2., 0.5)\n",
    "SNR = 2.\n",
    "# constantes de la ecuaci칩n diferencial (no modificar)\n",
    "a, b = 0.1, 0.2\n",
    "# comportamiento din치mico de Mackey-Glass (17, 30)\n",
    "tau = 30.\n",
    "print(\"SERIES RESTARTED WITH TAU=%f\"%tau)\n",
    "# paso de integraci칩n (no modificar)\n",
    "dt = 0.05\n",
    "# condici칩n incial (no modificar)\n",
    "y0 = 0.9\n",
    "# largo temporal (no modificar)\n",
    "tt = 5.\n",
    "t = np.linspace(0, tt, num=N)\n",
    "\n",
    "N_full, tau_full = int(N*tt/dt), int(tau/dt)\n",
    "ymg = y0*np.ones(shape=(N_full, ))\n",
    "# Runge-Kutta integration\n",
    "for n in range(tau_full, N_full-1):\n",
    "    byd = b*ymg[n-tau_full]/(1.0 + ymg[n-tau_full]**10.0)\n",
    "    yk1 = dt*(-a*ymg[n] + byd)\n",
    "    yk2 = dt*(-a*(ymg[n]+yk1/2) + byd)\n",
    "    yk3 = dt*(-a*(ymg[n]+yk2/2) + byd)\n",
    "    yk4 = dt*(-a*(ymg[n]+yk3) + byd)\n",
    "    ymg[n+1] = ymg[n] + yk1/6 + yk2/3 +yk3/3 +yk4/6;\n",
    "ymg = ymg[::int(tt/dt)]\n",
    "#ymg = ymg - np.mean(ymg) \n",
    "# Contaminaci칩n con ruido blanco aditivo\n",
    "s_noise = np.sqrt(np.var(ymg)/SNR) \n",
    "np.random.seed(0)\n",
    "y_obs = ymg + s_noise*np.random.randn(len(ymg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf24f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_opt = 100000000\n",
    "\n",
    "for l_val in L:\n",
    "    print(\"L = %i\"%l_val)\n",
    "    data_NMSE_train = []\n",
    "    data_NMSE_test = []\n",
    "    for mu_val in mu:\n",
    "        lms_update(l_val,mu_val,print_plot=False)\n",
    "    df = DataFrame({\"MU\":mu, \"MSE TRAIN\": data_NMSE_train,\"MSE TEST\": data_NMSE_test})\n",
    "    print(df)\n",
    "    \n",
    "    for opt in data_NMSE_test:\n",
    "        if opt < test_opt:\n",
    "            test_opt = opt\n",
    "            l_opt = l_val\n",
    "            mu_opt = mu[data_NMSE_test.index(opt)]\n",
    "    \n",
    "    if l_val < 30 : print(\"--------------------------------------------------------------------------\")\n",
    "    \n",
    "print(\"\\n Valores Optimos L = %i, mu = %f, test = %f\"%(l_opt,mu_opt,test_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0a1d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lms_slider(L,mu):\n",
    "    lms_update(L,mu,print_plot=True)\n",
    "\n",
    "print(\"Plot for TAU=%f\"%tau)\n",
    "interact(lms_slider, L=widgets.SelectionSlider(options=L, value=l_opt),mu=np.logspace(-2,0,num=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029366ed",
   "metadata": {},
   "source": [
    "***\n",
    "## Predicci칩n con algoritmo RLS\n",
    "\n",
    "1. Describa en detalle el algoritmo RLS indicando sus semejanzas y diferencias con el algoritmo LMS\n",
    "1. Partiendo del error hist칩rico $J_N(\\textbf{w}) = \\sum_{i=1}^N \\beta^{N-i} e_i^2$ derive la regla recursiva de actualizaci칩n de pesos \n",
    "1. La siguiente clase de *Python* predice y entrena un filtro RLS. Complete las l칤neas que dice \n",
    "\n",
    "` self.w = ? ` y `self.Phi_inv = ?`\n",
    "\n",
    "con el valor correcto de actualizaci칩n de peso del filtro RLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386f4bb6",
   "metadata": {},
   "source": [
    "1. El  algoritmo RLS provee una mayor velocidad de convergencia y menor error que el algoritmo LMS.\n",
    "Este algoritmo es una versi칩n muy parecida del algoritmo LMS, pero en el criterio de optimizaci칩n, \n",
    "es donde el algoritmo RLS, calcula una soluci칩n m치s exacta, es debido a esto que es m치s costoso (requiere muchas m치s operaciones).\n",
    "\n",
    "Semejanzas: \n",
    "            - Son estimadores/predictores\n",
    "            - Ambos son secuenciales, metodos online que resulven el mismo tipo de problema\n",
    "            - Ambos convergen a la \"misma\" soluci칩n\n",
    "\n",
    "Diferencias: \n",
    "\n",
    "             - RLS: Mayor velocidad de convergencia\n",
    "             - RLS: Menos error\n",
    "             - RLS: Mayor costo en operaciones \n",
    "             - RLS: Capaz de adaptarse a cambios m치s bruscos.\n",
    "             \n",
    "             - LMS: Menos costoso\n",
    "             - LMS: Proceso Markov\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1a7cae",
   "metadata": {},
   "source": [
    "***\n",
    "2. Partiendo de\n",
    "$$\n",
    "\\begin{align}\n",
    "    J_N(\\textbf{w}) = \\sum_{i=1}^n \\beta^{n-i} e_i^2, \\text{donde } 0<\\beta<=1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "El coste de la funcion se minimiza tomando la derivada parcial de $k$ de los coeficientes de $\\textbf{w}_n$ e igualando a cero.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\partial{J(\\textbf{w}_n)}}{\\partial \\textbf{w}_n(k)} = \\sum_{i=0}^n 2 \\beta^{n-i}e_i\\frac{\\partial e_i}{\\partial \\textbf{w}_n (k)} = -\\sum_{i=0}^n 2 \\beta^{n-i}e_i \\textbf{u}(i-k)=0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Reemplazando $e_i$ por la definicion de la se침al de error obtenemos.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{i=0}^n \\beta^{N-i} \\Big[ d_i - \\sum_{k=0}^L \\textbf{w}_{n,k} \\textbf{u}_{i-k} \\Big] d_{i-k} = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Reordenando las ecuaciones.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{l=0}^L \\textbf{w}_{n,k} \\Big[ \\sum_{i=0}^n \\beta^{n-i} \\textbf{u}_{i-l} \\textbf{u}_{i-k} \\Big] = \\sum_{i=0}^n\\beta^{n-i} d_i \\textbf{u}_{i-k}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Esta forma puede ser representada en terminos de matrices.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Phi_n \\textbf{w}_n = \\theta_n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Donde\n",
    "- Matriz de correlacion ponderada. $\\Phi_n = \\beta \\Phi_{n-1} + \\textbf{u}_n \\textbf{u}_n^T$\n",
    "- Vector de correlacion cruzada ponderada. $\\theta_n = \\theta_{n-1} + d_n \\textbf{u}_n$\n",
    "\n",
    "Asi, podemos encontrar los coeficientes que minimizan la funcion como:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{w}_n = \\Phi_n^{-1} \\theta_n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Queremos derivar una solucion recursiva de la forma\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{w}_n = \\textbf{w}_{n-1} - \\Delta \\textbf{w}_{n-1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Estableciendo las condiciones iniciales ($\\Phi_0 = \\delta I, \\theta_0 = 0 $) y usando la identidad de Woodbury: $(A + UCV)^{-1} = A^{-1} - A^{-1} U (C^{-1} + VA^{-1} U)^{-1} V A^{-1}$\n",
    "\n",
    "Con: $A = \\Phi_{n-1}^{-1}, U = \\textbf{u}_n, C = 1, V = \\textbf{u}_n^{-1}$.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Phi^{-1} &= \\Big[ \\beta \\Phi_{n-1} + \\textbf{u}_n \\textbf{u}_n^T\\Big]^{-1} \\\\\n",
    "&= \\beta^{-1} \\Phi_{n-1}^{-1} - \\beta^{-1} \\textbf{k}_n \\textbf{u}_n^T \\Phi_{n-1}^{-1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Donde $\\textbf{k}_n = \\Phi^{-1}_n \\textbf{u}_n$ es la ganancia.\n",
    "\n",
    "Asi\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{w}_n &= \\Phi^{-1}_n \\theta_n \\\\\n",
    "&= \\beta \\Phi^{-1}_n \\theta_{n-1} + d_n \\Phi^{-1}_n \\textbf{u}_n \\\\\n",
    "&=  \\Phi_{n-1}^{-1} \\theta_{n-1} - \\textbf{k}_n \\textbf{u}_n^T \\Phi_{n-1}^{-1} \\theta_{n-1} + \\Phi_n^{-1} \\textbf{u}_n d_n ; \\text{  si  }   \\textbf{w}_{n-1} =  \\Phi_{n-1}^{-1} \\theta_{n-1} \\\\\n",
    "&= \\textbf{w}_{n-1} + \\textbf{k}_n ( d_n - \\textbf{u}_n^T  \\textbf{w}_{n-1} ) \\\\\n",
    "&=  \\textbf{w}_{n-1} + \\textbf{k}_n e_n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Donde $\\textbf{k}_n e_n = \\Delta \\textbf{w}_{n-1}$ tambien llamado Factor de correccion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bcc912",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLS_filter(object):\n",
    "    \n",
    "    def __init__(self, L=1, beta=0.9, delta=10.):\n",
    "        self.L = L\n",
    "        self.beta = beta\n",
    "        self.w = np.zeros(shape=(L, ))\n",
    "        self.Phi_inv = delta*np.eye(L)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.L\n",
    "    \n",
    "    def predict(self, u):\n",
    "        return np.dot(self.w, u)\n",
    "    \n",
    "    def update(self, u, d):          \n",
    "        invbeta = 1.0/self.beta\n",
    "        d_pred = self.predict(u)\n",
    "        e = d - d_pred\n",
    "        r = 1. + invbeta*np.dot(np.dot(u, self.Phi_inv), u.T)\n",
    "        k = invbeta*np.dot(self.Phi_inv, u)/r\n",
    "        self.Phi_inv = invbeta* self.Phi_inv - invbeta * self.Phi_inv *np.sum(k*u)\n",
    "        self.w = self.w + np.dot(k,e)\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc9d563",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "1. Entrene el predictor con el algoritmo RLS usando el siguiente bloque de c칩digo\n",
    "1. Considere primero  $\\tau=17$\n",
    "1. Construya una tabla con los NMSE de entrenamiento y prueba para distintos valores de $\\beta$ y $L$\n",
    "    - Se recomienda hacer un barrido lineal en $\\beta$ (por ejemplo `mu=np.linspace(0.8, 1.0, num=20)`)\n",
    "    - Use al menos los siguientes valores de $L$: [5, 10, 20, 30]\n",
    "1. Describa cada experimento analizando sus resultados de forma cuantitativa y cualitativa\n",
    "    - 쮺u치nto demora el filtro en estabilizarse? \n",
    "    - 쯉e sobreajuste el filtro a los datos de entrenamiento? \n",
    "    - 쯉e desestabiliza el filtro?\n",
    "1. Indique que combinaci칩n obtiene menor MSE de prueba \n",
    "1. Repita el experimento para $\\tau=30$\n",
    "1. Compare con los resultados obtenidos con el algoritmo LMS 쯈u칠 algoritmo demora menos en converger?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6852ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n칰mero de observaciones (no modificar)\n",
    "N = 1000\n",
    "# Raz칩n se침al a ruido (2., 0.5)\n",
    "SNR = 2.\n",
    "# constantes de la ecuaci칩n diferencial (no modificar)\n",
    "a, b = 0.1, 0.2\n",
    "# comportamiento din치mico de Mackey-Glass (17, 30)\n",
    "tau = 17.\n",
    "print(\"SERIES RESTARTED WITH TAU=%f\"%tau)\n",
    "# paso de integraci칩n (no modificar)\n",
    "dt = 0.05\n",
    "# condici칩n incial (no modificar)\n",
    "y0 = 0.9\n",
    "# largo temporal (no modificar)\n",
    "tt = 5.\n",
    "t = np.linspace(0, tt, num=N)\n",
    "\n",
    "N_full, tau_full = int(N*tt/dt), int(tau/dt)\n",
    "ymg = y0*np.ones(shape=(N_full, ))\n",
    "# Runge-Kutta integration\n",
    "for n in range(tau_full, N_full-1):\n",
    "    byd = b*ymg[n-tau_full]/(1.0 + ymg[n-tau_full]**10.0)\n",
    "    yk1 = dt*(-a*ymg[n] + byd)\n",
    "    yk2 = dt*(-a*(ymg[n]+yk1/2) + byd)\n",
    "    yk3 = dt*(-a*(ymg[n]+yk2/2) + byd)\n",
    "    yk4 = dt*(-a*(ymg[n]+yk3) + byd)\n",
    "    ymg[n+1] = ymg[n] + yk1/6 + yk2/3 +yk3/3 +yk4/6;\n",
    "ymg = ymg[::int(tt/dt)]\n",
    "#ymg = ymg - np.mean(ymg) \n",
    "# Contaminaci칩n con ruido blanco aditivo\n",
    "s_noise = np.sqrt(np.var(ymg)/SNR) \n",
    "np.random.seed(0)\n",
    "y_obs = ymg + s_noise*np.random.randn(len(ymg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db0fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.linspace(0.8, 1.0, num=20)\n",
    "\n",
    "data_NMSE_train = []\n",
    "data_NMSE_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc1e051",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.linspace(0.8, 1.0, num=20)\n",
    "\n",
    "def rls_update(L,beta,print_plot):\n",
    "    NMSE = lambda y, yhat : np.sum((y - yhat)**2)/np.var(y)\n",
    "    \n",
    "    rls = RLS_filter(L, beta, delta=1.)\n",
    "    # Entrenamiento\n",
    "    y_pred = np.zeros(shape=(len(y_obs), ))\n",
    "    for k in range(rls.__len__(), 500):\n",
    "        y_window = y_obs[k-rls.__len__():k]\n",
    "        rls.update(d=y_obs[k], u=y_window)\n",
    "        y_pred[k] = rls.predict(y_window)\n",
    "    # Prueba\n",
    "    for k in range(500, len(y_obs)):\n",
    "        y_window = y_obs[k-rls.__len__():k]\n",
    "        y_pred[k] = rls.predict(y_window)\n",
    "        \n",
    "    data_NMSE_train.append(NMSE(ymg[rls.__len__():500], y_pred[rls.__len__():500]))\n",
    "    data_NMSE_test.append(NMSE(ymg[500:], y_pred[500:]))\n",
    "\n",
    "    if print_plot:\n",
    "        print(\"MSE entrenamiento %0.6f, prueba %0.6f\" %(NMSE(ymg[rls.__len__():500], y_pred[rls.__len__():500]), \n",
    "                                                        NMSE(ymg[500:], y_pred[500:])))\n",
    "        fig, ax = plt.subplots(3, figsize=(9, 6), tight_layout=True)\n",
    "        ax[0].plot(t, y_obs, 'k.', alpha=0.5, label='Observado'); ax[0].legend();\n",
    "        ax[1].plot(t, ymg, 'g-', alpha=0.5, lw=2, label='Intr칤nseco'); \n",
    "        ax[1].plot(t[:500], y_pred[:500], alpha=0.75, lw=2, label='Predicho train'); \n",
    "        ax[1].plot(t[500:], y_pred[500:], alpha=0.75, lw=2, label='Predicho test'); ax[1].legend();\n",
    "\n",
    "        ax[2].plot(t[:500], (ymg[:500] - y_pred[:500])**2, label='Error cuadr치tico train'); \n",
    "        ax[2].plot(t[500:], (ymg[500:] - y_pred[500:])**2, label='Error cuadr치tico test'); ax[2].legend(); \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e8e4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_opt = beta_opt = test_opt = 100000000\n",
    "\n",
    "\n",
    "for l_val in L:\n",
    "    print(\"L = %i\"%l_val)\n",
    "    data_NMSE_train = []\n",
    "    data_NMSE_test = []\n",
    "    for beta_val in beta:\n",
    "        rls_update(l_val,beta_val,print_plot=False)\n",
    "    df = DataFrame({\"BETA\":beta, \"MSE TRAIN\": data_NMSE_train,\"MSE TEST\": data_NMSE_test})\n",
    "    print(df)\n",
    "    \n",
    "    for opt in data_NMSE_test:\n",
    "        if opt < test_opt:\n",
    "            test_opt = opt\n",
    "            l_opt = l_val\n",
    "            beta_opt = beta[data_NMSE_test.index(opt)]\n",
    "    \n",
    "    if l_val < 30 : print(\"--------------------------------------------------------------------------\")\n",
    "    \n",
    "print(\"\\n Valores Optimos L = %i, BETA = %f, test = %f\"%(l_opt,beta_opt,test_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8d648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rls_slider(L,beta):\n",
    "    rls_update(L,beta,print_plot=True)\n",
    "\n",
    "print(\"Plot for TAU=%f\"%tau)\n",
    "interact(rls_slider, L=widgets.SelectionSlider(options=L, value=l_opt),beta = np.linspace(0.8, 1.0, num=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60038984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n칰mero de observaciones (no modificar)\n",
    "N = 1000\n",
    "# Raz칩n se침al a ruido (2., 0.5)\n",
    "SNR = 2.\n",
    "# constantes de la ecuaci칩n diferencial (no modificar)\n",
    "a, b = 0.1, 0.2\n",
    "# comportamiento din치mico de Mackey-Glass (17, 30)\n",
    "tau = 30.\n",
    "print(\"SERIES RESTARTED WITH TAU=%f\"%tau)\n",
    "# paso de integraci칩n (no modificar)\n",
    "dt = 0.05\n",
    "# condici칩n incial (no modificar)\n",
    "y0 = 0.9\n",
    "# largo temporal (no modificar)\n",
    "tt = 5.\n",
    "t = np.linspace(0, tt, num=N)\n",
    "\n",
    "N_full, tau_full = int(N*tt/dt), int(tau/dt)\n",
    "ymg = y0*np.ones(shape=(N_full, ))\n",
    "# Runge-Kutta integration\n",
    "for n in range(tau_full, N_full-1):\n",
    "    byd = b*ymg[n-tau_full]/(1.0 + ymg[n-tau_full]**10.0)\n",
    "    yk1 = dt*(-a*ymg[n] + byd)\n",
    "    yk2 = dt*(-a*(ymg[n]+yk1/2) + byd)\n",
    "    yk3 = dt*(-a*(ymg[n]+yk2/2) + byd)\n",
    "    yk4 = dt*(-a*(ymg[n]+yk3) + byd)\n",
    "    ymg[n+1] = ymg[n] + yk1/6 + yk2/3 +yk3/3 +yk4/6;\n",
    "ymg = ymg[::int(tt/dt)]\n",
    "#ymg = ymg - np.mean(ymg) \n",
    "# Contaminaci칩n con ruido blanco aditivo\n",
    "s_noise = np.sqrt(np.var(ymg)/SNR) \n",
    "np.random.seed(0)\n",
    "y_obs = ymg + s_noise*np.random.randn(len(ymg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b7b385",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_opt = beta_opt = test_opt = 100000000\n",
    "\n",
    "\n",
    "for l_val in L:\n",
    "    print(\"L = %i\"%l_val)\n",
    "    data_NMSE_train = []\n",
    "    data_NMSE_test = []\n",
    "    for beta_val in beta:\n",
    "        rls_update(l_val,beta_val,print_plot=False)\n",
    "    df = DataFrame({\"BETA\":beta, \"MSE TRAIN\": data_NMSE_train,\"MSE TEST\": data_NMSE_test})\n",
    "    print(df)\n",
    "    \n",
    "    for opt in data_NMSE_test:\n",
    "        if opt < test_opt:\n",
    "            test_opt = opt\n",
    "            l_opt = l_val\n",
    "            beta_opt = beta[data_NMSE_test.index(opt)]\n",
    "    \n",
    "    if l_val < 30 : print(\"--------------------------------------------------------------------------\")\n",
    "    \n",
    "print(\"\\n Valores Optimos L = %i, BETA = %f, test = %f\"%(l_opt,beta_opt,test_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d834fc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rls_slider(L,beta):\n",
    "    rls_update(L,beta,print_plot=True)\n",
    "\n",
    "print(\"Plot for TAU=%f\"%tau)\n",
    "interact(rls_slider, L=widgets.SelectionSlider(options=L, value=l_opt),beta = np.linspace(0.8, 1.0, num=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af85fc0",
   "metadata": {},
   "source": [
    "RLS\n",
    "Para tau = 17\n",
    "\n",
    "4.\n",
    "- Observando los graficos comparativos del filtro RLS, se puede afirmar que la convergencia (entiendase decaimiento  y estabilizacion del error cuadratico) del filtro RLS es bastante rapida (t < 0.2) si miramos el error cuadratico de entrenamiento.\n",
    "- Para ciertos valores de L y $\\beta$ (por ejemplo, L=30 y $\\beta$ > 0.9) se puede observar claramente el sobreajuste del filtro a los datos.\n",
    "- Por lo observado en el grafico, el filtro no alcanza a desestabilizarse de forma notoria, independiente de los valores de L y $\\beta$.\n",
    "5.\n",
    "A partir de los datos de la tabla para cada combinacion de L y $\\beta$, el mejor MSE de prueba para tau=17. se obtiene con \n",
    "$L = 30, \\beta = 0.821, test = 188.056$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9426e0b3",
   "metadata": {},
   "source": [
    "RLS\n",
    "Para tau = 30.\n",
    "\n",
    "4.\n",
    "- Para este valor de tau, la convergencia del filtro (entiendase decaimiento y estabilizacion del error cuadratico) es mucho mas rapida, observandose valores por debajo de 0.15[seg] (para L = 30, $\\beta$ = 0.905263), sin embargo, para valores de L = 5 y $\\beta$ < 1 el valor del error cuadratico medio cambia de manera muy rapida y su valor es en general ,comparativamente para este tau y L, el mas alto.\n",
    "- Se puede observar claramente un sobreajuste para valores de L=20 y $\\beta$ > 0.9. Este sobreajuste es bastante pronunciado, ya que la diferencia entre la seccion del grafico de prueba y el grafico intrinseco es bastante marcada.\n",
    "- El filtro no se desestabiliza de forma notoria para ningun valor de L y $\\beta$.\n",
    "5.\n",
    "Analizando los datos de la tabla para tau=30, L=[5,10,20,30] y $0.8 <= \\beta <= 1$; el mejor MSE de prueba se obtiene con $L = 30, \\beta = 0.905263$ y su valor es MSE Test = $308.8732$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d1826",
   "metadata": {},
   "source": [
    "7. Comparando los resultados obtenidos con los diferentes valores de tau, tanto para el filtro LMS como para el filtro RLS, el algoritmo que tiende a converger de manera mas rapida es el algoritmo RLS, aunque se requiera un gasto computacional mayor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75c630f",
   "metadata": {},
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
